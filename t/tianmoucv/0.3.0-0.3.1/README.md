# Comparing `tmp/tianmoucv-0.3.0-py3-none-any.whl.zip` & `tmp/tianmoucv-0.3.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,42 +1,51 @@
-Zip file size: 71594 bytes, number of entries: 40
--rw-r--r--  2.0 unx      341 b- defN 24-Apr-20 13:32 tianmoucv/__init__.py
--rw-r--r--  2.0 unx      994 b- defN 24-Apr-20 13:32 tianmoucv/tools.py
--rw-r--r--  2.0 unx       44 b- defN 24-Apr-20 13:32 tianmoucv/data/__init__.py
--rw-r--r--  2.0 unx    12544 b- defN 24-Apr-20 13:32 tianmoucv/data/tianmoucData.py
--rw-r--r--  2.0 unx    25536 b- defN 24-Apr-20 13:32 tianmoucv/data/tianmoucData_basic.py
--rw-r--r--  2.0 unx       49 b- defN 24-Apr-20 13:32 tianmoucv/isp/__init__.py
--rw-r--r--  2.0 unx    15720 b- defN 24-Apr-20 13:32 tianmoucv/isp/isp_basic.py
--rw-r--r--  2.0 unx     5423 b- defN 24-Apr-20 13:32 tianmoucv/isp/transform.py
--rw-r--r--  2.0 unx       62 b- defN 24-Apr-20 13:32 tianmoucv/nn/__init__.py
--rw-r--r--  2.0 unx       44 b- defN 24-Apr-20 13:32 tianmoucv/nn/unet_modules.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-20 13:32 tianmoucv/proc/__init__.py
--rw-r--r--  2.0 unx       51 b- defN 24-Apr-20 13:32 tianmoucv/proc/features/__init__.py
--rw-r--r--  2.0 unx    12331 b- defN 24-Apr-20 13:32 tianmoucv/proc/features/diff.py
--rw-r--r--  2.0 unx       48 b- defN 24-Apr-20 13:32 tianmoucv/proc/nn/__init__.py
--rw-r--r--  2.0 unx     5473 b- defN 24-Apr-20 13:32 tianmoucv/proc/nn/spy_modules.py
--rw-r--r--  2.0 unx     9559 b- defN 24-Apr-20 13:32 tianmoucv/proc/nn/unet_modules.py
--rw-r--r--  2.0 unx     9995 b- defN 24-Apr-20 13:32 tianmoucv/proc/nn/utils.py
--rw-r--r--  2.0 unx       67 b- defN 24-Apr-20 13:32 tianmoucv/proc/opticalflow/__init__.py
--rw-r--r--  2.0 unx     9037 b- defN 24-Apr-20 13:32 tianmoucv/proc/opticalflow/basic.py
--rw-r--r--  2.0 unx     8041 b- defN 24-Apr-20 13:32 tianmoucv/proc/opticalflow/estimator.py
--rw-r--r--  2.0 unx     3178 b- defN 24-Apr-20 13:32 tianmoucv/proc/opticalflow/spy_net.py
--rw-r--r--  2.0 unx      432 b- defN 24-Apr-20 13:32 tianmoucv/proc/reconstruct/__init__.py
--rw-r--r--  2.0 unx     6943 b- defN 24-Apr-20 13:32 tianmoucv/proc/reconstruct/basic.py
--rw-r--r--  2.0 unx     2495 b- defN 24-Apr-20 13:32 tianmoucv/proc/reconstruct/integration.py
--rw-r--r--  2.0 unx     5099 b- defN 24-Apr-20 13:32 tianmoucv/proc/reconstruct/tiny_unet.py
--rw-r--r--  2.0 unx       30 b- defN 24-Apr-20 13:32 tianmoucv/proc/tracking/__init__.py
--rw-r--r--  2.0 unx     3078 b- defN 24-Apr-20 13:32 tianmoucv/proc/tracking/feature_tracker.py
--rw-r--r--  2.0 unx      150 b- defN 24-Apr-20 13:32 tianmoucv/rdp_usb/CMakeLists.txt
--rw-r--r--  2.0 unx      283 b- defN 24-Apr-20 13:32 tianmoucv/rdp_usb/ReadMe.md
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-20 13:32 tianmoucv/rdp_usb/__init__.py
--rw-r--r--  2.0 unx      818 b- defN 24-Apr-20 13:32 tianmoucv/rdp_usb/compile_pybind.bat
--rwxr-xr-x  2.0 unx      165 b- defN 24-Apr-20 13:32 tianmoucv/rdp_usb/compile_pybind.sh
--rw-r--r--  2.0 unx    72985 b- defN 24-Apr-20 13:32 tianmoucv/rdp_usb/rod_decoder_py.cpp
--rw-r--r--  2.0 unx      677 b- defN 24-Apr-20 13:32 tianmoucv/rdp_usb/try_pcie2usb_conv.py
--rw-r--r--  2.0 unx     2505 b- defN 24-Apr-20 13:32 tianmoucv/rdp_usb/try_usb_data.py
--rw-r--r--  2.0 unx    35149 b- defN 24-Apr-20 13:34 tianmoucv-0.3.0.dist-info/LICENSE
--rw-r--r--  2.0 unx     2860 b- defN 24-Apr-20 13:34 tianmoucv-0.3.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-20 13:34 tianmoucv-0.3.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 24-Apr-20 13:34 tianmoucv-0.3.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     3484 b- defN 24-Apr-20 13:34 tianmoucv-0.3.0.dist-info/RECORD
-40 files, 255792 bytes uncompressed, 65960 bytes compressed:  74.2%
+Zip file size: 86215 bytes, number of entries: 49
+-rw-r--r--  2.0 unx      308 b- defN 24-May-06 08:38 tianmoucv/__init__.py
+-rw-r--r--  2.0 unx      994 b- defN 24-May-06 08:38 tianmoucv/tools.py
+-rw-r--r--  2.0 unx       99 b- defN 24-May-06 08:38 tianmoucv/data/__init__.py
+-rw-r--r--  2.0 unx    12911 b- defN 24-May-06 08:38 tianmoucv/data/tianmoucData.py
+-rw-r--r--  2.0 unx    25238 b- defN 24-May-06 08:38 tianmoucv/data/tianmoucData_basic.py
+-rw-r--r--  2.0 unx    25949 b- defN 24-May-06 08:38 tianmoucv/data/tianmoucData_pcie.py
+-rw-r--r--  2.0 unx       49 b- defN 24-May-06 08:38 tianmoucv/isp/__init__.py
+-rw-r--r--  2.0 unx    15704 b- defN 24-May-06 08:38 tianmoucv/isp/isp_basic.py
+-rw-r--r--  2.0 unx     5569 b- defN 24-May-06 08:38 tianmoucv/isp/transform.py
+-rw-r--r--  2.0 unx       96 b- defN 24-May-06 08:38 tianmoucv/nn/__init__.py
+-rw-r--r--  2.0 unx       44 b- defN 24-May-06 08:38 tianmoucv/nn/unet_modules.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-06 08:38 tianmoucv/proc/__init__.py
+-rw-r--r--  2.0 unx      100 b- defN 24-May-06 08:38 tianmoucv/proc/features/__init__.py
+-rw-r--r--  2.0 unx    12268 b- defN 24-May-06 08:38 tianmoucv/proc/features/diff.py
+-rw-r--r--  2.0 unx       48 b- defN 24-May-06 08:38 tianmoucv/proc/nn/__init__.py
+-rw-r--r--  2.0 unx     5473 b- defN 24-May-06 08:38 tianmoucv/proc/nn/spy_modules.py
+-rw-r--r--  2.0 unx     6552 b- defN 24-May-06 08:38 tianmoucv/proc/nn/unet_modules.py
+-rw-r--r--  2.0 unx     9995 b- defN 24-May-06 08:38 tianmoucv/proc/nn/utils.py
+-rw-r--r--  2.0 unx      177 b- defN 24-May-06 08:38 tianmoucv/proc/opticalflow/__init__.py
+-rw-r--r--  2.0 unx     8974 b- defN 24-May-06 08:38 tianmoucv/proc/opticalflow/basic.py
+-rw-r--r--  2.0 unx     6043 b- defN 24-May-06 08:38 tianmoucv/proc/opticalflow/estimator.py
+-rw-r--r--  2.0 unx     3206 b- defN 24-May-06 08:38 tianmoucv/proc/opticalflow/spy_net.py
+-rw-r--r--  2.0 unx      226 b- defN 24-May-06 08:38 tianmoucv/proc/reconstruct/__init__.py
+-rw-r--r--  2.0 unx     6884 b- defN 24-May-06 08:38 tianmoucv/proc/reconstruct/basic.py
+-rw-r--r--  2.0 unx     2505 b- defN 24-May-06 08:38 tianmoucv/proc/reconstruct/integration.py
+-rw-r--r--  2.0 unx     5093 b- defN 24-May-06 08:38 tianmoucv/proc/reconstruct/tiny_unet.py
+-rw-r--r--  2.0 unx       80 b- defN 24-May-06 08:38 tianmoucv/proc/tracking/__init__.py
+-rw-r--r--  2.0 unx     3078 b- defN 24-May-06 08:38 tianmoucv/proc/tracking/feature_tracker.py
+-rw-r--r--  2.0 unx      150 b- defN 24-May-06 08:38 tianmoucv/rdp_pcie/CMakeLists.txt
+-rw-r--r--  2.0 unx      283 b- defN 24-May-06 08:38 tianmoucv/rdp_pcie/ReadMe.md
+-rw-r--r--  2.0 unx        0 b- defN 24-May-06 08:38 tianmoucv/rdp_pcie/__init__.py
+-rw-r--r--  2.0 unx      818 b- defN 24-May-06 08:38 tianmoucv/rdp_pcie/compile_pybind.bat
+-rwxr-xr-x  2.0 unx      427 b- defN 24-May-06 08:38 tianmoucv/rdp_pcie/compile_pybind.sh
+-rw-r--r--  2.0 unx     2081 b- defN 24-May-06 08:38 tianmoucv/rdp_pcie/lyncam_compact_data.cpp
+-rw-r--r--  2.0 unx     2081 b- defN 24-May-06 08:38 tianmoucv/rdp_pcie/rod_compress.cpp
+-rw-r--r--  2.0 unx    31884 b- defN 24-May-06 08:38 tianmoucv/rdp_pcie/rod_decoder_py.cpp
+-rw-r--r--  2.0 unx      150 b- defN 24-May-06 08:38 tianmoucv/rdp_usb/CMakeLists.txt
+-rw-r--r--  2.0 unx      283 b- defN 24-May-06 08:38 tianmoucv/rdp_usb/ReadMe.md
+-rw-r--r--  2.0 unx        0 b- defN 24-May-06 08:38 tianmoucv/rdp_usb/__init__.py
+-rw-r--r--  2.0 unx      818 b- defN 24-May-06 08:38 tianmoucv/rdp_usb/compile_pybind.bat
+-rwxr-xr-x  2.0 unx      427 b- defN 24-May-06 08:38 tianmoucv/rdp_usb/compile_pybind.sh
+-rw-r--r--  2.0 unx    72985 b- defN 24-May-06 08:38 tianmoucv/rdp_usb/rod_decoder_py.cpp
+-rw-r--r--  2.0 unx      677 b- defN 24-May-06 08:38 tianmoucv/rdp_usb/try_pcie2usb_conv.py
+-rw-r--r--  2.0 unx     2505 b- defN 24-May-06 08:38 tianmoucv/rdp_usb/try_usb_data.py
+-rw-r--r--  2.0 unx    35149 b- defN 24-May-06 08:40 tianmoucv-0.3.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx     2891 b- defN 24-May-06 08:40 tianmoucv-0.3.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-06 08:40 tianmoucv-0.3.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 24-May-06 08:40 tianmoucv-0.3.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     4307 b- defN 24-May-06 08:40 tianmoucv-0.3.1.dist-info/RECORD
+49 files, 315681 bytes uncompressed, 79271 bytes compressed:  74.9%
```

## zipnote {}

```diff
@@ -9,14 +9,17 @@
 
 Filename: tianmoucv/data/tianmoucData.py
 Comment: 
 
 Filename: tianmoucv/data/tianmoucData_basic.py
 Comment: 
 
+Filename: tianmoucv/data/tianmoucData_pcie.py
+Comment: 
+
 Filename: tianmoucv/isp/__init__.py
 Comment: 
 
 Filename: tianmoucv/isp/isp_basic.py
 Comment: 
 
 Filename: tianmoucv/isp/transform.py
@@ -75,14 +78,38 @@
 
 Filename: tianmoucv/proc/tracking/__init__.py
 Comment: 
 
 Filename: tianmoucv/proc/tracking/feature_tracker.py
 Comment: 
 
+Filename: tianmoucv/rdp_pcie/CMakeLists.txt
+Comment: 
+
+Filename: tianmoucv/rdp_pcie/ReadMe.md
+Comment: 
+
+Filename: tianmoucv/rdp_pcie/__init__.py
+Comment: 
+
+Filename: tianmoucv/rdp_pcie/compile_pybind.bat
+Comment: 
+
+Filename: tianmoucv/rdp_pcie/compile_pybind.sh
+Comment: 
+
+Filename: tianmoucv/rdp_pcie/lyncam_compact_data.cpp
+Comment: 
+
+Filename: tianmoucv/rdp_pcie/rod_compress.cpp
+Comment: 
+
+Filename: tianmoucv/rdp_pcie/rod_decoder_py.cpp
+Comment: 
+
 Filename: tianmoucv/rdp_usb/CMakeLists.txt
 Comment: 
 
 Filename: tianmoucv/rdp_usb/ReadMe.md
 Comment: 
 
 Filename: tianmoucv/rdp_usb/__init__.py
@@ -99,23 +126,23 @@
 
 Filename: tianmoucv/rdp_usb/try_pcie2usb_conv.py
 Comment: 
 
 Filename: tianmoucv/rdp_usb/try_usb_data.py
 Comment: 
 
-Filename: tianmoucv-0.3.0.dist-info/LICENSE
+Filename: tianmoucv-0.3.1.dist-info/LICENSE
 Comment: 
 
-Filename: tianmoucv-0.3.0.dist-info/METADATA
+Filename: tianmoucv-0.3.1.dist-info/METADATA
 Comment: 
 
-Filename: tianmoucv-0.3.0.dist-info/WHEEL
+Filename: tianmoucv-0.3.1.dist-info/WHEEL
 Comment: 
 
-Filename: tianmoucv-0.3.0.dist-info/top_level.txt
+Filename: tianmoucv-0.3.1.dist-info/top_level.txt
 Comment: 
 
-Filename: tianmoucv-0.3.0.dist-info/RECORD
+Filename: tianmoucv-0.3.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tianmoucv/__init__.py

```diff
@@ -9,8 +9,8 @@
 
 # __init__.py
 __all__ = ['random', 'cv2', 'math', 'torch','os','sys']
 __author__ = 'Y. Lin'
 __contributor__ = 'T. Wang, Y. Chen'
 __authorEmail__ = '532109881@qq.com'
 
-print('TianMouCV™ 0.3.0, via',__author__,', update some nn-based function')
+print('TianMouCV™ 0.3.5, via',__author__,)
```

## tianmoucv/data/__init__.py

```diff
@@ -1 +1,2 @@
+from .tianmoucData_pcie import TianmoucDataReader_pcie
 from .tianmoucData import TianmoucDataReader
```

## tianmoucv/data/tianmoucData.py

```diff
@@ -17,15 +17,15 @@
     print("Current Path:", current_path)
     subprocess.run(['sh', './compile_pybind.sh'])
     from tianmoucv.rdp_usb import rod_decoder_py as rdc
     print('compile decoder successfully')
     
 #用于重建
 from tianmoucv.proc.reconstruct import laplacian_blending
-from tianmoucv.isp import default_rgb_isp,fourdirection2xy
+from tianmoucv.isp import default_rgb_isp
 from .tianmoucData_basic import TianmoucDataReader_basic
 
 class TianmoucDataReader(TianmoucDataReader_basic):
     '''
     - **TianmoucDataReader(0.3.3)**
         - ## [输入]
         - 输入dataPath：该路径下应当包含1个或多个子目录，每个子目录对应1段Tianmouc视频。
@@ -126,14 +126,18 @@
                     #拼接更多的sample，以待一起读取
                     if cone1[1]==cone2[0]:
                         newsample_merge['coneid'] += cone2[1:]
                         newsample_merge['rodid'] += sample_1['rodid'][1:]
                         newsample_merge[self.pathways[1]] += sample_1[self.pathways[1]][1:]
                         newsample_merge[self.pathways[0]] += sample_1[self.pathways[0]][1:]
                         accum_count += 1
+                    else:
+                        newsample_merge = dict([])
+                        accum_count = 1
+                        continue
                 
                 if len(new_legalFileList)>MAXLEN:
                     new_legalFileList = new_legalFileList[:MAXLEN]
                     
                 self.fileDict[key]['legalData']  = new_legalFileList
                 print(key,'origin length:',len(new_legalFileList))
 
@@ -161,19 +165,22 @@
         conefilename = self.fileDict[key][self.pathways[1]] # only read first one, if you want to use dual camera you can read it again
         rodfilename  = self.fileDict[key][self.pathways[0]]  # only read first one, if you want to use dual camera you can read it again
         coneAddrs = legalSample[self.pathways[1]]
         rodAddrs = legalSample[self.pathways[0]]
         
         rgb_list = []
         coneTimeStamp_list = []
-        
+        raw_list = []
+
         for i in range(self.N+1):
             #print('caddr:',coneAddrs[i])
             frame,timestamp = self.readConeFast(conefilename,coneAddrs[i])
+            frame_raw = np.reshape(frame.copy(), (self.cone_height,self.cone_width))
             frame = np.reshape(frame.astype(np.float32),(self.cone_height,self.cone_width))
+            raw_list.append(frame_raw)
             rgb_list.append(frame)
             coneTimeStamp_list.append(timestamp.astype(np.int64))
 
         metaInfo['C_name'] = conefilename
         metaInfo['C_timestamp'] = coneTimeStamp_list
         metaInfo['R_name'] = rodfilename
         metaInfo['R_timestamp'] = []
@@ -202,16 +209,18 @@
             tsdiff_inter  = self.tsd_preprocess(tsd)
             sample['tsdiff_160x320'] = tsdiff_inter
             tsdiff_resized = F.interpolate(tsdiff_inter,(320,640),mode='bilinear')
             sample['tsdiff'] = tsdiff_resized
             
             for i in range(self.N+1): 
                 frame,frame_without_isp = self.rgb_preprocess(rgb_list[i])
+                frame_raw = raw_list[i]
                 sample['F'+str(i)+'_without_isp'] = frame_without_isp
                 sample['F'+str(i)] = frame
+                sample['F' + str(i)+"_raw"] = frame_raw
                 SD_t = tsd[1:,mingap*i,...]
                 sample['F'+str(i)+'_HDR'] = self.HDRRecon(SD_t/128.0,frame)
         sample['meta'] = metaInfo
         sample['labels'] = legalSample['labels']
         sample['sysTimeStamp'] = legalSample['sysTimeStamp']
         return sample
```

## tianmoucv/data/tianmoucData_basic.py

```diff
@@ -4,30 +4,20 @@
 import torch
 import math,time,subprocess
 import torch.nn.functional as F
 
 try:
     from tianmoucv.rdp_usb import rod_decoder_py as rdc
 except:
-    import subprocess
-    print("WARNING: no decoder found, try to compile under ./rod_decoder_py")
-    current_file_path = os.path.abspath(__file__)
-    parent_folder_path = os.path.dirname(os.path.dirname(current_file_path))
-    aim_path = os.path.join(parent_folder_path,'rdp_usb')
-    os.chdir(aim_path)
-    current_path = os.getcwd()
-    print("Current Path:", current_path)
-    subprocess.run(['sh', './compile_pybind.sh'])
-    from tianmoucv.rdp_usb import rod_decoder_py as rdc
-    print('compile decoder successfully')
+    print("FATAL ERROR: no decoder found, please complie the decoder under ./rod_decoder_py")
 
 #用于重建
 #用于rgb的ISP
 from tianmoucv.proc.reconstruct import laplacian_blending
-from tianmoucv.isp import default_rgb_isp,fourdirection2xy,ACESToneMapping
+from tianmoucv.isp import default_rgb_isp,SD2XY,ACESToneMapping
 
 from ctypes import *
 
 flag = True
 
 class TianmoucDataReader_basic():
     '''
@@ -376,16 +366,18 @@
 
     def packRead(self,idx,key,ifSync =True, needPreProcess = True):
         '''
         use the decoder and isp preprocess to generate a paired (RGB,n*TSD) sample dict:
         
             sample['tsdiff_160x320'] = RAW TSD data ajusted to coorect space(with hollow)
             sample['tsdiff'] = TSD data upsample to 320*640
+            sample['F0_raw'] = unprocessed raw data, 320*320, t=t_0
+            sample['F1_raw'] = unprocessed raw data, 320*320, t=t_0
             sample['F0_without_isp'] = only demosaced frame data, 3*320*640, t=t_0
-            sample['F1_without_isp'] = only demosaced frame data, 3*320*640, t=t_0+33ms
+            sample['F1_without_isp'] = only demosaced frame data, 3*320*640, t=t_0
             sample['F0_HDR']: RGB+SD Blended HDR frame data, 3*320*640, t=t_0
             sample['F1_HDR']: RGB+SD Blended HDR frame data, 3*320*640, t=t_0+33ms
             sample['F0']: preprocessed frame data, 3*320*640, t=t_0
             sample['F1']: preprocessed frame data, 3*320*640, t=t_0+33ms
             sample['rawDiff']: raw TSD data, N*3*160*160, from t=t_0 to t=t+33ms
             sample['meta']: path infomation and and timestamps for each data
             sample['labels']: list of labels, if you have one
@@ -395,19 +387,19 @@
         metaInfo = dict([])
         legalSample = self.fileDict[key]['legalData'][idx]
         conefilename = self.fileDict[key][self.pathways[1]]
         rodfilename  = self.fileDict[key][self.pathways[0]]
         coneAddrs = legalSample[self.pathways[1]]
         rodAddrs = legalSample[self.pathways[0]]
         
-        start_frame,coneTimeStamp1 = self.readConeFast(conefilename,coneAddrs[0])
-        end_frame,coneTimeStamp2 = self.readConeFast(conefilename,coneAddrs[1])
+        start_frame_raw,coneTimeStamp1 = self.readConeFast(conefilename,coneAddrs[0])
+        end_frame_raw,coneTimeStamp2 = self.readConeFast(conefilename,coneAddrs[1])
         
-        start_frame = np.reshape(start_frame.astype(np.float32),(self.cone_height,self.cone_width))
-        end_frame = np.reshape(end_frame.astype(np.float32),(self.cone_height,self.cone_width))
+        start_frame_raw = np.reshape(start_frame_raw.astype(np.float32),(self.cone_height,self.cone_width))
+        end_frame_raw = np.reshape(end_frame_raw.astype(np.float32),(self.cone_height,self.cone_width))
         
         metaInfo['C_name'] = conefilename
         metaInfo['C_timestamp'] = (coneTimeStamp1.astype(np.int64),coneTimeStamp2.astype(np.int64))
         metaInfo['R_name'] = rodfilename
         metaInfo['R_timestamp'] = []
         metaInfo['key'] = key
         metaInfo['sample_length'] = len(self.fileDict[key]['legalData'])
@@ -431,37 +423,36 @@
             tsd[2,i,:,:] = torch.Tensor(sdr.astype(np.float32)).view(self.rod_height,self.rod_width)
             if i == 0:
                 SD_0 = tsd[1:,i,...]
             if i == itter - 1:
                 SD_1 = tsd[1:,i,...]
             
         if needPreProcess:
-            start_frame,end_frame,tsdiff_inter,F0_without_isp,F1_without_isp  = self.preprocess(start_frame,end_frame,tsd)
+            start_frame,end_frame,tsdiff_inter,F0_without_isp,F1_without_isp  = self.preprocess(start_frame_raw,end_frame_raw,tsd)
             sample['tsdiff_160x320'] = tsdiff_inter
             tsdiff_resized = F.interpolate(tsdiff_inter,(320,640),mode='bilinear')
             sample['tsdiff'] = tsdiff_resized
             sample['F0_without_isp'] = F0_without_isp
             sample['F1_without_isp'] = F1_without_isp
             sample['F0_HDR'] = self.HDRRecon(SD_0/128.0,start_frame)
             sample['F1_HDR'] = self.HDRRecon(SD_1/128.0,end_frame)
-    
         sample['F0'] = start_frame
         sample['F1'] = end_frame
         sample['rawDiff'] = tsd
         sample['meta'] = metaInfo
         sample['labels'] = legalSample['labels']
         sample['sysTimeStamp'] = legalSample['sysTimeStamp']
         return sample
     
     def HDRRecon(self,SD,F0):
         '''
         HDR fusion
         '''
         F0 = torch.Tensor(F0)
-        Ix,Iy= fourdirection2xy(SD)
+        Ix,Iy= SD2XY(SD)
         Ix = F.interpolate(torch.Tensor(Ix).unsqueeze(0).unsqueeze(0), size=(320,640), mode='bilinear').squeeze(0).squeeze(0)
         Iy = F.interpolate(torch.Tensor(Iy).unsqueeze(0).unsqueeze(0), size=(320,640), mode='bilinear').squeeze(0).squeeze(0)
         blend_hdr = laplacian_blending(-Ix,-Iy, srcimg=F0, iteration=20, mask_rgb=True, mask_th = 32)
         return blend_hdr
     
     def preprocess(self,F0_raw,F1_raw,tsdiff):
         '''
```

## tianmoucv/isp/isp_basic.py

```diff
@@ -1,11 +1,10 @@
 #基础的一些isp操作与可视化函数，有些和算法的效果绑定
 __author__ = 'Y. Lin'
 __authorEmail__ = '532109881@qq.com'
-import torch
 import numpy as np
 from scipy.signal import convolve2d
 import cv2
 
 ##############################
 #1. 图像基本处理
 #2. 可视化
@@ -408,15 +407,15 @@
     w = h = 0
     if len(diff.shape)==2:
         w,h = diff.shape
     else:
         diff = diff[...,0]
         w,h = diff.shape
         
-    rgb_diff = torch.ones([3,w,h]) * 255
+    rgb_diff = np.ones([3,w,h]) * 255
     diff[abs(diff)<thresh] = 0
     rgb_diff[0,...][diff>0] = 0
     rgb_diff[1,...][diff>0] = diff[diff>0]
     rgb_diff[2,...][diff>0] = diff[diff>0]
     rgb_diff[0,...][diff<0] = -diff[diff<0]
     rgb_diff[1,...][diff<0] = 0
     rgb_diff[2,...][diff<0] = -diff[diff<0]
```

## tianmoucv/isp/transform.py

```diff
@@ -1,52 +1,60 @@
 import torch
+import torch.nn.functional as F
+
 import numpy as np
 from scipy.spatial import ConvexHull
+from typing import Union
 
 # ===============================================================
 # SD坐标变换
 # ===============================================================
-#axis mapping
-#mapping ruls: tianmouc pixel pattern
-def fourdirection2xy(sd):
-    #print("warning: 0711version, Ix may be wrong direction")
-    if len(sd.shape)==4:
-        #input: [b,2,w,h]
-        #output: [b,2,w,h]
-        Ixy = torch.zeros(sd.shape).to(sd.device)
-        sdul = sd[:,0,0::2,...]
-        sdll = sd[:,0,1::2,...]
-        sdur = sd[:,1,0::2,...]
-        sdlr = sd[:,1,1::2,...]
-        Ixy[:,0, ::2,...] = Ixy[:,0,1::2,...] = ((sdul + sdll)/1.414 - (sdur + sdlr)/1.414)/2
-        Ixy[:,0,1::2,...] = Ixy[:,1, ::2,...] = ((sdur - sdlr)/1.414 + (sdul - sdll)/1.414)/2
-        return Ixy
-    else:
-        #input: [w,h,2]
-        #output: [w,h],[w,h]
-        if sd.shape[-1] == 2:
-            Ix = torch.zeros(sd.shape[:2]).to(sd.device)
-            Iy = torch.zeros(sd.shape[:2]).to(sd.device)
-            sdul = sd[0::2,...,0]
-            sdll = sd[1::2,...,0]
-            sdur = sd[0::2,...,1]
-            sdlr = sd[1::2,...,1]
-            Ix[::2,...] = Ix[1::2,...]= ((sdul + sdll)/1.414 - (sdur + sdlr)/1.414)/2
-            Iy[1::2,...]= Iy[::2,...] = ((sdur - sdlr)/1.414 + (sdul - sdll)/1.414)/2
-            return Ix,Iy
+def SD2XY(sd_raw:torch.tensor) -> torch.tensor:
+    '''
+    input: [h,w,2]/[2,h,w]/[n,2,h,w]
+    output: [h,2*w],[h,2*w] or [n,h,2*w],[n,h,2*w]
+    坐标变换规则参照http://www.tianmouc.cn:40000/tianmoucv/introduction.html
+    '''
+    if len(sd_raw.shape) == 3:
+        assert (sd_raw.shape[2]==2 or sd_raw.shape[0]==2)
+        if sd_raw.shape[2] == 2:
+            sd = sd_raw.permute(2,0,1).unsqueeze(0) #[h,w,c]->[1,c,h,w]
         else:
-            Ix = torch.zeros(sd.shape[1:]).to(sd.device)
-            Iy = torch.zeros(sd.shape[1:]).to(sd.device)
-            sdul = sd[0,0::2,...]
-            sdll = sd[0,1::2,...]
-            sdur = sd[1,0::2,...]
-            sdlr = sd[1,1::2,...]
-            Ix[::2,...] = Ix[1::2,...]= ((sdul + sdll)/1.414 - (sdur + sdlr)/1.414)/2
-            Iy[1::2,...]= Iy[::2,...] = ((sdur - sdlr)/1.414 + (sdul - sdll)/1.414)/2
-            return Ix,Iy
+            sd = sd_raw.unsqueeze(0)
+    else:
+        assert (len(sd_raw.shape) == 4 and sd_raw.shape[1]==2)
+        sd = sd_raw
+        
+    b,c,h,w = sd.shape
+    sdul = sd[:,0:1,0::2,...]
+    sdll = sd[:,0:1,1::2,...]
+    sdur = sd[:,1:2,0::2,...]
+    sdlr = sd[:,1:2,1::2,...]
+
+    target_size = (h,w*2)
+    sdul = F.interpolate(sdul, size=target_size, mode='bilinear', align_corners=False)
+    sdll = F.interpolate(sdll, size=target_size, mode='bilinear', align_corners=False)
+    sdur = F.interpolate(sdur, size=target_size, mode='bilinear', align_corners=False)
+    sdlr = F.interpolate(sdlr, size=target_size, mode='bilinear', align_corners=False)
+
+    sdx = ((sdul + sdll)/1.414 - (sdur + sdlr)/1.414)/2
+    sdy = ((sdur - sdlr)/1.414 + (sdul - sdll)/1.414)/2
+
+    if len(sd_raw.shape) == 3:
+        return sdx.squeeze(0).squeeze(0), sdy.squeeze(0).squeeze(0)
+    else:
+        return sdx.squeeze(1), sdy.squeeze(1)
+
+
+# ===============================================================
+# SD坐标变换
+# ===============================================================
+def fourdirection2xy(sd: Union[np.array,torch.tensor]) -> Union[np.array,torch.tensor]:
+    print('fourdirection2xy is decrepted, please use SD2XY')
+    return SD2XY(sd)
 
 # ===============================================================
 # 自卷积
 # ===============================================================
 def selfConv(img,clv_w = 11):
     from scipy import ndimage
     cov_len = clv_w # 尽量选取比较大的卷积核
```

## tianmoucv/nn/__init__.py

```diff
@@ -1,2 +1,2 @@
 from tianmoucv.proc.nn import *
-print("用以兼容旧版本")
+print("tianmoucv.nn is decrepted, please use tianmoucv.proc.nn")
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## tianmoucv/proc/features/__init__.py

```diff
@@ -1,3 +1,3 @@
-from . import diff
+from .diff import HarrisCorner,sift,hog,TomasiCorner,sobel_operator
 
 #这个包里是特征相关的
```

## tianmoucv/proc/features/diff.py

```diff
@@ -1,22 +1,33 @@
-import torch.nn.functional as F
-import torch.nn as nn
+import cv2
+import sys
+
 import numpy as np
+from scipy.optimize import linear_sum_assignment
 from scipy import signal
 from PIL import Image
-from tianmoucv import *
-import cv2
-import sys
-import torch.nn.functional as F
+
 import torch
-from scipy.optimize import linear_sum_assignment
+import torch.nn.functional as F
+import torch.nn as nn
+
+
+#===============================================================
+# sobel
+# ===============================================================
+def sobel_operator(Ix, Iy):
+    # 计算梯度幅值
+    magnitude = torch.sqrt(torch.pow(Ix, 2) + torch.pow(Iy, 2))
+    # 计算梯度角度
+    angle = torch.atan2(Iy, Ix)
+    return magnitude, angle
 
 
 # ===============================================================
-# 一些基本工具
+# 高斯卷积核
 # ===============================================================
 def gaussain_kernel(size=5,sigma=2):
     '''
     generate Gaussain blur kernel
     
     parameter:
         :param size: 特征的数量，int
@@ -29,14 +40,18 @@
     m = (size - 1) / 2
     y, x = torch.meshgrid(torch.arange(-m, m + 1), torch.arange(-m, m + 1))
     kernel = torch.exp(-(x * x + y * y) / (2 * sigma * sigma))
     kernel = kernel / torch.sum(kernel)
     kernel = kernel.unsqueeze(0).unsqueeze(0)
     return kernel
 
+
+# ===============================================================
+# 用现有高斯核做高斯模糊
+# ===============================================================
 def gaussian_smooth(inputTensor: torch.Tensor, kernel: torch.Tensor) -> torch.Tensor:
     '''
     用现有高斯核做高斯模糊
     
     parameter:
         :param inputTensor: 待处理矩阵,torch.Tensor
         :param kernel: 高斯模糊核,torch.Tensor
@@ -45,22 +60,19 @@
     '''
     padding_size = kernel.shape[-1] // 2
     input_padded = F.pad(inputTensor, (padding_size, padding_size, padding_size, padding_size), mode='reflect')
     kernel = kernel.to(inputTensor.device)
     return F.conv2d(input_padded, kernel, stride=1, padding=0)
 
 
-#=============================================================================================================================
-# 检测器
-#==============================================================================================================================
 
 # ===============================================================
 # Harris角点，用Ix和Iy做计算，可以用SD的两个方向
 # ===============================================================
-def HarrisCorner(Ix,Iy,k = 0.1,th = 0.5,size=5,sigma=1,nmsSize=11):
+def HarrisCorner(Ix:torch.Tensor,Iy:torch.Tensor,k = 0.1,th = 0.5,size=5,sigma=1,nmsSize=11):
     '''
     Harris 角点检测
     
     .. math:: R=det(H)−ktrace(H) 
     .. math:: \lambda1 + \lambda2 = \sum I_x^2 * \sum I_y^2 - \sum I_{xy} ^ 2
     .. math:: \lambda1 * \lambda2 = \sum I_{xy} ^ 2
     .. math:: R = det H - k trace H ^2= \lambda_1*\lambda_2 - k(\lambda_1 + \lambda_2)^ 2
@@ -94,20 +106,20 @@
     threshold =  float(torch.max(R)) * th
     R_Max = F.max_pool2d(R.unsqueeze(0).unsqueeze(0), kernel_size=nmsSize, 
                              stride=1, padding=nmsSize//2).squeeze(0).squeeze(0)
     idmap = (R >= threshold).int() * (R > R_Max-1e-5).int()
     R = R[idmap>0]
         
     return idmap,R
- 
+
+
 # ===============================================================
 # Shi-Tomasi角点，用Ix和Iy做计算，可以用SD的两个方向
 # ===============================================================
-
-def TomasiCorner(Ix, Iy, index=1000,size=5,sigma=2,nmsSize=11):
+def TomasiCorner(Ix:torch.Tensor, Iy:torch.Tensor, index=1000,size=5,sigma=2,nmsSize=11):
     '''
     Shi-Tomasi 角点检测
     在Harris角点检测的基础上，Shi和Tomasi 在1993的一篇论文《Good Features to track》中提出了基于Harris角点检测的Shi-Tomasi方法。
     经验参数需求更少，更快，但效果变差
     
     parameter:
         :param Ix: x方向梯度,[h,w],torch.Tensor
@@ -142,15 +154,15 @@
     idmap = (R >= threshold).int() * (R > R_Max-1e-5).int()
     return idmap,R
 
 
 # ===============================================================
 # ******in testing******  Harris3D角点，用Ix和Iy做计算，可以用SD的两个方向
 # ===============================================================
-def HarrisCorner3(Ix,Iy,It,k = 0.5,th = 0.95,size=5,sigma=2):
+def HarrisCorner3(Ix:torch.Tensor,Iy:torch.Tensor,It:torch.Tensor,k = 0.5,th = 0.95,size=5,sigma=2):
     '''
     Harris3D角点，用Ix和Iy做计算，可以用SD的两个方向
     
     如果小正方体沿z方向移动，那小正方体里的点云数量应该不变
     如果小正方体位于边缘上，则沿边缘移动，点云数量几乎不变，沿垂直边缘方向移动，点云数量改
     如果小正方体位于角点上，则有两个方向都会大幅改变点云数量
     拓展到3D中则使用法向量(包含法线和方向两个信息)
@@ -207,24 +219,20 @@
     R = detM - k * traceM ** 2
     threshold =  float(torch.max(R)) * th
     idmap = R >= threshold
     return idmap
 
 
 
-#=============================================================================================================================
-# 描述子
-#==============================================================================================================================
-
 
 #===============================================================
 # ******HOG****** 
 # ===============================================================
 
-def hog(Ix,Iy,kplist):
+def hog(Ix:torch.Tensor,Iy:torch.Tensor,kplist:list):
     '''
     hog 特征描述
     
     parameter:
         :param Ix: x方向梯度,[h,w],torch.Tensor
         :param Iy: y方向梯度,[h,w],torch.Tensor
         :param kplist: list of [x,y] 需要hog的坐标list, list
@@ -254,19 +262,20 @@
                 direction = cell_direction[i, j]
                 magnitude = cell_magnitude[i, j]
                 bin_index = int(num_bins * direction / (2 * math.pi))
                 histogram[bin_index] += magnitude
         discriptorList.append(histogram)
         goodkp.append(kp)
     return goodkp,discriptorList
-        
+
+
 #===============================================================
 # ******简化版SIFT中的描述子，缺少多尺度****** 
 # ===============================================================
-def sift(Ix,Iy, keypoints):
+def sift(Ix:torch.Tensor,Iy:torch.Tensor, keypoints:list):
     '''
     **简化版SIFT中的描述子，缺少多尺度**
     
     parameter:
         :param Ix: x方向梯度,[h,w],torch.Tensor
         :param Iy: y方向梯度,[h,w],torch.Tensor
         :param keypoints: list of [x,y] 需要sift的坐标list, list
@@ -279,16 +288,14 @@
     descriptors = []
     count = 0
     # 获取关键点坐标
     goofkp = []
     for kp in keypoints:
         descriptorlist = []
         y, x = int(kp[0]), int(kp[1])
-        
-        #0831晚上修改：可能要取平均一下，噪声影响大，主方向比较重要
         Xneighbor = Ix[y-1:y+2,x-1:x+2]
         Yneighbor = Iy[y-1:y+2,x-1:x+2]
         mask = (Xneighbor!=0) & (Yneighbor!=0)
         magnitude, majorAngle = cv2.cartToPolar(Xneighbor[mask],Yneighbor[mask], angleInDegrees=True)
         if majorAngle is None:
             continue
         majorAngle = np.mean(majorAngle)
@@ -311,9 +318,8 @@
                 hist, _ = np.histogram(angle-majorAngle, bins=4, range=(0, 360), weights=magnitude)
                 descriptorlist.append(torch.Tensor(hist))
         if(len(descriptorlist)>0):
             descriptors.append(torch.stack(descriptorlist,dim=0))
             count += 1
             goofkp.append(kp)
 
-    #print(descriptors)
     return goofkp,descriptors
```

## tianmoucv/proc/nn/unet_modules.py

```diff
@@ -130,105 +130,26 @@
         x = torch.cat([x2, x1], dim=1)
         x,_ = self.cbam(x)
         x = self.conv2(x)
         
         return x
 
 
-##Training Weakly Supervised Video Frame Interpolation with Events(accepted by ICCV2021)
-class FuseLayer(nn.Module):
-    def __init__(self, c_h, c_ef,debug=False):
-        super(FuseLayer, self).__init__()
-        self.c_ef = c_ef
-        self.c_h = c_h
-        self.debug = debug
-
-        self.convE1 = nn.Conv2d(c_ef, c_h, kernel_size=1, stride=1, padding=0, bias=True)
-        self.convE2 = nn.Conv2d(c_ef, c_h, kernel_size=1, stride=1, padding=0, bias=True)
-
-        self.convF1 = nn.Conv2d(c_ef, c_h, kernel_size=1, stride=1, padding=0, bias=True)
-        self.convF2 = nn.Conv2d(c_ef, c_h, kernel_size=1, stride=1, padding=0, bias=True)
-
-        self.convMask = nn.Sequential(nn.ReplicationPad2d(padding=[1, 1, 1, 1]),
-                                      nn.Conv2d(c_h, 1, kernel_size=3, stride=1, bias=False),
-                                      nn.Sigmoid())
-
-    def forward(self, h, z_e, z_f,h_prompt=None):
-        gammaE = self.convE1(z_e)
-        betaE = self.convE2(z_e)
-        E = gammaE * h + betaE
-
-        gammaF = self.convF1(z_f)
-        betaF = self.convF2(z_f)
-        F = gammaF * h + betaF
-
-        if h_prompt is None:
-            h_prompt = h 
-            
-        M = self.convMask(h_prompt)
-        out = M * E + (1.0 - M) * F
-
-        if self.debug:
-            print('>>>recon rate:',float(torch.mean(M)),' flow rate:',float(torch.mean(1-M)),float(torch.std(M)) )
-            # tmp_mask_attn_img = torch.mean(M,dim=1).detach().cpu().numpy()[0,...] * 255
-            # cv2.imwrite('./tmp.png',tmp_mask_attn_img.astype(np.uint8))
-        return out,M
-
 
 class Interp(nn.Module):
     def __init__(self, scale=None, size=None):
         super(Interp, self).__init__()
         self.scale = scale
         self.size = size
 
     def forward(self, x):
         y = F.interpolate(x, self.size, self.scale, mode='bilinear', align_corners=True)
         return y
 
-    
-class FuseBlock(nn.Module):
-    def __init__(self, cin, cout, c_ef,debug=False):
-        super(FuseBlock, self).__init__()
-        self.cin = cin
-        self.cout = cout
-        self.debug = debug
-
-        self.AAD1 = FuseLayer(cin, c_ef,self.debug)
-        self.conv1 = nn.Sequential(
-            nn.LeakyReLU(negative_slope=0.2, inplace=True),
-            nn.ReplicationPad2d(padding=[1, 1, 1, 1]),
-            nn.Conv2d(cin, cin, kernel_size=3, stride=1, bias=True)
-        )
-
-        self.AAD2 = FuseLayer(cin, c_ef)
-        self.conv2 = nn.Sequential(
-            nn.LeakyReLU(negative_slope=0.2, inplace=True),
-            nn.ReplicationPad2d(padding=[1, 1, 1, 1]),
-            nn.Conv2d(cin, cout, kernel_size=3, stride=1, bias=True)
-        )
-
-        if cin != cout:
-            self.AAD3 = FuseLayer(cin, c_ef)
-            self.conv3 = nn.Sequential(
-                nn.LeakyReLU(negative_slope=0.2, inplace=True),
-                nn.ReplicationPad2d(padding=[1, 1, 1, 1]),
-                nn.Conv2d(cin, cout, kernel_size=3, stride=1, bias=True)
-            )
-
-    def forward(self, h, z_e,z_f,h_prompt = None):
-        x,M = self.AAD1(h,z_e,z_f,h_prompt)
-        x = self.conv1(x)
-        x,M = self.AAD2(x,z_e,z_f,h_prompt)
-        x = self.conv2(x)
-        if self.cin != self.cout:
-            h,M = self.AAD3(h,z_e,z_f,h_prompt)
-            h = self.conv3(h)
-        x = x + h
-        return x,M
-    
+
 
 #############################
 #  @simpleNN
 #############################
 class UNetRecon(nn.Module):
     def __init__(self, inChannels, outChannels):
         super().__init__()
```

## tianmoucv/proc/opticalflow/__init__.py

```diff
@@ -1,3 +1,3 @@
-from . import estimator
-from . import spy_net
-from .basic import *
+from .estimator import LK_optical_flow,HS_optical_flow
+from .spy_net import TianmoucOF_SpyNet
+from .basic import interpolate_image,flow_to_image,backWarp,opticalDetector_Maxone
```

## tianmoucv/proc/opticalflow/basic.py

```diff
@@ -2,15 +2,15 @@
 import numpy as np
 import torch
 import torch.nn as nn
 
 # ===============================================================
 # 用算出的光流插帧
 # =============================================================== 
-def interpolate_image(image, u,v):
+def interpolate_image(image:np.array, u: torch.Tensor,v: torch.Tensor):
     '''
     用算出的光流插帧
 
     parameter:
         :param image: [h,w,3],np.array
         :param u: x向光流,[h,w],np.array
         :param v: y向光流,[h,w],np.array
@@ -27,15 +27,15 @@
                                    interpolation=cv2.INTER_LINEAR)
     return interpolated_image
 
 
 # ===============================================================
 # 可视化光流
 # ===============================================================
-def compute_color(u, v):
+def compute_color(u:np.array, v:np.array):
     """
     compute optical flow color map
     :param u: optical flow horizontal map
     :param v: optical flow vertical map
     :return: optical flow in color code
     """
     [h, w] = u.shape
@@ -105,15 +105,15 @@
     colorwheel[col:col+MR, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, MR) / MR))
     colorwheel[col:col+MR, 0] = 255
     return colorwheel
 
 # ===============================================================
 # 光流uv to RGB
 # ===============================================================
-def flow_to_image(flow):
+def flow_to_image(flow:np.array):
     """
     Convert flow into middlebury color code image
     :param flow: optical flow map
     :return: optical flow image in middlebury color
     """
     u = flow[:, :, 0]
     v = flow[:, :, 1]
@@ -163,15 +163,15 @@
         # create a grid
         gridX, gridY = np.meshgrid(np.arange(W), np.arange(H))
         self.W = W
         self.H = H
         self.gridX = torch.tensor(gridX, requires_grad=False, device=device)
         self.gridY = torch.tensor(gridY, requires_grad=False, device=device)
         
-    def forward(self, img, flow):
+    def forward(self, img:torch.Tensor, flow:torch.Tensor):
         # uv有奇怪的偏移
         MAGIC_NUM =  0.5
         # Extract horizontal and vertical flows.
         self.W = flow.size(3)
         self.H = flow.size(2)
         u = flow[:, 0, :, :]
         v = flow[:, 1, :, :]
@@ -193,22 +193,20 @@
     
     def __init__(self,noiseThresh=8,distanceThresh=0.2):
         self.noiseThresh = noiseThresh
         self.th = distanceThresh
         self.accumU = 0
         self.accumV = 0
         
-    def __call__(self,sd,td,ifInterploted = False):
+    def __call__(self,sd:torch.Tensor,td:torch.Tensor,ifInterploted = False):
         
         td[abs(td)<self.noiseThresh] = 0
         sd[abs(sd)<self.noiseThresh] = 0
-       
-        #rawflow = cal_optical_flow(sd,td,win=7,stride=3,mask=None,ifInterploted = ifInterploted)
-        #rawflow = recurrentOF(sd,td,ifInterploted = ifInterploted)
-        rawflow = recurrentMultiScaleOF(sd,td,ifInterploted = ifInterploted)
+
+        rawflow = HS_optical_flow(sd,td,ifInterploted = ifInterploted)
         
         flow = flow_to_image(rawflow.permute(1,2,0).numpy())
         
         flowup = np.zeros([flow.shape[0]*2,flow.shape[1]*2,3])
         flowup[1::2,1::2,:] = flow/255.0
         flowup[0::2,1::2,:] = flow/255.0
         flowup[1::2,0::2,:] = flow/255.0
```

## tianmoucv/proc/opticalflow/estimator.py

```diff
@@ -5,30 +5,30 @@
 import torch.nn.functional as F
 import torch
 import time
 
 from .basic import *
 from tianmoucv.isp import *
 
-# ===============================================================
-# LK方法计算稠密光流 
-# ===============================================================
-def local_norm(Diff):
+def local_norm(Diff: torch.Tensor) -> torch.Tensor:
     '''
     梯度归一化
     
     parameter:
         :param SD: 待归一化项
 
     '''
-
     grad_norm = (Diff[0,...]**2 + Diff[1,...]**2 + 1e-18)**0.5 + 1e-9
     return Diff / torch.max(grad_norm)
-        
-def cal_optical_flow(SD,TD,win=5,stride=0,mask=None,ifInterploted = False):    
+    
+# ===============================================================
+# LK方法计算稠密光流 
+# ===============================================================
+def LK_optical_flow(SD: torch.Tensor ,TD: torch.Tensor, win=5,
+                    stride=0,mask=None,ifInterploted = False) -> torch.Tensor:    
     '''
     LK方法计算稠密光流
     
     .. math:: [dx,dy]*[dI/dx,dI/dy]^T + dI/dt = 0
 
     parameter:
         :param SD: 原始SD，SD[0,1]: x,y方向上的梯度,[2,h,w],torch.Tensor
@@ -37,14 +37,18 @@
         :param stride=0: 取邻域做最小二乘,计算步长
         :param mask=None: 特征点tensor,binary Tensor,[h,w]
         :param ifInterploted = False: 计算结果是否与COP等大
 
     '''
     I = SD.size(-2)
     J = SD.size(-1)
+    
+    SD = SD.cpu()
+    TD = TD.cpu()
+    
     i_step  = win//2
     j_step  = win//2
     if stride == 0:
         stride =  win//2
     flow = torch.zeros([2,I//stride,J//stride])
     
     #加权
@@ -83,83 +87,22 @@
                 pass
     if not ifInterploted:
         flow = F.interpolate(flow.unsqueeze(0), size=(I,J*2), mode='bilinear').squeeze(0)
     else:
         flow = F.interpolate(flow.unsqueeze(0), size=(I,J), mode='bilinear').squeeze(0)
     return flow
 
-# ===============================================================
-# HS方法计算稠密光流，效果更好
-# ===============================================================
-def recurrentOF(SD,TD,ifInterploted = False):
-    '''
-    HS方法计算稠密光流
-    ![referemce](https://kns.cnki.net/kcms2/article/abstract?v=3uoqIhG8C475KOm_zrgu4h_jQYuCnj_co8vp4jCXSivDpWurecxFtEV8HAD0GySfgFWAxYnv5c-oQfA7zWjworscSCTy1fWb&uniplatform=NZKPT)
-    
-    .. math:: Energy_term = (u+v+\lambda)^2?
-    .. math:: newu = u - Ix * (Ix*u + Iy * v + It) / (\lambda*\lambda + Ix*Ix + Iy*Iy)
-    .. math:: newv = v - Iy * (Ix*u + Iy * v + It) / (\lambda*\lambda + Ix*Ix + Iy*Iy)
-
-    parameter:
-        :param SD: 原始SD，SD[0,1]: x,y方向上的梯度,[2,h,w],torch.Tensor
-        :param TD: 原始SD，TD[0]: t方向上的梯度,[1,h,w],torch.Tensor
-        :param ifInterploted = False: 计算结果是否与COP等大
-
-    '''
-    epsilon = 1e-8
-    maxIteration = 50
-    
-    def uitter(u,v,Ix,Iy,It,lambdaL):
-        newu = u - Ix * (Ix*u + Iy * v + It) / (lambdaL*lambdaL + Ix*Ix + Iy*Iy)
-        return newu
-    def vitter(u,v,Ix,Iy,It,lambdaL):
-        newv = v - Iy * (Ix*u + Iy * v + It) / (lambdaL*lambdaL + Ix*Ix + Iy*Iy)
-        return newv
-        
-    uitter_vector = np.vectorize(uitter)
-    vitter_vector = np.vectorize(vitter)
-        
-    I = SD.size(-2)
-    J = SD.size(-1)
-    
-    #加权
-    Ix = SD[0,...].numpy()
-    Iy = SD[1,...].numpy()
-    It = TD[0,...].numpy()
-    
-    u = np.zeros([I,J])
-    v = np.zeros([I,J])
-
-    lambdaL = np.ones([I,J])
-    
-    for it in range(maxIteration):
-        u_new = uitter_vector(u,v,Ix,Iy,It,lambdaL)
-        v_new = vitter_vector(u,v,Ix,Iy,It,lambdaL)
-        erroru = abs(u_new-u)
-        errorv = abs(v_new-v)
-        u = u_new
-        v = v_new
-        if np.max(erroru) < epsilon and np.max(errorv) < epsilon:
-            break
-    flow = torch.stack([torch.FloatTensor(u),torch.FloatTensor(v)],dim=0)
-    
-    if not ifInterploted:
-        flow = F.interpolate(flow.unsqueeze(0), size=(I,J*2), mode='bilinear').squeeze(0)
-    else:
-        flow = F.interpolate(flow.unsqueeze(0), size=(I,J), mode='bilinear').squeeze(0)
-    return flow
-
 
 # ===============================================================
 # 多尺度HS方法计算稠密光流，效果更好
 # ===============================================================
-def recurrentMultiScaleOF(SD,TD,ifInterploted = False,epsilon = 1e-8,maxIteration = 50,scales = 4,labmda=10):
+def HS_optical_flow(SD: torch.Tensor,TD: torch.Tensor,
+                    ifInterploted = False,epsilon = 1e-8,maxIteration = 50,scales = 4,labmda=10) -> torch.Tensor:    
     '''
     多尺度HS方法计算稠密光流，效果更好
-
     parameter:
         :param SD: 原始SD，SD[0,1]: x,y方向上的梯度,[2,h,w],torch.Tensor
         :param TD: 原始SD，TD[0]: t方向上的梯度,[1,h,w],torch.Tensor
         :param ifInterploted = False: 计算结果是否与COP等大
         :param epsilon = 1e-8: 收敛界
         :param maxIteration = 50: 最大迭代次数
         :param scales = 4: 尺度数量
```

## tianmoucv/proc/opticalflow/spy_net.py

```diff
@@ -7,29 +7,29 @@
 import time
 
 from .basic import *
 from tianmoucv.isp import *
 from tianmoucv.proc.nn.spy_modules import *
 from tianmoucv.tools import check_url_or_local_path,download_file
 
-class DenseOF_NN(nn.Module):
+class TianmoucOF_SpyNet(nn.Module):
     '''
     计算稠密光流的nn方法
     默认权重存储于'of_0918_ver_best.ckpt'
     或初始化时指定ckpt_path
     
     parameter:
     
     :param imgsize: (w,h),list
     :param ckpt\_path: string, path to weight dictionary
 
     '''
     #temp network
     def __init__(self,imgsize,ckpt_path = None):
-        super(DenseOF_NN, self).__init__()
+        super(TianmoucOF_SpyNet, self).__init__()
         current_dir=os.path.dirname(__file__)
         
         if ckpt_path is None:
             ckpt_path = 'https://cloud.tsinghua.edu.cn/f/84ac6e32060443e2975d/?dl=1'
         status = check_url_or_local_path(ckpt_path)
         print('loading..:',ckpt_path)
         if status == 1:
@@ -63,15 +63,15 @@
         main_version = int(torch.__version__[0])
         if main_version==2:
             print('compiling model for pytorch version>= 2.0.0')
             self.flowComp = torch.compile(self.flowComp)
             print('compiled!')
 
     @torch.no_grad() 
-    def forward_time_range(self, tsdiff,t1,t2,F0=None):
+    def forward_time_range(self, tsdiff: torch.Tensor,t1,t2,F0=None):
         '''
         Args:
           @tsdiff: [c,n,w,h], -1~1,torch，decoder的输出直接concate的结果
           
           @t1,t2 \in [0,n]  calculate the OF between t1-t2
           
         '''
```

## tianmoucv/proc/reconstruct/__init__.py

```diff
@@ -1,8 +1,9 @@
-from .basic import *
-from . import integration  # 点号代表下级，比如这里的意思就是导入Mypackage包下的me模块
-from . import tiny_unet  # 点号代表下级，比如这里的意思就是导入Mypackage包下的me模块
+#只在这里暴露接口,不暴露子包
+
+from .basic import laplacian_blending,batch_inference,laplacian_blending_1c_batch
+
+from .integration import TD_integration,SD_integration
+
+from .tiny_unet import TianmoucRecon_tiny
 
 
-# 严格按照“包名.子包名(无子包则不写).模块名.功能”语法使用
-# 这个包是重建相关的，module不暴露给外部
-# 自带一个轻量化的简单的重建网络
```

## tianmoucv/proc/reconstruct/basic.py

```diff
@@ -24,15 +24,15 @@
         lap_blend[1:-1,1:-1] = lap_blend_old_tmp + grad
         # Check for convergence
         if torch.sum(torch.abs(lap_blend - lap_blend_old)) < 0.1:
             return lap_blend
     # Return the blended image
     return lap_blend
 
-def laplacian_blending_1c_batch(Ix,Iy,gray,iteration=50):
+def laplacian_blending_1c_batch(Ix,Iy,gray=None,iteration=50):
     '''
     # 灰度重建-支持batch的网络训练用接口
     # vectorized by Y. Lin
     # Function to apply Poisson blending to two images
     '''
     if gray is None:
         gray = torch.zeros_like(Ix).to(Ix.device)
@@ -49,20 +49,18 @@
         lap_blend[:,0,1:-1,1:-1] = lap_blend_old_tmp + grad
         # Check for convergence
         if torch.sum(torch.abs(lap_blend - lap_blend_old)) < 0.1:
             return lap_blend
     # Return the blended image
     return lap_blend
 
-#兼容旧接口
-def poisson_blend(Ix,Iy,iteration=50):
-    return laplacian_blending_1c(Ix,Iy,None,iteration=iteration)
-
 def genMask(gray,th = 24, maxV=255, minV = 0):
-
+    '''
+    生成过欠曝区域遮罩
+    '''
     gap = maxV- minV
     mask_ts = ( (gray < (maxV-th)/gap) * (gray > (minV+th)/gap) ).float()
     mask_np = mask_ts.cpu().numpy()
     mask_np_b = (mask_np * gap).astype(np.uint8)
     kernel = np.ones((5,5),np.uint8) * gap
     kernel[0,4] = kernel[4,0] = kernel[4,4] = kernel[0,0] = 0
     mask_np_b = cv2.erode(mask_np_b,kernel,iterations = 2)
@@ -80,15 +78,15 @@
     :sciimg: [None],[h,w],[h,w,3]，分别进入不同模式
     '''
     if mask_rgb and not srcimg is None:
         mask = genMask(srcimg, th = mask_th, maxV=255, minV = 0)
     result = None
     
     if srcimg is None:
-        result = laplacian_blending_1c(Ix,Iy,iteration=iteration)
+        result = laplacian_blending_1c(Ix,Iy,gray=srcimg,iteration=iteration)
     elif len(srcimg.shape)==2:
         img = srcimg.clone()
         result = laplacian_blending_1c(Ix,Iy,img,iteration=iteration)
     elif len(srcimg.shape)==3:
         img = srcimg.clone()
         for c in range(img.shape[-1]):
             target = img[...,c]
@@ -169,8 +167,7 @@
                 Ft2  = model.forward_batch(F_batch, td_batch,SD0_batch,SD1_batch)
                 Ft1 = (Ft1+Ft2)/2
                 
             Ft_batch[biast:biast+res,...] = Ft1.clone()
 
         return Ft_batch
     
-
```

## tianmoucv/proc/reconstruct/integration.py

```diff
@@ -1,27 +1,26 @@
 import numpy as np
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import os
 import time
 
-from .basic import poisson_blend
-from tianmoucv.isp import fourdirection2xy,upsampleTSD
+from .basic import laplacian_blending
+from tianmoucv.isp import SD2XY,upsampleTSD
 
-
-def grayReconstructor(tsdiff,F0,F1,t, TD_BG_NOISE = 0, threshGate=4/255, dig_scaling= 1.5):
+def TD_integration(tsdiff,F0,F1,t, TD_BG_NOISE = 0, threshGate=4/255, dig_scaling= 1.5):
     '''
     AOP+COP合成灰度
     
     1. 校正TD的正向和负向差分的不一致性
     
     2. 计算AOP到COP的线性缩放系数
     
-    3. SD使用泊松blending合成灰度
+    3. laplacian_blending
     
     4. 双向TD积累+SD灰度合成最终结果
     
     parameter:
         :param F0: [h,w,3],torch.Tensor
         :param F0: [h,w,3],torch.Tensor
         :param tsdiff: [3,T,h,w],torch.Tensor, 默认decoded结果的堆积
@@ -51,27 +50,32 @@
     AOP_COP_scale_pos = torch.sum(TD_COP[TD_COP>0])/torch.sum(AOPDiff[AOPDiff>0]) 
 
     TD[TD<0] *= AOP_COP_scale_neg * dig_scaling
     TD[TD>0] *= AOP_COP_scale_pos  * dig_scaling
 
     forward_TD =  torch.sum(TD[0:t,...],dim=0)
     backward_TD =  torch.sum(TD[t:,...],dim=0)
-    #print(TD[0:t,...].shape,TD[t:,...].shape,TD.shape)
-    
-    '''
-    SDt = tsdiff[1:,t,...].permute(1,2,0) * (AOP_COP_scale_neg+AOP_COP_scale_pos)/2
-    Ix,Iy = fourdirection2xy(SDt)
-    gray = -poisson_blend(Ix,Iy,iteration=20)
-    gray = F.interpolate(gray.unsqueeze(0).unsqueeze(0), 
-                         size=(320,640), mode='bilinear').squeeze(0).squeeze(0)
-    '''
+
 
     forward_TD  = F.interpolate(forward_TD.unsqueeze(0).unsqueeze(0), size=(320,640), mode='bilinear').squeeze(0).squeeze(0)
     backward_TD = F.interpolate(backward_TD.unsqueeze(0).unsqueeze(0), size=(320,640), mode='bilinear').squeeze(0).squeeze(0)
     
     hdr =  (gray0+forward_TD + gray1 - backward_TD)/2
 
     #hdr[hdr<0]=0
     #hdr[hdr>1]=1
     
     return hdr
 
+
+def SD_integration(SDx:np.array, SDy:np.array)  -> np.array:
+    '''
+    SD直接积分累加重建，简单可视化用
+    use mapped SDx and SDy to conduct direct integration
+    '''
+    canvas = np.zeros([SDx.shape[0],SDx.shape[1]+1])
+    grayy = np.cumsum(SDy,axis=0)
+    goody_first = grayy[:,0]
+    canvas[:,0] = goody_first
+    canvas[:,1:] = SDx
+    gray = np.cumsum(canvas[:,:-1],axis=1)
+    return gray
```

## tianmoucv/proc/reconstruct/tiny_unet.py

```diff
@@ -1,29 +1,29 @@
 import numpy as np
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import os
 import time
 
-from .basic import poisson_blend
+from .basic import laplacian_blending
 
 from tianmoucv.tools import check_url_or_local_path,download_file
 from tianmoucv.proc.nn.unet_modules import UNetRecon
-from tianmoucv.isp import fourdirection2xy,upsampleTSD
+from tianmoucv.isp import upsampleTSD
 from tianmoucv.proc.nn.utils import tdiff_split
 
-class Reconstrutor_NN(nn.Module):
+class TianmoucRecon_tiny(nn.Module):
     '''
     重建网络
     权重链接:https://cloud.tsinghua.edu.cn/f/2baddb35cc034d31956e/?dl=1
     old:https://cloud.tsinghua.edu.cn/f/9d4adcfa7f0245959747/?dl=1
     '''
     def __init__(self,ckpt_path =None,_optim=True):
-        super(Reconstrutor_NN, self).__init__()
+        super(TianmoucRecon_tiny, self).__init__()
         current_dir=os.path.dirname(__file__)
         
         if ckpt_path is None:
             ckpt_path = 'https://cloud.tsinghua.edu.cn/f/2baddb35cc034d31956e/?dl=1'
         self.reconNet =  UNetRecon(7, 3)
         status = check_url_or_local_path(ckpt_path)
         print('loading..:',ckpt_path)
```

## tianmoucv/proc/tracking/__init__.py

```diff
@@ -1 +1 @@
-from .feature_tracker import *
+from .feature_tracker import feature_matching,mini_l2_cost_matching,align_images
```

## tianmoucv/rdp_usb/compile_pybind.sh

```diff
@@ -1,3 +1,7 @@
 #!/bin/bash
-which python3
-g++ -O2  -shared -std=c++17 -fPIC `python3 -m pybind11 --includes` rod_decoder_py.cpp -o rod_decoder_py`python3-config --extension-suffix`
+PYTHON_PATH=$(which python)
+PYTHON_VERSION=$(python -c 'import sys; print(sys.version_info.minor)')
+# 根据Python解释器的路径获取site-packages目录
+SITE_PACKAGES_PATH=$(dirname $(dirname $(echo $PYTHON_PATH)))'/lib'
+
+g++ -O2 -shared -std=c++17 -fPIC `python3 -m pybind11 --includes` -L$SITE_PACKAGES_PATH -lpython3'.'$PYTHON_VERSION rod_decoder_py.cpp -o rod_decoder_py`python3-config --extension-suffix`
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## Comparing `tianmoucv-0.3.0.dist-info/LICENSE` & `tianmoucv-0.3.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tianmoucv-0.3.0.dist-info/METADATA` & `tianmoucv-0.3.1.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tianmoucv
-Version: 0.3.0
+Version: 0.3.1
 Summary: Algorithms library for Tianmouc sensor
 Home-page: https://github.com/Tianmouc/tianmoucv
 Author: Yihan Lin,Taoyi Wang
 Author-email: 532109881@qq.com
 Keywords: tianmoucv
 Classifier: Programming Language :: Python :: 3 :: Only
 Classifier: License :: OSI Approved :: MIT License
@@ -30,15 +30,15 @@
 
 ![usbmodule](/resources/usb_module.jpg)
 
 **The official version will be available at [tianmoucv/tianmocv](https://github.com/Tianmouc/tianmoucv)**
 
 This is the Python tool for the first complementary vision sensor (CVS), TianMouC.
 
-More details about the project can be found on our project page. [doc](http://www.tianmouc.cn:38325)
+More details about the project can be found on our project page. [Tianmouc Project](https://www.cbicr.tsinghua.edu.cn/?page_id=971)
 
 ## Installation
 
 (0) Prepare pytorch environment
 
 **Python version should be larger than 3.8 and less than 3.12, recommend 3.10**
```

## Comparing `tianmoucv-0.3.0.dist-info/RECORD` & `tianmoucv-0.3.1.dist-info/RECORD`

 * *Files 26% similar despite different names*

```diff
@@ -1,40 +1,49 @@
-tianmoucv/__init__.py,sha256=qFCiuWf1zoDCQxV6ZUP4C0LTqeIJRlwIgGraC-6CgRY,341
+tianmoucv/__init__.py,sha256=Wl3hSUN0jn_n2CNNPUpEc4AkUKz81Ea_vGD-q9Veup0,308
 tianmoucv/tools.py,sha256=P_sc--I3XBTABGkX3LBBUv7cLWfeEbivVFBEdsEYshg,994
-tianmoucv/data/__init__.py,sha256=OsKXlJ5C6NBC4_TUfjEwGrygn86y88JiR0jHldY2ZZk,44
-tianmoucv/data/tianmoucData.py,sha256=MeV4T41xQjNX7buIBFQ1-cDytESaOksX6mdCg47KuMw,12544
-tianmoucv/data/tianmoucData_basic.py,sha256=zsrZJz2i3Xv452v2zGQPZxi2E8U8G5fosWqGTvCkhNs,25536
+tianmoucv/data/__init__.py,sha256=Gdll5DfJKeYwkJuHw_u1pLLKiYn_QSmXOnOaCE6KO5g,99
+tianmoucv/data/tianmoucData.py,sha256=7vutt5-83CimJTqSBtzoPF9FBZX5-P2XZ99B-UfCQvM,12911
+tianmoucv/data/tianmoucData_basic.py,sha256=uEp0F72OCSV0cC4_BmGrvHc0xcjAvauSmTPgWiuR3N0,25238
+tianmoucv/data/tianmoucData_pcie.py,sha256=4pBnZfKvW8BTDKKI_Us9P6PawF9yPOHhkYbm5m7rHiA,25949
 tianmoucv/isp/__init__.py,sha256=ffC5gY5ouq6FBPwKrdl-Mx4a1TBAXh6AQ3YoNBHI5WE,49
-tianmoucv/isp/isp_basic.py,sha256=DnsGAhu0-O5okDpgbIbH6JjlM-FRj1qElqhAelMRCJI,15720
-tianmoucv/isp/transform.py,sha256=aLuTjFsVCxeo0li1LnoVZTY2dTiu5Jx4puQLqmKpa-Q,5423
-tianmoucv/nn/__init__.py,sha256=8ZnYDPqL_TpDe9AMNaRx5Ud9QmkQBCY8KWm67PxiPY4,62
+tianmoucv/isp/isp_basic.py,sha256=BbMHKH_OHvvIS7Qs8gLZQi-jTXlAsbK8zocvwZrxOuU,15704
+tianmoucv/isp/transform.py,sha256=_65L-5avS9Zy_l9BPoKokERiP1dMH9Iui51pxMVWaXI,5569
+tianmoucv/nn/__init__.py,sha256=LwLQY7kq5ZPB3SOk1dJCrh048YKKLofATmpV8TpdxyU,96
 tianmoucv/nn/unet_modules.py,sha256=OTuQyIklMRJ5udTCbmrp16fe5V-Jrw6K0dNj14SwFyg,44
 tianmoucv/proc/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-tianmoucv/proc/features/__init__.py,sha256=zdgpZDsH1-gfjS0Ve1DspyEeqCLCUA3QzTQvoQ31jas,51
-tianmoucv/proc/features/diff.py,sha256=8UHi-d4husgLCQXGIhvcxrf4omdosNWIFaj0K6-ox3g,12331
+tianmoucv/proc/features/__init__.py,sha256=dapR6YLjhivtcqKz4Wa2rjL2K0SDgWc39fXGiuqnm7g,100
+tianmoucv/proc/features/diff.py,sha256=pkuXURX3TWah4yfLWAE4tMtBSvovp_wcYyHW5D2priA,12268
 tianmoucv/proc/nn/__init__.py,sha256=yGj6rsmkoFfAHbJJKZf9PE-M4fk2l4qTnwn5hUu3K2E,48
 tianmoucv/proc/nn/spy_modules.py,sha256=7OqquWsd4qHXmAiK1L1s1rcC00vCHlVIEPhQmaZw5lE,5473
-tianmoucv/proc/nn/unet_modules.py,sha256=0scpI1b1AKcb31T7U0SiTpOYbLYXJYLzVUCss9rDNmo,9559
+tianmoucv/proc/nn/unet_modules.py,sha256=cKbLjvD0lXZ5RI5cWNstYGUOjUIEK_exqV-HT4etV7o,6552
 tianmoucv/proc/nn/utils.py,sha256=7uiFZJbceQNozj3c20UfX89goGETKJmLHTXgla0eW7c,9995
-tianmoucv/proc/opticalflow/__init__.py,sha256=CMhJCG8CRvbgFTDj9tsi2sHTNGUOaeX-GgyU_G9GCBo,67
-tianmoucv/proc/opticalflow/basic.py,sha256=sMFh9PDCH2wzXizXOky-KYbLWnVtrCk_UuGkcEtLhVQ,9037
-tianmoucv/proc/opticalflow/estimator.py,sha256=HtgJpKs1jBKsXpQGbyZE_BcKqj0_EAGFwtOMlf00iJ4,8041
-tianmoucv/proc/opticalflow/spy_net.py,sha256=yy4xs_svY8XrhrPXpuDS3kP5PgSg_36z23ISyEbcDdo,3178
-tianmoucv/proc/reconstruct/__init__.py,sha256=TPpc-eEAN51gK4lrRw6EDCDaZ6EysWfMVlZQItDuxZc,432
-tianmoucv/proc/reconstruct/basic.py,sha256=Qq7kXSdiejaNPi9j7CwAx95iJpr857pa08mXwYGUQFM,6943
-tianmoucv/proc/reconstruct/integration.py,sha256=koUjKZYxeTo6ryxpZT5xcx3ZCCAvlxrs1ep2XNIortE,2495
-tianmoucv/proc/reconstruct/tiny_unet.py,sha256=66Wk6WTvLZacRFQWKQkx-4rsTZu_LwwgneNpiBNvIRg,5099
-tianmoucv/proc/tracking/__init__.py,sha256=nqhY6h6W270MetyJGPC1r7YIPVrGo8yv5va-ZLcxZug,30
+tianmoucv/proc/opticalflow/__init__.py,sha256=yn60Sonr4M5BbXRV55ttHT72i5RNsCLKh1BBEPE2B3k,177
+tianmoucv/proc/opticalflow/basic.py,sha256=w093VKOl4_oVcFMlGspsGJpdmNIoXBBTHzMcUxPG2b0,8974
+tianmoucv/proc/opticalflow/estimator.py,sha256=3kgYmuPghBqfDTo__sCu7sAUZ7CW6vVnwcJI5j5s7Sw,6043
+tianmoucv/proc/opticalflow/spy_net.py,sha256=3ryvKdoIF7qJDokuMsTIzQ7PvE6jnYXjvtpPYUXuJUU,3206
+tianmoucv/proc/reconstruct/__init__.py,sha256=idwwACWZ2RHrYPwosAak2U-3ZFZOMcoID1eghQB58Ao,226
+tianmoucv/proc/reconstruct/basic.py,sha256=t1_MVTWjK9zDLEaWMXm3X_fFjhdUYwccEwu4wayd6n4,6884
+tianmoucv/proc/reconstruct/integration.py,sha256=OpY-FX07KR58uDx2j66auitKa_EV1krltKIGjht513o,2505
+tianmoucv/proc/reconstruct/tiny_unet.py,sha256=JGsg_8ch-15we0bCYQd_ZO1VKdfLVX-CgNynvCiTE5Q,5093
+tianmoucv/proc/tracking/__init__.py,sha256=rXQD56kSe1-16MjZVONDRG-_RACeluE50WxCUkO59sI,80
 tianmoucv/proc/tracking/feature_tracker.py,sha256=llMHRotmR-HnU_Ow4QsRP2FcPCQC__95M5VzWS3gBu0,3078
+tianmoucv/rdp_pcie/CMakeLists.txt,sha256=G0ulvvBvLG6rxq9dkwq0bwGnTijAD8ROqUO0PlnqyxM,150
+tianmoucv/rdp_pcie/ReadMe.md,sha256=39wZURwFlop-pB4shl_MPbE4GSqOINos6rUMeOiWTYU,283
+tianmoucv/rdp_pcie/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+tianmoucv/rdp_pcie/compile_pybind.bat,sha256=lQl42Zr8yY0YolIwskU2xq2YvVX_NSjaK1hRGBj9h-E,818
+tianmoucv/rdp_pcie/compile_pybind.sh,sha256=axL5AqzbfLwLZOb2rXMMRLNWag8tO9ozt9an4m3y8-c,427
+tianmoucv/rdp_pcie/lyncam_compact_data.cpp,sha256=fkr7S7Sut0uc_Tt60PkEWXgPBb706JE7xMUlWQvC7YA,2081
+tianmoucv/rdp_pcie/rod_compress.cpp,sha256=fkr7S7Sut0uc_Tt60PkEWXgPBb706JE7xMUlWQvC7YA,2081
+tianmoucv/rdp_pcie/rod_decoder_py.cpp,sha256=sALfYMezadpNoC7eMsPcNWv2lA6XVbOVw_VI9l7LKw4,31884
 tianmoucv/rdp_usb/CMakeLists.txt,sha256=G0ulvvBvLG6rxq9dkwq0bwGnTijAD8ROqUO0PlnqyxM,150
 tianmoucv/rdp_usb/ReadMe.md,sha256=39wZURwFlop-pB4shl_MPbE4GSqOINos6rUMeOiWTYU,283
 tianmoucv/rdp_usb/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tianmoucv/rdp_usb/compile_pybind.bat,sha256=lQl42Zr8yY0YolIwskU2xq2YvVX_NSjaK1hRGBj9h-E,818
-tianmoucv/rdp_usb/compile_pybind.sh,sha256=wgfANZLN7FuQ1J6NUJgTzKOiyNjy-Nt-ufMRHH9SFb8,165
+tianmoucv/rdp_usb/compile_pybind.sh,sha256=axL5AqzbfLwLZOb2rXMMRLNWag8tO9ozt9an4m3y8-c,427
 tianmoucv/rdp_usb/rod_decoder_py.cpp,sha256=zIzuNepeKVAvw5KIDqqFvTpcryxKbjsF5sicT_vKu4g,72985
 tianmoucv/rdp_usb/try_pcie2usb_conv.py,sha256=-6mB7yQNipbE4jhzU4c6fLT3Fm28d6MS6NtTrrH7fOg,677
 tianmoucv/rdp_usb/try_usb_data.py,sha256=_TU9O4ew-80bvYWp2rd_aCVRctAWU3iZFKrpLsVbRdI,2505
-tianmoucv-0.3.0.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
-tianmoucv-0.3.0.dist-info/METADATA,sha256=2HkFpkrd0gPTtmxA_rowMZiwqAHFj2qpLK4WjuvhQGg,2860
-tianmoucv-0.3.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-tianmoucv-0.3.0.dist-info/top_level.txt,sha256=G30zBqK-scBLtrBY5-F4Og4o4mRKBcrX9O2sLLUrXUE,10
-tianmoucv-0.3.0.dist-info/RECORD,,
+tianmoucv-0.3.1.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
+tianmoucv-0.3.1.dist-info/METADATA,sha256=nFX-NHt-5GZBtHIjmwgRfFjaDU2kGKslRo1slXZBA9A,2891
+tianmoucv-0.3.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+tianmoucv-0.3.1.dist-info/top_level.txt,sha256=G30zBqK-scBLtrBY5-F4Og4o4mRKBcrX9O2sLLUrXUE,10
+tianmoucv-0.3.1.dist-info/RECORD,,
```

