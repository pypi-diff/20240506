# Comparing `tmp/OmniGenome-0.0.3a0-py3-none-any.whl.zip` & `tmp/OmniGenome-0.0.4a0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,52 +1,52 @@
-Zip file size: 53483 bytes, number of entries: 50
--rw-rw-rw-  2.0 fat     3712 b- defN 24-May-02 13:19 omnigenome/__init__.py
+Zip file size: 53599 bytes, number of entries: 50
+-rw-rw-rw-  2.0 fat     3712 b- defN 24-May-05 22:56 omnigenome/__init__.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:24 omnigenome/bench/__init__.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-11 17:29 omnigenome/bench/auto_bench/__init__.py
--rw-rw-rw-  2.0 fat     7968 b- defN 24-May-02 13:20 omnigenome/bench/auto_bench/auto_bench.py
+-rw-rw-rw-  2.0 fat     7907 b- defN 24-May-05 22:56 omnigenome/bench/auto_bench/auto_bench.py
 -rw-rw-rw-  2.0 fat     7366 b- defN 24-Apr-29 14:11 omnigenome/bench/auto_bench/auto_bench_config.py
 -rw-rw-rw-  2.0 fat      971 b- defN 24-Apr-06 13:44 omnigenome/bench/auto_bench/config_check.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-11 17:29 omnigenome/bench/bench_hub/__init__.py
 -rw-rw-rw-  2.0 fat      439 b- defN 24-Apr-14 13:16 omnigenome/bench/bench_hub/bench_hub.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-27 14:21 omnigenome/src/__init__.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-06 13:21 omnigenome/src/abc/__init__.py
 -rw-rw-rw-  2.0 fat    12137 b- defN 24-May-02 13:20 omnigenome/src/abc/abstract_dataset.py
 -rw-rw-rw-  2.0 fat     1773 b- defN 24-Apr-13 10:46 omnigenome/src/abc/abstract_metric.py
--rw-rw-rw-  2.0 fat    13927 b- defN 24-May-01 20:17 omnigenome/src/abc/abstract_model.py
--rw-rw-rw-  2.0 fat     3284 b- defN 24-May-02 13:20 omnigenome/src/abc/abstract_tokenizer.py
+-rw-rw-rw-  2.0 fat    14817 b- defN 24-May-05 22:56 omnigenome/src/abc/abstract_model.py
+-rw-rw-rw-  2.0 fat     3272 b- defN 24-May-05 22:56 omnigenome/src/abc/abstract_tokenizer.py
 -rw-rw-rw-  2.0 fat      634 b- defN 24-Apr-29 14:11 omnigenome/src/dataset/__init__.py
--rw-rw-rw-  2.0 fat     8896 b- defN 24-Apr-30 20:16 omnigenome/src/dataset/omnigenome_dataset.py
+-rw-rw-rw-  2.0 fat     8896 b- defN 24-May-04 09:01 omnigenome/src/dataset/omnigenome_dataset.py
 -rw-rw-rw-  2.0 fat      493 b- defN 24-Apr-29 14:11 omnigenome/src/metric/__init__.py
 -rw-rw-rw-  2.0 fat     2819 b- defN 24-Apr-30 02:44 omnigenome/src/metric/classification_metric.py
 -rw-rw-rw-  2.0 fat     2424 b- defN 24-Apr-30 02:45 omnigenome/src/metric/ranking_metric.py
 -rw-rw-rw-  2.0 fat     2785 b- defN 24-Apr-30 02:45 omnigenome/src/metric/regression_metric.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:34 omnigenome/src/misc/__init__.py
--rw-rw-rw-  2.0 fat     6784 b- defN 24-Apr-29 14:11 omnigenome/src/misc/utils.py
+-rw-rw-rw-  2.0 fat     7178 b- defN 24-May-05 22:56 omnigenome/src/misc/utils.py
 -rw-rw-rw-  2.0 fat      470 b- defN 24-Apr-29 14:11 omnigenome/src/model/__init__.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 20:27 omnigenome/src/model/classiifcation/__init__.py
--rw-rw-rw-  2.0 fat    12353 b- defN 24-Apr-30 19:17 omnigenome/src/model/classiifcation/model.py
+-rw-rw-rw-  2.0 fat    10416 b- defN 24-May-05 22:56 omnigenome/src/model/classiifcation/model.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-10 12:33 omnigenome/src/model/mlm/__init__.py
--rw-rw-rw-  2.0 fat     4534 b- defN 24-May-01 20:25 omnigenome/src/model/mlm/model.py
+-rw-rw-rw-  2.0 fat     3916 b- defN 24-May-03 11:41 omnigenome/src/model/mlm/model.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 20:27 omnigenome/src/model/regression/__init__.py
--rw-rw-rw-  2.0 fat    10956 b- defN 24-Apr-30 19:44 omnigenome/src/model/regression/model.py
+-rw-rw-rw-  2.0 fat    10180 b- defN 24-May-03 11:28 omnigenome/src/model/regression/model.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 21:23 omnigenome/src/model/seq2seq/__init__.py
 -rw-rw-rw-  2.0 fat      607 b- defN 24-Apr-27 14:47 omnigenome/src/model/seq2seq/model.py
 -rw-rw-rw-  2.0 fat      512 b- defN 24-Apr-29 14:11 omnigenome/src/tokenizer/__init__.py
 -rw-rw-rw-  2.0 fat     2996 b- defN 24-Apr-29 14:11 omnigenome/src/tokenizer/bpe_tokenizer.py
 -rw-rw-rw-  2.0 fat     4598 b- defN 24-Apr-29 14:11 omnigenome/src/tokenizer/kmers_tokenizer.py
 -rw-rw-rw-  2.0 fat     3868 b- defN 24-May-02 13:20 omnigenome/src/tokenizer/single_nucleotide_tokenizer.py
 -rw-rw-rw-  2.0 fat      409 b- defN 24-Apr-14 10:45 omnigenome/src/trainer/__init__.py
 -rw-rw-rw-  2.0 fat     1092 b- defN 24-Apr-29 14:11 omnigenome/src/trainer/hf_trainer.py
--rw-rw-rw-  2.0 fat    10478 b- defN 24-May-02 13:20 omnigenome/src/trainer/trainer.py
+-rw-rw-rw-  2.0 fat    10664 b- defN 24-May-02 14:45 omnigenome/src/trainer/trainer.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:25 omnigenome/utility/__init__.py
 -rw-rw-rw-  2.0 fat     9554 b- defN 24-Apr-24 20:40 omnigenome/utility/ensemble.py
 -rw-rw-rw-  2.0 fat    10629 b- defN 24-Apr-29 14:11 omnigenome/utility/hub_utils.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-11 17:29 omnigenome/utility/model_hub/__init__.py
 -rw-rw-rw-  2.0 fat     2993 b- defN 24-Apr-27 21:32 omnigenome/utility/model_hub/model_hub.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-06 13:21 omnigenome/utility/pipeline_hub/__init__.py
 -rw-rw-rw-  2.0 fat     5603 b- defN 24-Apr-29 14:11 omnigenome/utility/pipeline_hub/pipeline.py
 -rw-rw-rw-  2.0 fat      883 b- defN 24-Apr-14 15:43 omnigenome/utility/pipeline_hub/pipeline_hub.py
--rw-rw-rw-  2.0 fat     2272 b- defN 24-May-02 13:20 OmniGenome-0.0.3a0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-May-02 13:20 OmniGenome-0.0.3a0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       11 b- defN 24-May-02 13:20 OmniGenome-0.0.3a0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     4655 b- defN 24-May-02 13:20 OmniGenome-0.0.3a0.dist-info/RECORD
-50 files, 169397 bytes uncompressed, 45891 bytes compressed:  72.9%
+-rw-rw-rw-  2.0 fat     2272 b- defN 24-May-05 22:56 OmniGenome-0.0.4a0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-May-05 22:56 OmniGenome-0.0.4a0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       11 b- defN 24-May-05 22:56 OmniGenome-0.0.4a0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     4655 b- defN 24-May-05 22:56 OmniGenome-0.0.4a0.dist-info/RECORD
+50 files, 167463 bytes uncompressed, 46007 bytes compressed:  72.5%
```

## zipnote {}

```diff
@@ -132,20 +132,20 @@
 
 Filename: omnigenome/utility/pipeline_hub/pipeline.py
 Comment: 
 
 Filename: omnigenome/utility/pipeline_hub/pipeline_hub.py
 Comment: 
 
-Filename: OmniGenome-0.0.3a0.dist-info/METADATA
+Filename: OmniGenome-0.0.4a0.dist-info/METADATA
 Comment: 
 
-Filename: OmniGenome-0.0.3a0.dist-info/WHEEL
+Filename: OmniGenome-0.0.4a0.dist-info/WHEEL
 Comment: 
 
-Filename: OmniGenome-0.0.3a0.dist-info/top_level.txt
+Filename: OmniGenome-0.0.4a0.dist-info/top_level.txt
 Comment: 
 
-Filename: OmniGenome-0.0.3a0.dist-info/RECORD
+Filename: OmniGenome-0.0.4a0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## omnigenome/__init__.py

```diff
@@ -4,15 +4,15 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 __name__ = "OmniGenome"
-__version__ = "0.0.3alpha"
+__version__ = "0.0.4alpha"
 __author__ = "YANG, HENG"
 __email__ = "yangheng2021@gmail.com"
 __license__ = "MIT"
 
 
 from .bench.auto_bench.auto_bench import AutoBench
 from .bench.auto_bench.auto_bench_config import AutoBenchConfig
```

## omnigenome/bench/auto_bench/auto_bench.py

```diff
@@ -13,42 +13,41 @@
 
 import autocuda
 import findfile
 import torch
 from metric_visualizer import MetricVisualizer
 
 from ...src.abc.abstract_tokenizer import OmniGenomeTokenizer
-from ...src.misc.utils import seed_everything, fprint
+from ...src.misc.utils import seed_everything, fprint, load_module_from_path
 from ...src.trainer.trainer import Trainer
 
 
 class AutoBench:
     def __init__(
         self, bench_root, model_name_or_path, tokenizer=None, device=None, **kwargs
     ):
         self.bench_root = bench_root.rstrip("/")
         self.model_name_or_path = model_name_or_path.rstrip("/")
         self.tokenizer = tokenizer.rstrip("/") if tokenizer else None
         self.device = device if device else autocuda.auto_cuda()
         self.overwrite = kwargs.pop("overwrite", False)
         # Import benchmark list
-        self.bench_metadata = importlib.import_module(f"{self.bench_root}.metadata")
+        self.bench_metadata = load_module_from_path(
+            f"bench_metadata", f"{self.bench_root}/metadata.py"
+        )
         fprint("Loaded benchmarks: ", self.bench_metadata.bench_list)
 
         self.mv_path = f"{self.bench_root}-{self.model_name_or_path}.mv".replace(
             "/", "-"
         )
+
+        self.mv = MetricVisualizer(f"{self.bench_root}-{self.model_name_or_path}")
         if os.path.exists(self.mv_path) and not self.overwrite:
             self.mv = MetricVisualizer.load(self.mv_path)
             self.mv.summary()
-        elif os.path.exists(self.mv_path) and self.overwrite:
-            os.remove(self.mv_path)
-            self.mv = MetricVisualizer(f"{self.bench_root}-{self.model_name_or_path}")
-        else:
-            self.mv = MetricVisualizer(f"{self.bench_root}-{self.model_name_or_path}")
 
         self.bench_info()
 
     def bench_info(self):
         info = f"Benchmark Root: {self.bench_root}\n"
         info += f"Benchmark List: {self.bench_metadata.bench_list}\n"
         info += f"Model Name or Path: {self.model_name_or_path}\n"
@@ -67,28 +66,25 @@
         """
 
         # Import benchmark config
         for bench in self.bench_metadata.bench_list:
             bench_config_path = findfile.find_file(
                 self.bench_root, f"{self.bench_root}.{bench}.config".split(".")
             )
-            config = importlib.import_module(
-                bench_config_path.rstrip("/").replace(os.sep, ".").replace(".py", "")
-            )
+            config = load_module_from_path("config", bench_config_path)
             bench_config = config.bench_config
 
             for key, value in kwargs.items():
                 if key in bench_config:
                     fprint(
                         "Override", key, "with", value, "according to the input kwargs"
                     )
-                    bench_config[key] = kwargs.pop(key)
-
-            if kwargs:
-                warnings.warn(f"Unused kwargs: {kwargs}")
+                    bench_config[key] = kwargs.get(key)
+                else:
+                    warnings.warn(f"Unused kwarg: {key}")
 
             fprint(
                 f"AutoBench Config for {bench}:",
                 "\n".join([f"{k}: {v}" for k, v in bench_config.items()]),
             )
 
             # Init Tokenizer and Model
@@ -182,15 +178,19 @@
                     if "loss_fn" in bench_config
                     else None,
                     compute_metrics=bench_config["compute_metrics"],
                     seed=seed,
                     device=self.device,
                 )
 
-                metrics = trainer.train()
+                metrics = trainer.train(
+                    autocast=bench_config["autocast"]
+                    if "autocast" in bench_config
+                    else False
+                )
 
                 for key, value in metrics["test"][-1].items():
                     self.mv.log(record_name, key, value)
                 fprint(metrics)
                 self.mv.summary(round=4)
                 self.mv.dump(self.mv_path)
                 del model, trainer, optimizer, train_loader, valid_loader, test_loader
```

## omnigenome/src/abc/abstract_model.py

```diff
@@ -205,23 +205,23 @@
         raise NotImplementedError(
             "The loss_function() function should be implemented for your model."
         )
 
     def set_loss_fn(self, loss_function):
         self.loss_fn = loss_function
 
-    def predict(self, inputs, **kwargs):
-        raise NotImplementedError(
-            "The predict() function should be implemented for your model."
-        )
-
-    def inference(self, inputs, **kwargs):
-        raise NotImplementedError(
-            "The inference() function should be implemented for your model."
-        )
+    def predict(self, sequence_or_inputs, **kwargs):
+        # Please implement the predict() function for your model
+        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)
+        return raw_outputs
+
+    def inference(self, sequence_or_inputs, **kwargs):
+        # Please implement the predict() function for your model
+        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)
+        return raw_outputs
 
     def forward(self, inputs):
         last_hidden_state = last_hidden_state_forward(self.model, inputs)
         last_hidden_state = self.dropout(last_hidden_state)
         last_hidden_state = self.activation(last_hidden_state)
         outputs = {"last_hidden_state": last_hidden_state}
         return outputs
@@ -330,14 +330,35 @@
         ) or (
             hasattr(self.config, "auto_map") and "CausalLM" in str(self.config.auto_map)
         ):
             return True
         else:
             return False
 
+    def _forward_from_raw_input(self, sequence_or_inputs, **kwargs):
+        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
+            sequence_or_inputs, dict
+        ):
+            inputs = self.tokenizer(
+                sequence_or_inputs,
+                padding=kwargs.pop("padding", "max_length"),
+                max_length=kwargs.pop("max_length", 512),
+                truncation=True,
+                return_tensors="pt",
+                **kwargs,
+            )
+        else:
+            inputs = sequence_or_inputs
+        inputs = inputs.to(self.model.device)
+
+        with torch.no_grad():
+            raw_outputs = self(inputs)
+            raw_outputs["inputs"] = inputs
+        return raw_outputs
+
     @staticmethod
     def from_pretrained(model_name_or_path, tokenizer, *args, **kwargs):
         config = kwargs.pop("config", None)
         if config is None:
             config = AutoConfig.from_pretrained(model_name_or_path, **kwargs)
         base_model = AutoModel.from_pretrained(model_name_or_path, **kwargs)
         if tokenizer is None:
```

## omnigenome/src/abc/abstract_tokenizer.py

```diff
@@ -6,15 +6,15 @@
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 import warnings
 
 from transformers import AutoTokenizer
 
-from ..misc.utils import env_meta_info
+from ..misc.utils import env_meta_info, load_module_from_path
 
 
 class OmniGenomeTokenizer:
     def __init__(self, base_tokenizer=None, max_length=512, **kwargs):
         self.metadata = env_meta_info()
 
         self.base_tokenizer = base_tokenizer
@@ -25,27 +25,23 @@
 
         self.u2t = kwargs.get("u2t", False)
         self.t2u = kwargs.get("t2u", False)
         self.add_whitespace = kwargs.get("add_whitespace", False)
 
     @staticmethod
     def from_pretrained(model_name_or_path, **kwargs):
-        import importlib
-
+        wrapper_path = f"{model_name_or_path.rstrip('/')}/omnigenome_wrapper.py"
         try:
-            wrapper_name = (
-                f"{model_name_or_path.rstrip('/')}.omnigenome_wrapper".replace("/", ".")
-            )
-            wrapper = importlib.import_module(wrapper_name)
+            wrapper = load_module_from_path("omnigenome_wrapper", wrapper_path)
             tokenizer = wrapper.Tokenizer(
                 AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)
             )
-        except ImportError:
+        except ImportError or FileNotFoundError as e:
             warnings.warn(
-                f"Cannot find the tokenizer wrapper from {wrapper_name},"
+                f"Cannot find the tokenizer wrapper from {wrapper_path},"
                 " using the default OmniGenomeTokenizer."
             )
             tokenizer = OmniGenomeTokenizer(
                 AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)
             )
         return tokenizer
```

## omnigenome/src/misc/utils.py

```diff
@@ -216,7 +216,19 @@
         ),
         *objects,
         sep=sep,
         end=end,
         file=file,
         flush=flush,
     )
+
+
+def load_module_from_path(module_name, file_path):
+    import importlib
+
+    spec = importlib.util.spec_from_file_location(module_name, file_path)
+    module = importlib.util.module_from_spec(spec)
+    try:
+        spec.loader.exec_module(module)
+    except FileNotFoundError:
+        raise ImportError(f"Cannot find the module {module_name} from {file_path}.")
+    return module
```

## omnigenome/src/model/classiifcation/model.py

```diff
@@ -32,70 +32,40 @@
         last_hidden_state = self.activation(last_hidden_state)
         logits = self.classifier(last_hidden_state)
         logits = self.softmax(logits)
         outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
-            sequence_or_inputs, dict
-        ):
-            inputs = self.tokenizer(
-                sequence_or_inputs,
-                padding=kwargs.pop("padding", "max_length"),
-                max_length=kwargs.pop("max_length", 512),
-                truncation=True,
-                return_tensors="pt",
-                **kwargs
-            )
-        else:
-            inputs = sequence_or_inputs
-
-        inputs = inputs.to(self.model.device)
-
-        with torch.no_grad():
-            outputs = self(inputs)
-        logits = outputs["logits"]
-        last_hidden_state = outputs["last_hidden_state"]
+        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)
+        logits = raw_outputs["logits"]
+        last_hidden_state = raw_outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i].argmax(dim=-1).detach().cpu().numpy())
 
         outputs = {
             "predictions": predictions,
             "logits": logits,
             "last_hidden_state": last_hidden_state,
         }
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
-        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
-            sequence_or_inputs, dict
-        ):
-            inputs = self.tokenizer(
-                sequence_or_inputs,
-                padding=kwargs.pop("padding", "max_length"),
-                max_length=kwargs.pop("max_length", 512),
-                truncation=True,
-                return_tensors="pt",
-                **kwargs
-            )
-        else:
-            inputs = sequence_or_inputs
-        inputs = inputs.to(self.model.device)
-
-        with torch.no_grad():
-            outputs = self(inputs)
-        logits = outputs["logits"][:, 1:-1:, :]
-        last_hidden_state = outputs["last_hidden_state"][:, 1:-1:, :]
+        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)
+        inputs = raw_outputs["inputs"]
+        logits = raw_outputs["logits"][:, 1:-1:, :]
+        last_hidden_state = raw_outputs["last_hidden_state"][:, 1:-1:, :]
 
         predictions = []
         for i in range(logits.shape[0]):
+            # Note that the first and last tokens are removed,
+            # and the length of outputs are calculated based on the tokenized inputs.
             i_logit = logits[i][
                 : inputs["input_ids"][i].ne(self.config.pad_token_id).sum(dim=-1)
             ]
             prediction = [
                 self.config.id2label.get(x.item(), "") for x in i_logit.argmax(dim=-1)
             ]
             predictions.append(prediction)
@@ -150,66 +120,36 @@
             logits = self.classifier(last_hidden_state)
             logits = self.softmax(logits)
 
         outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
-            sequence_or_inputs, dict
-        ):
-            inputs = self.tokenizer(
-                sequence_or_inputs,
-                padding=kwargs.pop("padding", "max_length"),
-                max_length=kwargs.pop("max_length", 512),
-                truncation=True,
-                return_tensors="pt",
-                **kwargs
-            )
-        else:
-            inputs = sequence_or_inputs
-        inputs = inputs.to(self.model.device)
+        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)
 
-        with torch.no_grad():
-            outputs = self(inputs)
-        logits = outputs["logits"]
-        last_hidden_state = outputs["last_hidden_state"]
+        logits = raw_outputs["logits"]
+        last_hidden_state = raw_outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i].argmax(dim=-1).item())
 
         outputs = {
             "predictions": predictions,
             "logits": logits,
             "last_hidden_state": last_hidden_state,
         }
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
-        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
-            sequence_or_inputs, dict
-        ):
-            inputs = self.tokenizer(
-                sequence_or_inputs,
-                padding=kwargs.pop("padding", "max_length"),
-                max_length=kwargs.pop("max_length", 512),
-                truncation=True,
-                return_tensors="pt",
-                **kwargs
-            )
-        else:
-            inputs = sequence_or_inputs
-        inputs = inputs.to(self.model.device)
+        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)
 
-        with torch.no_grad():
-            outputs = self(inputs)
-        logits = outputs["logits"]
-        last_hidden_state = outputs["last_hidden_state"]
+        logits = raw_outputs["logits"]
+        last_hidden_state = raw_outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(
                 self.config.id2label.get(logits[i].argmax(dim=-1).item(), "")
             )
 
@@ -306,27 +246,26 @@
         conv_output = self.conv1d(cat_last_hidden_state.transpose(1, 2)).transpose(1, 2)
 
         last_hidden_state = self.cat_layer(
             torch.cat([last_hidden_state, conv_output], dim=-1)
         )
         last_hidden_state = self.dropout(last_hidden_state)
         last_hidden_state = self.activation(last_hidden_state)
-        last_hidden_state = self.pooler(last_hidden_state)
         logits = self.classifier(last_hidden_state)
 
         if self._is_causal_lm():
             pad_token_id = getattr(self.config, "pad_token_id", -100)
             sequence_lengths = inputs["input_ids"].ne(pad_token_id).sum(dim=1) - 1
             logits = logits[
                 torch.arange(inputs["input_ids"].size(0), device=logits.device),
                 sequence_lengths,
             ]
         else:
+            last_hidden_state = self.pooler(last_hidden_state)
             logits = self.softmax(logits)
-            logits = self.pooler(logits)
 
         outputs = {
             "logits": logits,
             "last_hidden_state": last_hidden_state,
             "ss_last_hidden_state": ss_last_hidden_state,
         }
         return outputs
```

## omnigenome/src/model/mlm/model.py

```diff
@@ -41,26 +41,18 @@
             "loss": loss,
             "logits": logits,
             "last_hidden_state": last_hidden_state,
         }
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
-            sequence_or_inputs, dict
-        ):
-            inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
-        else:
-            inputs = sequence_or_inputs
-        inputs = inputs.to(self.model.device)
+        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)
 
-        with torch.no_grad():
-            outputs = self(inputs)
-        logits = outputs["logits"]
-        last_hidden_state = outputs["last_hidden_state"]
+        logits = raw_outputs["logits"]
+        last_hidden_state = raw_outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i].argmax(dim=-1).detach().cpu().numpy())
 
         if not isinstance(sequence_or_inputs, list):
             outputs = {
@@ -74,35 +66,26 @@
                 "logits": logits,
                 "last_hidden_state": last_hidden_state,
             }
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
-        inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
-        inputs = inputs.to(self.model.device)
+        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)
 
-        with torch.no_grad():
-            outputs = self(inputs)
-        logits = outputs["logits"][:, 1:-1:, :]
-        last_hidden_state = outputs["last_hidden_state"][:, 1:-1:, :]
+        inputs = raw_outputs["inputs"]
+        logits = raw_outputs["logits"][:, 1:-1:, :]
+        last_hidden_state = raw_outputs["last_hidden_state"][:, 1:-1:, :]
 
         predictions = []
         for i in range(logits.shape[0]):
             i_logits = logits[i][
                 : inputs["input_ids"][i].ne(self.tokenizer.pad_token_id).sum().item()
             ]
             prediction = self.tokenizer.decode(i_logits.argmax(dim=-1)).replace(" ", "")
-            if (
-                torch.sum(
-                    inputs["input_ids"][i] == self.tokenizer.convert_tokens_to_ids("U")
-                )
-                > 0
-            ):
-                prediction = prediction.replace("U", "T")
             predictions.append(list(prediction))
 
         if not isinstance(sequence_or_inputs, list):
             outputs = {
                 "predictions": predictions[0],
                 "logits": logits[0],
                 "last_hidden_state": last_hidden_state[0],
```

## omnigenome/src/model/regression/model.py

```diff
@@ -30,47 +30,37 @@
         last_hidden_state = self.dropout(last_hidden_state)
         last_hidden_state = self.activation(last_hidden_state)
         logits = self.classifier(last_hidden_state)
         outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
-            sequence_or_inputs, dict
-        ):
-            inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
-        else:
-            inputs = sequence_or_inputs
-        inputs = inputs.to(self.model.device)
+        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)
 
-        with torch.no_grad():
-            outputs = self(inputs)
-        logits = outputs["logits"]
-        last_hidden_state = outputs["last_hidden_state"]
+        logits = raw_outputs["logits"]
+        last_hidden_state = raw_outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i].detach().cpu().numpy())
 
         outputs = {
             "predictions": predictions,
             "logits": logits,
             "last_hidden_state": last_hidden_state,
         }
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
-        inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
-        inputs = inputs.to(self.model.device)
+        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)
 
-        with torch.no_grad():
-            outputs = self(inputs)
-        logits = outputs["logits"][:, 1:-1:, :]
-        last_hidden_state = outputs["last_hidden_state"][:, 1:-1:, :]
+        inputs = raw_outputs["inputs"]
+        logits = raw_outputs["logits"][:, 1:-1:, :]
+        last_hidden_state = raw_outputs["last_hidden_state"][:, 1:-1:, :]
 
         predictions = []
         for i in range(logits.shape[0]):
             i_logits = logits[i][
                 : inputs["input_ids"][i].ne(self.config.pad_token_id).sum().item()
             ]
             predictions.append(i_logits.detach().cpu().numpy())
@@ -133,47 +123,36 @@
             last_hidden_state = self.pooler(last_hidden_state)
             logits = self.classifier(last_hidden_state)
 
         outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
-            sequence_or_inputs, dict
-        ):
-            inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
-        else:
-            inputs = sequence_or_inputs
-        inputs = inputs.to(self.model.device)
+        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)
 
-        with torch.no_grad():
-            outputs = self(inputs)
-        logits = outputs["logits"]
-        last_hidden_state = outputs["last_hidden_state"]
+        logits = raw_outputs["logits"]
+        last_hidden_state = raw_outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i].cpu().numpy())
 
         outputs = {
             "predictions": predictions,
             "logits": logits,
             "last_hidden_state": last_hidden_state,
         }
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
-        inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
-        inputs = inputs.to(self.model.device)
+        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)
 
-        with torch.no_grad():
-            outputs = self(inputs)
-        logits = outputs["logits"]
-        last_hidden_state = outputs["last_hidden_state"]
+        logits = raw_outputs["logits"]
+        last_hidden_state = raw_outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i].cpu().numpy())
 
         if not isinstance(sequence_or_inputs, list):
             outputs = {
```

## omnigenome/src/trainer/trainer.py

```diff
@@ -15,45 +15,62 @@
 
 from ..misc.utils import env_meta_info, fprint, seed_everything
 
 import sklearn.metrics
 
 
 def _infer_optimization_direction(metrics, prev_metrics):
-    if "score" in list(prev_metrics[0].keys())[0]:
-        return "larger_is_better"
-    elif "precision" in list(prev_metrics[0].keys())[0]:
-        return "larger_is_better"
-    elif "loss" in list(prev_metrics[0].keys())[0]:
-        return "smaller_is_better"
-    elif "error" in list(prev_metrics[0].keys())[0]:
-        return "smaller_is_better"
-    else:
-        fprint(
-            "Cannot determine the optimization direction. Trying to infer from the metrics."
-        )
-        is_prev_increasing = np.mean(list(prev_metrics[0].values())[0]) < np.mean(
-            list(prev_metrics[-1].values())[0]
-        )
-        is_still_increasing = np.mean(list(prev_metrics[1].values())[0]) < np.mean(
-            list(metrics.values())[0]
-        )
-
-        if is_prev_increasing and is_still_increasing:
+    larger_is_better_metrics = [
+        "accuracy",
+        "f1",
+        "recall",
+        "precision",
+        "roc_auc",
+        "pr_auc",
+        "score",
+        # ...
+    ]
+    smaller_is_better_metrics = [
+        "loss",
+        "error",
+        "mse",
+        "mae",
+        "r2",
+        "distance",
+        # ...
+    ]
+    for metric in larger_is_better_metrics:
+        if metric in list(prev_metrics[0].keys())[0]:
             return "larger_is_better"
+    for metric in smaller_is_better_metrics:
+        if metric in list(prev_metrics[0].keys())[0]:
+            return "smaller_is_better"
 
-        is_prev_decreasing = np.mean(list(prev_metrics[0].values())[0]) > np.mean(
-            list(prev_metrics[-1].values())[0]
-        )
-        is_still_decreasing = np.mean(list(prev_metrics[1].values())[0]) > np.mean(
-            list(metrics.values())
-        )
+    fprint(
+        "Cannot determine the optimization direction. Trying to infer from the metrics."
+    )
+    is_prev_increasing = np.mean(list(prev_metrics[0].values())[0]) < np.mean(
+        list(prev_metrics[-1].values())[0]
+    )
+    is_still_increasing = np.mean(list(prev_metrics[1].values())[0]) < np.mean(
+        list(metrics.values())[0]
+    )
 
-        if is_prev_decreasing and is_still_decreasing:
-            return "smaller_is_better"
+    if is_prev_increasing and is_still_increasing:
+        return "larger_is_better"
+
+    is_prev_decreasing = np.mean(list(prev_metrics[0].values())[0]) > np.mean(
+        list(prev_metrics[-1].values())[0]
+    )
+    is_still_decreasing = np.mean(list(prev_metrics[1].values())[0]) > np.mean(
+        list(metrics.values())
+    )
+
+    if is_prev_decreasing and is_still_decreasing:
+        return "smaller_is_better"
 
 
 class Trainer:
     def __init__(
         self,
         model,
         train_loader: torch.utils.data.DataLoader = None,
```

## Comparing `OmniGenome-0.0.3a0.dist-info/METADATA` & `OmniGenome-0.0.4a0.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: OmniGenome
-Version: 0.0.3a0
+Version: 0.0.4a0
 Summary: OmniGenome: A comprehensive toolkit for genome analysis.
 Home-page: https://github.com/yangheng95/OmniGenome
 Author: Yang, Heng
 Author-email: hy345@exeter.ac.uk
 License: MIT
 Requires-Python: >=3.9
 Description-Content-Type: text/markdown
```

## Comparing `OmniGenome-0.0.3a0.dist-info/RECORD` & `OmniGenome-0.0.4a0.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,50 +1,50 @@
-omnigenome/__init__.py,sha256=nVHH2iXy0Ytqx-g_tv639QIzx3MZHiVaUHX4um5uRac,3712
+omnigenome/__init__.py,sha256=DqXv1sgxnycHq0XxlKk0d_rDMT6FUFcxo6ie4y1kErk,3712
 omnigenome/bench/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 omnigenome/bench/auto_bench/__init__.py,sha256=JEdZznP93R9CQ2zZCXPOIxN0OizNa2RNa30EDkRCz3o,342
-omnigenome/bench/auto_bench/auto_bench.py,sha256=ktjoa13nwNmwbQc52OAhmazpnxKIlR3HFSN3IKx8kcY,7968
+omnigenome/bench/auto_bench/auto_bench.py,sha256=s-VsRaeRhl635kyvJu5Ui8WIeFxPx1S_LkS3EaB4zec,7907
 omnigenome/bench/auto_bench/auto_bench_config.py,sha256=PN1B2EBJA43N5TApj-kvMPMuI02UDIH8oUKwDtDiWI4,7366
 omnigenome/bench/auto_bench/config_check.py,sha256=PWusimXDSLJyOivrVRMnMFolMI16ttAmdDSzb5_VrTE,971
 omnigenome/bench/bench_hub/__init__.py,sha256=JEdZznP93R9CQ2zZCXPOIxN0OizNa2RNa30EDkRCz3o,342
 omnigenome/bench/bench_hub/bench_hub.py,sha256=Up4JjqGgKH1jtYPRwvb4w1UpxE6W3FxlUEuUcEelyKI,439
 omnigenome/src/__init__.py,sha256=P_xAnv8ydAA49O2NeRBf7TEU78OZwGE5xEAT8V2FdoQ,342
 omnigenome/src/abc/__init__.py,sha256=ir0dYJyibYfFlZaRG4PzPgqNmhHReqrvFRg8KzmFc4w,342
 omnigenome/src/abc/abstract_dataset.py,sha256=yA2HX1nDrfKR6p9_B4DFBSC_BfbX6yywmSc3n7h1PTE,12137
 omnigenome/src/abc/abstract_metric.py,sha256=xGHNFEcnSwZcnanEAUs9__cNLPzgtN5r26xc6x2ehWU,1773
-omnigenome/src/abc/abstract_model.py,sha256=JaU-c4lvZ-WuAfVwMgHlmSIgC8b_NiIuejiWeO10xSM,13927
-omnigenome/src/abc/abstract_tokenizer.py,sha256=IRMZwy7og5aKd8M7XPEFc5CQyHnCIYj94tO6Vz-lPj8,3284
+omnigenome/src/abc/abstract_model.py,sha256=v2LNnqtKmNdYnM3xCK6UAMlLX6xpQPzdUGCx8t5tcMo,14817
+omnigenome/src/abc/abstract_tokenizer.py,sha256=zb-Ab0hCtrkgmoPUH5slII1DPihedLDMGE9WbXWx2II,3272
 omnigenome/src/dataset/__init__.py,sha256=GI5cNvFQyAGjlPtytIt3--nIcz-ZBQQZRAcf4tsHLkw,634
 omnigenome/src/dataset/omnigenome_dataset.py,sha256=PQG_t89atEAZQWDQt4022UdrTaqU9g9Kri9HTM-7h2M,8896
 omnigenome/src/metric/__init__.py,sha256=9xXOdKCh1TtV0uwPW759-7pWwyT_pDU7GeSrKuztpYw,493
 omnigenome/src/metric/classification_metric.py,sha256=djDoVY7-QqyC0lai9ATpvysjB7LF1rqFhLr5DMrDeX8,2819
 omnigenome/src/metric/ranking_metric.py,sha256=FBW7ojH027ODbsd6a-K_4a5JMJVB0mWVrhcXgvSfZu4,2424
 omnigenome/src/metric/regression_metric.py,sha256=fm9U5_K6CXRSIMeNp5Z0gXkN1xpcOhu_jeDw15uxtgQ,2785
 omnigenome/src/misc/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-omnigenome/src/misc/utils.py,sha256=qyL9-nO9tgTHAcuawhdo7CLVPX1O2xF7TI2MPozMBFk,6784
+omnigenome/src/misc/utils.py,sha256=3QqNKxytSJIQY0T5E7k_8uyROwUYYQz5DZno-xf48XY,7178
 omnigenome/src/model/__init__.py,sha256=1eubHrpfpufX_njO9IUv3ofPbAdHT-XBRkItkzKi3CM,470
 omnigenome/src/model/classiifcation/__init__.py,sha256=dPugrswvrM58nVxS5FlCBnx2meuTPY9SzOA4YLrP-l4,342
-omnigenome/src/model/classiifcation/model.py,sha256=AgS3-rKrxj59KVjySjrSW_RcHKVoKXEOJS9geUWTGg8,12353
+omnigenome/src/model/classiifcation/model.py,sha256=YqBB772X7HAhc-9DgrTLrAI5lXrrjT9ftUfN1QB5PEc,10416
 omnigenome/src/model/mlm/__init__.py,sha256=0-S-GmzGIx7qB7Aixh_KovHxAcvKHgCZJcxYHifjZ-A,342
-omnigenome/src/model/mlm/model.py,sha256=JxtcLcP50eKny7fSgj_q0WbCM8t9OyIedcV2LqpM67o,4534
+omnigenome/src/model/mlm/model.py,sha256=MfnDaomM8trqyL5zplFL14laiuipSOLIZKOCCh-EamM,3916
 omnigenome/src/model/regression/__init__.py,sha256=P_xAnv8ydAA49O2NeRBf7TEU78OZwGE5xEAT8V2FdoQ,342
-omnigenome/src/model/regression/model.py,sha256=IE8iaQzrazP7hUTrjPEvMidRNQJ7febgYDOt8lu8520,10956
+omnigenome/src/model/regression/model.py,sha256=Ts53jY1-BY0Bo7QxTK3Nw7yQSisVcNTaMpI4NO0NWYg,10180
 omnigenome/src/model/seq2seq/__init__.py,sha256=sP2vP_2XzMI80eWAbTT5cboKY9lxjPxe-Mtfiu_vK-A,342
 omnigenome/src/model/seq2seq/model.py,sha256=gr9ve3zn_Q8oveTZkjqsG2erow05dIRer1CMsfpAFCc,607
 omnigenome/src/tokenizer/__init__.py,sha256=uugoAoUSB-C-V4XJIoYLgAzB1nLKcTxGGb1bCBm4_-U,512
 omnigenome/src/tokenizer/bpe_tokenizer.py,sha256=4KZzG-IeXfeW6vZqiAe38o2qvBm1H4inSTAqfqENzEI,2996
 omnigenome/src/tokenizer/kmers_tokenizer.py,sha256=YlFWzOaNj-lxToAtkw-PyIyO7TrxcTZy_igysZhWwYU,4598
 omnigenome/src/tokenizer/single_nucleotide_tokenizer.py,sha256=ZeLEIz3fMs9J19mNvHZXcEYSuPpX4jFddd6k__1HVcI,3868
 omnigenome/src/trainer/__init__.py,sha256=oM-Jh4_-RDi0j2omG-7Vxwx3LAR_LAgc2gAERCWMi4Q,409
 omnigenome/src/trainer/hf_trainer.py,sha256=ZQipIZj0WXwHpgf8nsss2KRq9bCH4LI32Uo_iQKT5Bg,1092
-omnigenome/src/trainer/trainer.py,sha256=CDpSexoW7jQhf-FphJ2JNyTO0bAgjMQznweej6kSKNU,10478
+omnigenome/src/trainer/trainer.py,sha256=NM2uCRdPkrkx-CEEqwDrb9mp7uGRedVSbDCsgIp6TmM,10664
 omnigenome/utility/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 omnigenome/utility/ensemble.py,sha256=kpX-R_FpAYIkOX24iZle4Q_V7dcSDpim1jx2XJczra4,9554
 omnigenome/utility/hub_utils.py,sha256=8JEUdcs2BmicvwgjLjmBjmxkW7KmYC4Fg-3rYIkjOeY,10629
 omnigenome/utility/model_hub/__init__.py,sha256=p_rxaidCUZTW_Ws4q8B36RMiqK_wnRRsjtxA4CFp6lc,342
 omnigenome/utility/model_hub/model_hub.py,sha256=48U1qykhrFkaxvfc9LZmlwofmToJ6nRfUo0MnfLh28w,2993
 omnigenome/utility/pipeline_hub/__init__.py,sha256=5EQKxssQeyl195CeJG31QuwjTt8vrUWbBjim0vzzF1I,342
 omnigenome/utility/pipeline_hub/pipeline.py,sha256=ETxIVyAPISSszuCuDjrCmVdEWkwGITu3tSG6iBfRJ1c,5603
 omnigenome/utility/pipeline_hub/pipeline_hub.py,sha256=XJOHduSM-T7olGiqdTp-fg_QKQ9-y40IGg-IcoX24-0,883
-OmniGenome-0.0.3a0.dist-info/METADATA,sha256=RjmiIiy_7HvoHP4urkEGgsetZWYd7anLhcr1VW_TPoo,2272
-OmniGenome-0.0.3a0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-OmniGenome-0.0.3a0.dist-info/top_level.txt,sha256=LVFxm_WPaxjj9KnAqdW94W4D4lbOk30gdsaKlJiSzTo,11
-OmniGenome-0.0.3a0.dist-info/RECORD,,
+OmniGenome-0.0.4a0.dist-info/METADATA,sha256=YFPFSp6x8Cs3ziA8WFsbR0alhd8OMg3DmZQROvkPnJU,2272
+OmniGenome-0.0.4a0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+OmniGenome-0.0.4a0.dist-info/top_level.txt,sha256=LVFxm_WPaxjj9KnAqdW94W4D4lbOk30gdsaKlJiSzTo,11
+OmniGenome-0.0.4a0.dist-info/RECORD,,
```

