# Comparing `tmp/spandrel-0.3.1-py3-none-any.whl.zip` & `tmp/spandrel-0.3.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,108 +1,116 @@
-Zip file size: 255253 bytes, number of entries: 106
--rw-r--r--  2.0 unx     1136 b- defN 24-Mar-26 12:39 spandrel/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-26 12:39 spandrel/__helpers/__init__.py
--rw-r--r--  2.0 unx     1421 b- defN 24-Mar-26 12:39 spandrel/__helpers/canonicalize.py
--rw-r--r--  2.0 unx     3498 b- defN 24-Mar-26 12:39 spandrel/__helpers/loader.py
--rw-r--r--  2.0 unx     2360 b- defN 24-Mar-26 12:39 spandrel/__helpers/main_registry.py
--rw-r--r--  2.0 unx    19466 b- defN 24-Mar-26 12:39 spandrel/__helpers/model_descriptor.py
--rw-r--r--  2.0 unx     5853 b- defN 24-Mar-26 12:39 spandrel/__helpers/registry.py
--rw-r--r--  2.0 unx     3026 b- defN 24-Mar-26 12:39 spandrel/__helpers/size_req.py
--rw-r--r--  2.0 unx      976 b- defN 24-Mar-26 12:39 spandrel/__helpers/unpickler.py
--rw-r--r--  2.0 unx      117 b- defN 24-Mar-26 12:39 spandrel/architectures/__init__.py
--rw-r--r--  2.0 unx     6706 b- defN 24-Mar-26 12:39 spandrel/architectures/ATD/__init__.py
--rw-r--r--  2.0 unx    40948 b- defN 24-Mar-26 12:39 spandrel/architectures/ATD/arch/atd_arch.py
--rw-r--r--  2.0 unx     4084 b- defN 24-Mar-26 12:39 spandrel/architectures/CRAFT/__init__.py
--rw-r--r--  2.0 unx    30821 b- defN 24-Mar-26 12:39 spandrel/architectures/CRAFT/arch/CRAFT.py
--rw-r--r--  2.0 unx     1635 b- defN 24-Mar-26 12:39 spandrel/architectures/Compact/__init__.py
--rw-r--r--  2.0 unx     2835 b- defN 24-Mar-26 12:39 spandrel/architectures/Compact/arch/SRVGG.py
--rw-r--r--  2.0 unx     4839 b- defN 24-Mar-26 12:39 spandrel/architectures/DAT/__init__.py
--rw-r--r--  2.0 unx    36851 b- defN 24-Mar-26 12:39 spandrel/architectures/DAT/arch/DAT.py
--rw-r--r--  2.0 unx     2773 b- defN 24-Mar-26 12:39 spandrel/architectures/DCTLSA/__init__.py
--rw-r--r--  2.0 unx    15247 b- defN 24-Mar-26 12:39 spandrel/architectures/DCTLSA/arch/dctlsa.py
--rw-r--r--  2.0 unx     2660 b- defN 24-Mar-26 12:39 spandrel/architectures/DITN/__init__.py
--rw-r--r--  2.0 unx     9484 b- defN 24-Mar-26 12:39 spandrel/architectures/DITN/arch/DITN_Real.py
--rw-r--r--  2.0 unx     3163 b- defN 24-Mar-26 12:39 spandrel/architectures/DRUNet/__init__.py
--rw-r--r--  2.0 unx     3630 b- defN 24-Mar-26 12:39 spandrel/architectures/DRUNet/arch/network_unet.py
--rw-r--r--  2.0 unx     3849 b- defN 24-Mar-26 12:39 spandrel/architectures/DnCNN/__init__.py
--rw-r--r--  2.0 unx     2889 b- defN 24-Mar-26 12:39 spandrel/architectures/DnCNN/arch/network_dncnn.py
--rw-r--r--  2.0 unx     7553 b- defN 24-Mar-26 12:39 spandrel/architectures/ESRGAN/__init__.py
--rw-r--r--  2.0 unx     4456 b- defN 24-Mar-26 12:39 spandrel/architectures/ESRGAN/arch/RRDB.py
--rw-r--r--  2.0 unx     2578 b- defN 24-Mar-26 12:39 spandrel/architectures/FBCNN/__init__.py
--rw-r--r--  2.0 unx    15971 b- defN 24-Mar-26 12:39 spandrel/architectures/FBCNN/arch/FBCNN.py
--rw-r--r--  2.0 unx     3587 b- defN 24-Mar-26 12:39 spandrel/architectures/FFTformer/__init__.py
--rw-r--r--  2.0 unx    11840 b- defN 24-Mar-26 12:39 spandrel/architectures/FFTformer/arch/fftformer_arch.py
--rw-r--r--  2.0 unx     1773 b- defN 24-Mar-26 12:39 spandrel/architectures/GFPGAN/__init__.py
--rw-r--r--  2.0 unx     8166 b- defN 24-Mar-26 12:39 spandrel/architectures/GFPGAN/arch/arcface_arch.py
--rw-r--r--  2.0 unx     2299 b- defN 24-Mar-26 12:39 spandrel/architectures/GFPGAN/arch/fused_act.py
--rw-r--r--  2.0 unx    14296 b- defN 24-Mar-26 12:39 spandrel/architectures/GFPGAN/arch/gfpgan_bilinear_arch.py
--rw-r--r--  2.0 unx    19614 b- defN 24-Mar-26 12:39 spandrel/architectures/GFPGAN/arch/gfpganv1_arch.py
--rw-r--r--  2.0 unx    13677 b- defN 24-Mar-26 12:39 spandrel/architectures/GFPGAN/arch/gfpganv1_clean_arch.py
--rw-r--r--  2.0 unx    28263 b- defN 24-Mar-26 12:39 spandrel/architectures/GFPGAN/arch/stylegan2_arch.py
--rw-r--r--  2.0 unx    23081 b- defN 24-Mar-26 12:39 spandrel/architectures/GFPGAN/arch/stylegan2_bilinear_arch.py
--rw-r--r--  2.0 unx    16045 b- defN 24-Mar-26 12:39 spandrel/architectures/GFPGAN/arch/stylegan2_clean_arch.py
--rw-r--r--  2.0 unx     5589 b- defN 24-Mar-26 12:39 spandrel/architectures/GFPGAN/arch/upfirdn2d.py
--rw-r--r--  2.0 unx    13523 b- defN 24-Mar-26 12:39 spandrel/architectures/GRL/__init__.py
--rw-r--r--  2.0 unx     1225 b- defN 24-Mar-26 12:39 spandrel/architectures/GRL/arch/config.py
--rw-r--r--  2.0 unx    26006 b- defN 24-Mar-26 12:39 spandrel/architectures/GRL/arch/grl.py
--rw-r--r--  2.0 unx    39569 b- defN 24-Mar-26 12:39 spandrel/architectures/GRL/arch/mixed_attn_block.py
--rw-r--r--  2.0 unx    20122 b- defN 24-Mar-26 12:39 spandrel/architectures/GRL/arch/mixed_attn_block_efficient.py
--rw-r--r--  2.0 unx    19820 b- defN 24-Mar-26 12:39 spandrel/architectures/GRL/arch/ops.py
--rw-r--r--  2.0 unx    20723 b- defN 24-Mar-26 12:39 spandrel/architectures/GRL/arch/swin_v1_block.py
--rw-r--r--  2.0 unx     1575 b- defN 24-Mar-26 12:39 spandrel/architectures/GRL/arch/upsample.py
--rw-r--r--  2.0 unx     7782 b- defN 24-Mar-26 12:39 spandrel/architectures/HAT/__init__.py
--rw-r--r--  2.0 unx    40884 b- defN 24-Mar-26 12:39 spandrel/architectures/HAT/arch/HAT.py
--rw-r--r--  2.0 unx     5399 b- defN 24-Mar-26 12:39 spandrel/architectures/KBNet/__init__.py
--rw-r--r--  2.0 unx     4410 b- defN 24-Mar-26 12:39 spandrel/architectures/KBNet/arch/kb_utils.py
--rw-r--r--  2.0 unx    11639 b- defN 24-Mar-26 12:39 spandrel/architectures/KBNet/arch/kbnet_l.py
--rw-r--r--  2.0 unx     8357 b- defN 24-Mar-26 12:39 spandrel/architectures/KBNet/arch/kbnet_s.py
--rw-r--r--  2.0 unx     1484 b- defN 24-Mar-26 12:39 spandrel/architectures/LaMa/__init__.py
--rw-r--r--  2.0 unx    20902 b- defN 24-Mar-26 12:39 spandrel/architectures/LaMa/arch/LaMa.py
--rw-r--r--  2.0 unx     6225 b- defN 24-Mar-26 12:39 spandrel/architectures/MMRealSR/__init__.py
--rw-r--r--  2.0 unx    23377 b- defN 24-Mar-26 12:39 spandrel/architectures/MMRealSR/arch/mmrealsr_arch.py
--rw-r--r--  2.0 unx     4239 b- defN 24-Mar-26 12:39 spandrel/architectures/MixDehazeNet/__init__.py
--rw-r--r--  2.0 unx     8445 b- defN 24-Mar-26 12:39 spandrel/architectures/MixDehazeNet/arch/MixDehazeNet.py
--rw-r--r--  2.0 unx     2489 b- defN 24-Mar-26 12:39 spandrel/architectures/NAFNet/__init__.py
--rw-r--r--  2.0 unx     5984 b- defN 24-Mar-26 12:39 spandrel/architectures/NAFNet/arch/NAFNet_arch.py
--rw-r--r--  2.0 unx     1477 b- defN 24-Mar-26 12:39 spandrel/architectures/NAFNet/arch/arch_util.py
--rw-r--r--  2.0 unx     2916 b- defN 24-Mar-26 12:39 spandrel/architectures/OmniSR/__init__.py
--rw-r--r--  2.0 unx     3106 b- defN 24-Mar-26 12:39 spandrel/architectures/OmniSR/arch/ChannelAttention.py
--rw-r--r--  2.0 unx    15054 b- defN 24-Mar-26 12:39 spandrel/architectures/OmniSR/arch/OSA.py
--rw-r--r--  2.0 unx     1714 b- defN 24-Mar-26 12:39 spandrel/architectures/OmniSR/arch/OSAG.py
--rw-r--r--  2.0 unx     2666 b- defN 24-Mar-26 12:39 spandrel/architectures/OmniSR/arch/OmniSR.py
--rw-r--r--  2.0 unx     8315 b- defN 24-Mar-26 12:39 spandrel/architectures/OmniSR/arch/esa.py
--rw-r--r--  2.0 unx     2275 b- defN 24-Mar-26 12:39 spandrel/architectures/OmniSR/arch/layernorm.py
--rw-r--r--  2.0 unx      850 b- defN 24-Mar-26 12:39 spandrel/architectures/OmniSR/arch/pixelshuffle.py
--rw-r--r--  2.0 unx     5937 b- defN 24-Mar-26 12:39 spandrel/architectures/RGT/__init__.py
--rw-r--r--  2.0 unx    29903 b- defN 24-Mar-26 12:39 spandrel/architectures/RGT/arch/rgt.py
--rw-r--r--  2.0 unx     3651 b- defN 24-Mar-26 12:39 spandrel/architectures/RealCUGAN/__init__.py
--rw-r--r--  2.0 unx    14795 b- defN 24-Mar-26 12:39 spandrel/architectures/RealCUGAN/arch/upcunet_v3.py
--rw-r--r--  2.0 unx     3086 b- defN 24-Mar-26 12:39 spandrel/architectures/RestoreFormer/__init__.py
--rw-r--r--  2.0 unx    23887 b- defN 24-Mar-26 12:39 spandrel/architectures/RestoreFormer/arch/restoreformer_arch.py
--rw-r--r--  2.0 unx     2100 b- defN 24-Mar-26 12:39 spandrel/architectures/SAFMN/__init__.py
--rw-r--r--  2.0 unx     5019 b- defN 24-Mar-26 12:39 spandrel/architectures/SAFMN/arch/safmn.py
--rw-r--r--  2.0 unx     1904 b- defN 24-Mar-26 12:39 spandrel/architectures/SCUNet/__init__.py
--rw-r--r--  2.0 unx    13786 b- defN 24-Mar-26 12:39 spandrel/architectures/SCUNet/arch/SCUNet.py
--rw-r--r--  2.0 unx     2289 b- defN 24-Mar-26 12:39 spandrel/architectures/SPAN/__init__.py
--rw-r--r--  2.0 unx     9234 b- defN 24-Mar-26 12:39 spandrel/architectures/SPAN/arch/span.py
--rw-r--r--  2.0 unx     1733 b- defN 24-Mar-26 12:39 spandrel/architectures/SwiftSRGAN/__init__.py
--rw-r--r--  2.0 unx     4104 b- defN 24-Mar-26 12:39 spandrel/architectures/SwiftSRGAN/arch/SwiftSRGAN.py
--rw-r--r--  2.0 unx     6726 b- defN 24-Mar-26 12:39 spandrel/architectures/Swin2SR/__init__.py
--rw-r--r--  2.0 unx    46536 b- defN 24-Mar-26 12:39 spandrel/architectures/Swin2SR/arch/Swin2SR.py
--rw-r--r--  2.0 unx     6331 b- defN 24-Mar-26 12:39 spandrel/architectures/SwinIR/__init__.py
--rw-r--r--  2.0 unx    38906 b- defN 24-Mar-26 12:39 spandrel/architectures/SwinIR/arch/SwinIR.py
--rw-r--r--  2.0 unx     5727 b- defN 24-Mar-26 12:39 spandrel/architectures/Uformer/__init__.py
--rw-r--r--  2.0 unx    55152 b- defN 24-Mar-26 12:39 spandrel/architectures/Uformer/arch/Uformer.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-26 12:39 spandrel/architectures/__arch_helpers/__init__.py
--rw-r--r--  2.0 unx    13495 b- defN 24-Mar-26 12:39 spandrel/architectures/__arch_helpers/block.py
--rw-r--r--  2.0 unx    10601 b- defN 24-Mar-26 12:39 spandrel/architectures/__arch_helpers/dpir_basic_block.py
--rw-r--r--  2.0 unx      770 b- defN 24-Mar-26 12:39 spandrel/architectures/__arch_helpers/padding.py
--rw-r--r--  2.0 unx     5431 b- defN 24-Mar-26 12:39 spandrel/util/__init__.py
--rw-r--r--  2.0 unx     7368 b- defN 24-Mar-26 12:39 spandrel/util/timm/__drop.py
--rw-r--r--  2.0 unx      788 b- defN 24-Mar-26 12:39 spandrel/util/timm/__helpers.py
--rw-r--r--  2.0 unx      314 b- defN 24-Mar-26 12:39 spandrel/util/timm/__init__.py
--rw-r--r--  2.0 unx     5088 b- defN 24-Mar-26 12:39 spandrel/util/timm/__weight_init.py
--rw-r--r--  2.0 unx    14005 b- defN 24-Mar-26 12:40 spandrel-0.3.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-26 12:40 spandrel-0.3.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 24-Mar-26 12:40 spandrel-0.3.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    10525 b- defN 24-Mar-26 12:40 spandrel-0.3.1.dist-info/RECORD
-106 files, 1070899 bytes uncompressed, 238135 bytes compressed:  77.8%
+Zip file size: 263694 bytes, number of entries: 114
+-rw-r--r--  2.0 unx     1136 b- defN 24-May-06 17:24 spandrel/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-06 17:24 spandrel/__helpers/__init__.py
+-rw-r--r--  2.0 unx     1421 b- defN 24-May-06 17:24 spandrel/__helpers/canonicalize.py
+-rw-r--r--  2.0 unx     3801 b- defN 24-May-06 17:24 spandrel/__helpers/loader.py
+-rw-r--r--  2.0 unx     2558 b- defN 24-May-06 17:24 spandrel/__helpers/main_registry.py
+-rw-r--r--  2.0 unx    19466 b- defN 24-May-06 17:24 spandrel/__helpers/model_descriptor.py
+-rw-r--r--  2.0 unx     5853 b- defN 24-May-06 17:24 spandrel/__helpers/registry.py
+-rw-r--r--  2.0 unx     3026 b- defN 24-May-06 17:24 spandrel/__helpers/size_req.py
+-rw-r--r--  2.0 unx      976 b- defN 24-May-06 17:24 spandrel/__helpers/unpickler.py
+-rw-r--r--  2.0 unx      117 b- defN 24-May-06 17:24 spandrel/architectures/__init__.py
+-rw-r--r--  2.0 unx     6537 b- defN 24-May-06 17:24 spandrel/architectures/ATD/__init__.py
+-rw-r--r--  2.0 unx    41329 b- defN 24-May-06 17:24 spandrel/architectures/ATD/arch/atd_arch.py
+-rw-r--r--  2.0 unx     4084 b- defN 24-May-06 17:24 spandrel/architectures/CRAFT/__init__.py
+-rw-r--r--  2.0 unx    30815 b- defN 24-May-06 17:24 spandrel/architectures/CRAFT/arch/CRAFT.py
+-rw-r--r--  2.0 unx     1635 b- defN 24-May-06 17:24 spandrel/architectures/Compact/__init__.py
+-rw-r--r--  2.0 unx     2835 b- defN 24-May-06 17:24 spandrel/architectures/Compact/arch/SRVGG.py
+-rw-r--r--  2.0 unx     6629 b- defN 24-May-06 17:24 spandrel/architectures/DAT/__init__.py
+-rw-r--r--  2.0 unx    36845 b- defN 24-May-06 17:24 spandrel/architectures/DAT/arch/DAT.py
+-rw-r--r--  2.0 unx     2773 b- defN 24-May-06 17:24 spandrel/architectures/DCTLSA/__init__.py
+-rw-r--r--  2.0 unx    15247 b- defN 24-May-06 17:24 spandrel/architectures/DCTLSA/arch/dctlsa.py
+-rw-r--r--  2.0 unx     2660 b- defN 24-May-06 17:24 spandrel/architectures/DITN/__init__.py
+-rw-r--r--  2.0 unx     9484 b- defN 24-May-06 17:24 spandrel/architectures/DITN/arch/DITN_Real.py
+-rw-r--r--  2.0 unx     5144 b- defN 24-May-06 17:24 spandrel/architectures/DRCT/__init__.py
+-rw-r--r--  2.0 unx    28533 b- defN 24-May-06 17:24 spandrel/architectures/DRCT/arch/drct_arch.py
+-rw-r--r--  2.0 unx     3163 b- defN 24-May-06 17:24 spandrel/architectures/DRUNet/__init__.py
+-rw-r--r--  2.0 unx     3630 b- defN 24-May-06 17:24 spandrel/architectures/DRUNet/arch/network_unet.py
+-rw-r--r--  2.0 unx     3849 b- defN 24-May-06 17:24 spandrel/architectures/DnCNN/__init__.py
+-rw-r--r--  2.0 unx     2889 b- defN 24-May-06 17:24 spandrel/architectures/DnCNN/arch/network_dncnn.py
+-rw-r--r--  2.0 unx     7553 b- defN 24-May-06 17:24 spandrel/architectures/ESRGAN/__init__.py
+-rw-r--r--  2.0 unx     4456 b- defN 24-May-06 17:24 spandrel/architectures/ESRGAN/arch/RRDB.py
+-rw-r--r--  2.0 unx     2578 b- defN 24-May-06 17:24 spandrel/architectures/FBCNN/__init__.py
+-rw-r--r--  2.0 unx    15971 b- defN 24-May-06 17:24 spandrel/architectures/FBCNN/arch/FBCNN.py
+-rw-r--r--  2.0 unx     3587 b- defN 24-May-06 17:24 spandrel/architectures/FFTformer/__init__.py
+-rw-r--r--  2.0 unx    11840 b- defN 24-May-06 17:24 spandrel/architectures/FFTformer/arch/fftformer_arch.py
+-rw-r--r--  2.0 unx     1773 b- defN 24-May-06 17:24 spandrel/architectures/GFPGAN/__init__.py
+-rw-r--r--  2.0 unx     8166 b- defN 24-May-06 17:24 spandrel/architectures/GFPGAN/arch/arcface_arch.py
+-rw-r--r--  2.0 unx     2299 b- defN 24-May-06 17:24 spandrel/architectures/GFPGAN/arch/fused_act.py
+-rw-r--r--  2.0 unx    14296 b- defN 24-May-06 17:24 spandrel/architectures/GFPGAN/arch/gfpgan_bilinear_arch.py
+-rw-r--r--  2.0 unx    19614 b- defN 24-May-06 17:24 spandrel/architectures/GFPGAN/arch/gfpganv1_arch.py
+-rw-r--r--  2.0 unx    13677 b- defN 24-May-06 17:24 spandrel/architectures/GFPGAN/arch/gfpganv1_clean_arch.py
+-rw-r--r--  2.0 unx    28263 b- defN 24-May-06 17:24 spandrel/architectures/GFPGAN/arch/stylegan2_arch.py
+-rw-r--r--  2.0 unx    23081 b- defN 24-May-06 17:24 spandrel/architectures/GFPGAN/arch/stylegan2_bilinear_arch.py
+-rw-r--r--  2.0 unx    16045 b- defN 24-May-06 17:24 spandrel/architectures/GFPGAN/arch/stylegan2_clean_arch.py
+-rw-r--r--  2.0 unx     5589 b- defN 24-May-06 17:24 spandrel/architectures/GFPGAN/arch/upfirdn2d.py
+-rw-r--r--  2.0 unx    13385 b- defN 24-May-06 17:24 spandrel/architectures/GRL/__init__.py
+-rw-r--r--  2.0 unx     1225 b- defN 24-May-06 17:24 spandrel/architectures/GRL/arch/config.py
+-rw-r--r--  2.0 unx    25424 b- defN 24-May-06 17:24 spandrel/architectures/GRL/arch/grl.py
+-rw-r--r--  2.0 unx     7043 b- defN 24-May-06 17:24 spandrel/architectures/GRL/arch/mixed_attn_block.py
+-rw-r--r--  2.0 unx    20088 b- defN 24-May-06 17:24 spandrel/architectures/GRL/arch/mixed_attn_block_efficient.py
+-rw-r--r--  2.0 unx     8423 b- defN 24-May-06 17:24 spandrel/architectures/GRL/arch/ops.py
+-rw-r--r--  2.0 unx     2021 b- defN 24-May-06 17:24 spandrel/architectures/GRL/arch/swin_v1_block.py
+-rw-r--r--  2.0 unx     1575 b- defN 24-May-06 17:24 spandrel/architectures/GRL/arch/upsample.py
+-rw-r--r--  2.0 unx     8179 b- defN 24-May-06 17:24 spandrel/architectures/HAT/__init__.py
+-rw-r--r--  2.0 unx    39467 b- defN 24-May-06 17:24 spandrel/architectures/HAT/arch/HAT.py
+-rw-r--r--  2.0 unx     5362 b- defN 24-May-06 17:24 spandrel/architectures/IPT/__init__.py
+-rw-r--r--  2.0 unx     2645 b- defN 24-May-06 17:24 spandrel/architectures/IPT/arch/common.py
+-rw-r--r--  2.0 unx    11879 b- defN 24-May-06 17:24 spandrel/architectures/IPT/arch/ipt.py
+-rw-r--r--  2.0 unx     9255 b- defN 24-May-06 17:24 spandrel/architectures/IPT/arch/model.py
+-rw-r--r--  2.0 unx     5521 b- defN 24-May-06 17:24 spandrel/architectures/KBNet/__init__.py
+-rw-r--r--  2.0 unx     4410 b- defN 24-May-06 17:24 spandrel/architectures/KBNet/arch/kb_utils.py
+-rw-r--r--  2.0 unx    11639 b- defN 24-May-06 17:24 spandrel/architectures/KBNet/arch/kbnet_l.py
+-rw-r--r--  2.0 unx     8357 b- defN 24-May-06 17:24 spandrel/architectures/KBNet/arch/kbnet_s.py
+-rw-r--r--  2.0 unx     1484 b- defN 24-May-06 17:24 spandrel/architectures/LaMa/__init__.py
+-rw-r--r--  2.0 unx    20902 b- defN 24-May-06 17:24 spandrel/architectures/LaMa/arch/LaMa.py
+-rw-r--r--  2.0 unx     6225 b- defN 24-May-06 17:24 spandrel/architectures/MMRealSR/__init__.py
+-rw-r--r--  2.0 unx    23377 b- defN 24-May-06 17:24 spandrel/architectures/MMRealSR/arch/mmrealsr_arch.py
+-rw-r--r--  2.0 unx     4239 b- defN 24-May-06 17:24 spandrel/architectures/MixDehazeNet/__init__.py
+-rw-r--r--  2.0 unx     8445 b- defN 24-May-06 17:24 spandrel/architectures/MixDehazeNet/arch/MixDehazeNet.py
+-rw-r--r--  2.0 unx     2489 b- defN 24-May-06 17:24 spandrel/architectures/NAFNet/__init__.py
+-rw-r--r--  2.0 unx     5984 b- defN 24-May-06 17:24 spandrel/architectures/NAFNet/arch/NAFNet_arch.py
+-rw-r--r--  2.0 unx     1477 b- defN 24-May-06 17:24 spandrel/architectures/NAFNet/arch/arch_util.py
+-rw-r--r--  2.0 unx     2916 b- defN 24-May-06 17:24 spandrel/architectures/OmniSR/__init__.py
+-rw-r--r--  2.0 unx     3106 b- defN 24-May-06 17:24 spandrel/architectures/OmniSR/arch/ChannelAttention.py
+-rw-r--r--  2.0 unx    15054 b- defN 24-May-06 17:24 spandrel/architectures/OmniSR/arch/OSA.py
+-rw-r--r--  2.0 unx     1714 b- defN 24-May-06 17:24 spandrel/architectures/OmniSR/arch/OSAG.py
+-rw-r--r--  2.0 unx     2666 b- defN 24-May-06 17:24 spandrel/architectures/OmniSR/arch/OmniSR.py
+-rw-r--r--  2.0 unx     8315 b- defN 24-May-06 17:24 spandrel/architectures/OmniSR/arch/esa.py
+-rw-r--r--  2.0 unx     2275 b- defN 24-May-06 17:24 spandrel/architectures/OmniSR/arch/layernorm.py
+-rw-r--r--  2.0 unx      850 b- defN 24-May-06 17:24 spandrel/architectures/OmniSR/arch/pixelshuffle.py
+-rw-r--r--  2.0 unx     5925 b- defN 24-May-06 17:24 spandrel/architectures/RGT/__init__.py
+-rw-r--r--  2.0 unx    29897 b- defN 24-May-06 17:24 spandrel/architectures/RGT/arch/rgt.py
+-rw-r--r--  2.0 unx     3651 b- defN 24-May-06 17:24 spandrel/architectures/RealCUGAN/__init__.py
+-rw-r--r--  2.0 unx    14795 b- defN 24-May-06 17:24 spandrel/architectures/RealCUGAN/arch/upcunet_v3.py
+-rw-r--r--  2.0 unx     3410 b- defN 24-May-06 17:24 spandrel/architectures/RestoreFormer/__init__.py
+-rw-r--r--  2.0 unx    23884 b- defN 24-May-06 17:24 spandrel/architectures/RestoreFormer/arch/restoreformer_arch.py
+-rw-r--r--  2.0 unx     2100 b- defN 24-May-06 17:24 spandrel/architectures/SAFMN/__init__.py
+-rw-r--r--  2.0 unx     5019 b- defN 24-May-06 17:24 spandrel/architectures/SAFMN/arch/safmn.py
+-rw-r--r--  2.0 unx     2596 b- defN 24-May-06 17:24 spandrel/architectures/SAFMNBCIE/__init__.py
+-rw-r--r--  2.0 unx     4168 b- defN 24-May-06 17:24 spandrel/architectures/SAFMNBCIE/arch/safmn_bcie.py
+-rw-r--r--  2.0 unx     1904 b- defN 24-May-06 17:24 spandrel/architectures/SCUNet/__init__.py
+-rw-r--r--  2.0 unx    13786 b- defN 24-May-06 17:24 spandrel/architectures/SCUNet/arch/SCUNet.py
+-rw-r--r--  2.0 unx     2289 b- defN 24-May-06 17:24 spandrel/architectures/SPAN/__init__.py
+-rw-r--r--  2.0 unx     9234 b- defN 24-May-06 17:24 spandrel/architectures/SPAN/arch/span.py
+-rw-r--r--  2.0 unx     1733 b- defN 24-May-06 17:24 spandrel/architectures/SwiftSRGAN/__init__.py
+-rw-r--r--  2.0 unx     4104 b- defN 24-May-06 17:24 spandrel/architectures/SwiftSRGAN/arch/SwiftSRGAN.py
+-rw-r--r--  2.0 unx     6551 b- defN 24-May-06 17:24 spandrel/architectures/Swin2SR/__init__.py
+-rw-r--r--  2.0 unx    46536 b- defN 24-May-06 17:24 spandrel/architectures/Swin2SR/arch/Swin2SR.py
+-rw-r--r--  2.0 unx     6067 b- defN 24-May-06 17:24 spandrel/architectures/SwinIR/__init__.py
+-rw-r--r--  2.0 unx    38906 b- defN 24-May-06 17:24 spandrel/architectures/SwinIR/arch/SwinIR.py
+-rw-r--r--  2.0 unx     5727 b- defN 24-May-06 17:24 spandrel/architectures/Uformer/__init__.py
+-rw-r--r--  2.0 unx    54675 b- defN 24-May-06 17:24 spandrel/architectures/Uformer/arch/Uformer.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-06 17:24 spandrel/architectures/__arch_helpers/__init__.py
+-rw-r--r--  2.0 unx    13495 b- defN 24-May-06 17:24 spandrel/architectures/__arch_helpers/block.py
+-rw-r--r--  2.0 unx    10601 b- defN 24-May-06 17:24 spandrel/architectures/__arch_helpers/dpir_basic_block.py
+-rw-r--r--  2.0 unx      770 b- defN 24-May-06 17:24 spandrel/architectures/__arch_helpers/padding.py
+-rw-r--r--  2.0 unx     6433 b- defN 24-May-06 17:24 spandrel/util/__init__.py
+-rw-r--r--  2.0 unx     7368 b- defN 24-May-06 17:24 spandrel/util/timm/__drop.py
+-rw-r--r--  2.0 unx      788 b- defN 24-May-06 17:24 spandrel/util/timm/__helpers.py
+-rw-r--r--  2.0 unx      314 b- defN 24-May-06 17:24 spandrel/util/timm/__init__.py
+-rw-r--r--  2.0 unx     5088 b- defN 24-May-06 17:24 spandrel/util/timm/__weight_init.py
+-rw-r--r--  2.0 unx    14270 b- defN 24-May-06 17:25 spandrel-0.3.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-06 17:25 spandrel-0.3.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 24-May-06 17:25 spandrel-0.3.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    11316 b- defN 24-May-06 17:25 spandrel-0.3.2.dist-info/RECORD
+114 files, 1080140 bytes uncompressed, 245296 bytes compressed:  77.3%
```

## zipnote {}

```diff
@@ -60,14 +60,20 @@
 
 Filename: spandrel/architectures/DITN/__init__.py
 Comment: 
 
 Filename: spandrel/architectures/DITN/arch/DITN_Real.py
 Comment: 
 
+Filename: spandrel/architectures/DRCT/__init__.py
+Comment: 
+
+Filename: spandrel/architectures/DRCT/arch/drct_arch.py
+Comment: 
+
 Filename: spandrel/architectures/DRUNet/__init__.py
 Comment: 
 
 Filename: spandrel/architectures/DRUNet/arch/network_unet.py
 Comment: 
 
 Filename: spandrel/architectures/DnCNN/__init__.py
@@ -150,14 +156,26 @@
 
 Filename: spandrel/architectures/HAT/__init__.py
 Comment: 
 
 Filename: spandrel/architectures/HAT/arch/HAT.py
 Comment: 
 
+Filename: spandrel/architectures/IPT/__init__.py
+Comment: 
+
+Filename: spandrel/architectures/IPT/arch/common.py
+Comment: 
+
+Filename: spandrel/architectures/IPT/arch/ipt.py
+Comment: 
+
+Filename: spandrel/architectures/IPT/arch/model.py
+Comment: 
+
 Filename: spandrel/architectures/KBNet/__init__.py
 Comment: 
 
 Filename: spandrel/architectures/KBNet/arch/kb_utils.py
 Comment: 
 
 Filename: spandrel/architectures/KBNet/arch/kbnet_l.py
@@ -237,14 +255,20 @@
 
 Filename: spandrel/architectures/SAFMN/__init__.py
 Comment: 
 
 Filename: spandrel/architectures/SAFMN/arch/safmn.py
 Comment: 
 
+Filename: spandrel/architectures/SAFMNBCIE/__init__.py
+Comment: 
+
+Filename: spandrel/architectures/SAFMNBCIE/arch/safmn_bcie.py
+Comment: 
+
 Filename: spandrel/architectures/SCUNet/__init__.py
 Comment: 
 
 Filename: spandrel/architectures/SCUNet/arch/SCUNet.py
 Comment: 
 
 Filename: spandrel/architectures/SPAN/__init__.py
@@ -300,20 +324,20 @@
 
 Filename: spandrel/util/timm/__init__.py
 Comment: 
 
 Filename: spandrel/util/timm/__weight_init.py
 Comment: 
 
-Filename: spandrel-0.3.1.dist-info/METADATA
+Filename: spandrel-0.3.2.dist-info/METADATA
 Comment: 
 
-Filename: spandrel-0.3.1.dist-info/WHEEL
+Filename: spandrel-0.3.2.dist-info/WHEEL
 Comment: 
 
-Filename: spandrel-0.3.1.dist-info/top_level.txt
+Filename: spandrel-0.3.2.dist-info/top_level.txt
 Comment: 
 
-Filename: spandrel-0.3.1.dist-info/RECORD
+Filename: spandrel-0.3.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## spandrel/__init__.py

```diff
@@ -1,12 +1,12 @@
 """
 Spandrel is a library for loading and running pre-trained PyTorch models. It automatically detects the model architecture and hyper parameters from model files, and provides a unified interface for running models.
 """
 
-__version__ = "0.3.1"
+__version__ = "0.3.2"
 
 from .__helpers.canonicalize import canonicalize_state_dict
 from .__helpers.loader import ModelLoader
 from .__helpers.main_registry import MAIN_REGISTRY
 from .__helpers.model_descriptor import (
     ArchId,
     Architecture,
```

## spandrel/__helpers/loader.py

```diff
@@ -53,19 +53,32 @@
         Throws a `ValueError` if the file extension is not supported.
         """
 
         extension = os.path.splitext(path)[1].lower()
 
         state_dict: StateDict
         if extension == ".pt":
-            state_dict = self._load_torchscript(path)
-        elif extension == ".pth":
+            try:
+                state_dict = self._load_torchscript(path)
+            except RuntimeError:
+                # If torchscript loading fails, try loading as a normal state dict
+                try:
+                    pth_state_dict = self._load_pth(path)
+                except Exception:
+                    pth_state_dict = None
+
+                if pth_state_dict is None:
+                    # the file was likely a torchscript file, but failed to load
+                    # re-raise the original error, so the user knows what went wrong
+                    raise
+
+                state_dict = pth_state_dict
+
+        elif extension == ".pth" or extension == ".ckpt":
             state_dict = self._load_pth(path)
-        elif extension == ".ckpt":
-            state_dict = self._load_ckpt(path)
         elif extension == ".safetensors":
             state_dict = self._load_safetensors(path)
         else:
             raise ValueError(
                 f"Unsupported model file extension {extension}. Please try a supported model type."
             )
 
@@ -90,14 +103,7 @@
     def _load_torchscript(self, path: str | Path) -> StateDict:
         return torch.jit.load(  # type: ignore
             path, map_location=self.device
         ).state_dict()
 
     def _load_safetensors(self, path: str | Path) -> StateDict:
         return load_file(path, device=str(self.device))
-
-    def _load_ckpt(self, path: str | Path) -> StateDict:
-        return torch.load(
-            path,
-            map_location=self.device,
-            pickle_module=RestrictedUnpickle,  # type: ignore
-        )
```

## spandrel/__helpers/main_registry.py

```diff
@@ -2,21 +2,24 @@
 
 from ..architectures import (
     ATD,
     CRAFT,
     DAT,
     DCTLSA,
     DITN,
+    DRCT,
     ESRGAN,
     FBCNN,
     GFPGAN,
     GRL,
     HAT,
+    IPT,
     RGT,
     SAFMN,
+    SAFMNBCIE,
     SPAN,
     Compact,
     DnCNN,
     DRUNet,
     FFTformer,
     KBNet,
     LaMa,
@@ -60,16 +63,19 @@
     ArchSupport.from_architecture(CRAFT.CRAFTArch()),
     ArchSupport.from_architecture(KBNet.KBNetArch()),
     ArchSupport.from_architecture(DITN.DITNArch()),
     ArchSupport.from_architecture(MMRealSR.MMRealSRArch()),
     ArchSupport.from_architecture(SPAN.SPANArch()),
     ArchSupport.from_architecture(RealCUGAN.RealCUGANArch()),
     ArchSupport.from_architecture(SAFMN.SAFMNArch()),
+    ArchSupport.from_architecture(SAFMNBCIE.SAFMNBCIEArch()),
     ArchSupport.from_architecture(DCTLSA.DCTLSAArch()),
     ArchSupport.from_architecture(FFTformer.FFTformerArch()),
     ArchSupport.from_architecture(NAFNet.NAFNetArch()),
     ArchSupport.from_architecture(ATD.ATDArch()),
     ArchSupport.from_architecture(MixDehazeNet.MixDehazeNetArch()),
     ArchSupport.from_architecture(DRUNet.DRUNetArch()),
     ArchSupport.from_architecture(DnCNN.DnCNNArch()),
+    ArchSupport.from_architecture(IPT.IPTArch()),
+    ArchSupport.from_architecture(DRCT.DRCTArch()),
     ArchSupport.from_architecture(ESRGAN.ESRGANArch()),
 )
```

## spandrel/architectures/ATD/__init__.py

```diff
@@ -1,12 +1,12 @@
 import math
 
 from typing_extensions import override
 
-from spandrel.util import KeyCondition, get_seq_len
+from spandrel.util import KeyCondition, get_pixelshuffle_params, get_seq_len
 
 from ...__helpers.model_descriptor import (
     Architecture,
     ImageModelDescriptor,
     SizeRequirements,
     StateDict,
 )
@@ -70,14 +70,15 @@
         qkv_bias = True
         ape = False
         patch_norm = True
         upscale = 1
         img_range = 1.0  # cannot be deduced from state dict
         upsampler = ""
         resi_connection = "1conv"
+        norm = True
 
         in_chans = state_dict["conv_first.weight"].shape[1]
         embed_dim = state_dict["conv_first.weight"].shape[0]
 
         window_size = math.isqrt(state_dict["relative_position_index_SA"].shape[0])
 
         num_layers = get_seq_len(state_dict, "layers")
@@ -109,30 +110,24 @@
         resi_connection = "1conv" if "layers.0.conv.weight" in state_dict else "3conv"
 
         if "conv_up1.weight" in state_dict:
             upsampler = "nearest+conv"
             upscale = 4
         elif "conv_before_upsample.0.weight" in state_dict:
             upsampler = "pixelshuffle"
-            upscale = 1
-            for i in range(0, 10, 2):
-                if f"upsample.{i}.weight" not in state_dict:
-                    break
-                num_feat = state_dict[f"upsample.{i}.weight"].shape[1]
-
-                upscale *= math.isqrt(
-                    state_dict[f"upsample.{i}.weight"].shape[0] // num_feat
-                )
+            upscale, _ = get_pixelshuffle_params(state_dict, "upsample")
         elif "conv_last.weight" in state_dict:
             upsampler = ""
             upscale = 1
         else:
             upsampler = "pixelshuffledirect"
             upscale = math.isqrt(state_dict["upsample.0.weight"].shape[0] // in_chans)
 
+        norm = "no_norm" not in state_dict
+
         is_light = upsampler == "pixelshuffledirect" and embed_dim == 48
         # use a heuristic for category_size
         category_size = 128 if is_light else 256
 
         tags = [f"{embed_dim}dim", f"{window_size}w", f"{category_size}cat"]
         if is_light:
             tags.insert(0, "light")
@@ -153,14 +148,15 @@
             qkv_bias=qkv_bias,
             ape=ape,
             patch_norm=patch_norm,
             upscale=upscale,
             img_range=img_range,
             upsampler=upsampler,
             resi_connection=resi_connection,
+            norm=norm,
         )
 
         return ImageModelDescriptor(
             model,
             state_dict,
             architecture=self,
             purpose="Restoration" if upscale == 1 else "SR",
```

## spandrel/architectures/ATD/arch/atd_arch.py

```diff
@@ -1,14 +1,16 @@
 """
 An official Pytorch impl of `Transcending the Limit of Local Window:
 Advanced Super-Resolution Transformer with Adaptive Token Dictionary`.
 
 Arxiv: 'https://arxiv.org/abs/2401.08209'
 """
 
+from __future__ import annotations
+
 import math
 
 import numpy as np
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
@@ -923,20 +925,28 @@
         norm_layer=nn.LayerNorm,
         ape=False,
         patch_norm=True,
         upscale=1,
         img_range=1.0,
         upsampler="",
         resi_connection="1conv",
+        norm=True,
     ):
         super().__init__()
         num_in_ch = in_chans
         num_out_ch = in_chans
         num_feat = 64
         self.img_range = img_range
+
+        self.no_norm: torch.Tensor | None
+        if not norm:
+            self.register_buffer("no_norm", torch.zeros(1))
+        else:
+            self.no_norm = None
+
         if in_chans == 3:
             rgb_mean = (0.4488, 0.4371, 0.4040)
             self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)
         else:
             self.mean = torch.zeros(1, 1, 1, 1)
         self.upscale = upscale
         self.upsampler = upsampler
@@ -1132,26 +1142,32 @@
         attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
         attn_mask = attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(
             attn_mask == 0, 0.0
         )
 
         return attn_mask
 
+    @property
+    def is_norm(self):
+        return self.no_norm is None
+
     def forward(self, x):
         # padding
         h_ori, w_ori = x.size()[-2], x.size()[-1]
         mod = self.window_size
         h_pad = ((h_ori + mod - 1) // mod) * mod - h_ori
         w_pad = ((w_ori + mod - 1) // mod) * mod - w_ori
         h, w = h_ori + h_pad, w_ori + w_pad
         x = torch.cat([x, torch.flip(x, [2])], 2)[:, :, :h, :]
         x = torch.cat([x, torch.flip(x, [3])], 3)[:, :, :, :w]
 
-        self.mean = self.mean.type_as(x)
-        x = (x - self.mean) * self.img_range
+        # rgb norm
+        if self.is_norm:
+            self.mean = self.mean.type_as(x)
+            x = (x - self.mean) * self.img_range
 
         attn_mask = self.calculate_mask([h, w]).to(x.device)
         params = {"attn_mask": attn_mask, "rpi_sa": self.relative_position_index_SA}
 
         if self.upsampler == "pixelshuffle":
             # for classical SR
             x = self.conv_first(x)
@@ -1177,13 +1193,14 @@
             x = self.conv_last(self.lrelu(self.conv_hr(x)))
         else:
             # for image denoising and JPEG compression artifact reduction
             x_first = self.conv_first(x)
             res = self.conv_after_body(self.forward_features(x_first, params)) + x_first
             x = x + self.conv_last(res)
 
-        x = x / self.img_range + self.mean
+        if self.is_norm:
+            x = x / self.img_range + self.mean
 
         # unpadding
         x = x[..., : h_ori * self.upscale, : w_ori * self.upscale]
 
         return x
```

## spandrel/architectures/CRAFT/arch/CRAFT.py

```diff
@@ -137,16 +137,15 @@
         head_dim = dim // num_heads
         self.scale = qk_scale or head_dim**-0.5
         if idx == 0:
             H_sp, W_sp = self.split_size[0], self.split_size[1]
         elif idx == 1:
             W_sp, H_sp = self.split_size[0], self.split_size[1]
         else:
-            print("ERROR MODE", idx)
-            exit(0)
+            raise ValueError(f"ERROR MODE: {idx}")
         self.H_sp = H_sp
         self.W_sp = W_sp
 
         self.pos = DynamicPosBias(self.dim // 4, self.num_heads, residual=False)
         self.softmax = nn.Softmax(dim=-1)
 
     def im2win(self, x, H, W):
```

## spandrel/architectures/DAT/__init__.py

```diff
@@ -1,12 +1,12 @@
 import math
 
 from typing_extensions import override
 
-from spandrel.util import KeyCondition, get_seq_len
+from spandrel.util import KeyCondition, get_pixelshuffle_params, get_seq_len
 
 from ...__helpers.model_descriptor import (
     Architecture,
     ImageModelDescriptor,
     SizeRequirements,
     StateDict,
 )
@@ -14,17 +14,55 @@
 
 
 class DATArch(Architecture[DAT]):
     def __init__(self) -> None:
         super().__init__(
             id="DAT",
             detect=KeyCondition.has_all(
-                "layers.0.blocks.2.attn.attn_mask_0",
-                "layers.0.blocks.0.ffn.fc1.weight",
                 "conv_first.weight",
+                "before_RG.1.weight",
+                "before_RG.1.bias",
+                "layers.0.blocks.0.norm1.weight",
+                "layers.0.blocks.0.norm2.weight",
+                "layers.0.blocks.0.ffn.fc1.weight",
+                "layers.0.blocks.0.ffn.sg.norm.weight",
+                "layers.0.blocks.0.ffn.sg.conv.weight",
+                "layers.0.blocks.0.ffn.fc2.weight",
+                "layers.0.blocks.0.attn.qkv.weight",
+                "layers.0.blocks.0.attn.proj.weight",
+                "layers.0.blocks.0.attn.dwconv.0.weight",
+                "layers.0.blocks.0.attn.dwconv.1.running_mean",
+                "layers.0.blocks.0.attn.channel_interaction.1.weight",
+                "layers.0.blocks.0.attn.channel_interaction.2.running_mean",
+                "layers.0.blocks.0.attn.channel_interaction.4.weight",
+                "layers.0.blocks.0.attn.spatial_interaction.0.weight",
+                "layers.0.blocks.0.attn.spatial_interaction.1.running_mean",
+                "layers.0.blocks.0.attn.spatial_interaction.3.weight",
+                "layers.0.blocks.0.attn.attns.0.rpe_biases",
+                "layers.0.blocks.0.attn.attns.0.relative_position_index",
+                "layers.0.blocks.0.attn.attns.0.pos.pos_proj.weight",
+                "layers.0.blocks.0.attn.attns.0.pos.pos1.0.weight",
+                "layers.0.blocks.0.attn.attns.0.pos.pos3.0.weight",
+                "norm.weight",
+                KeyCondition.has_any(
+                    # resi_connection="1conv"
+                    "conv_after_body.weight",
+                    # resi_connection="3conv"
+                    "conv_after_body.4.weight",
+                ),
+                KeyCondition.has_any(
+                    # upsampler="pixelshuffle"
+                    KeyCondition.has_all(
+                        "conv_after_body.weight",
+                        "conv_before_upsample.0.weight",
+                        "conv_last.weight",
+                    ),
+                    # upsampler="pixelshuffledirect"
+                    "upsample.0.weight",
+                ),
             ),
         )
 
     @override
     def load(self, state_dict: StateDict) -> ImageModelDescriptor[DAT]:
         # defaults
         img_size = 64  # cannot be deduced from state dict in general
@@ -65,19 +103,15 @@
 
         upsampler = (
             "pixelshuffle" if "conv_last.weight" in state_dict else "pixelshuffledirect"
         )
         resi_connection = "1conv" if "conv_after_body.weight" in state_dict else "3conv"
 
         if upsampler == "pixelshuffle":
-            upscale = 1
-            for i in range(0, get_seq_len(state_dict, "upsample"), 2):
-                num_feat = state_dict[f"upsample.{i}.weight"].shape[1]
-                shape = state_dict[f"upsample.{i}.weight"].shape[0]
-                upscale *= int(math.sqrt(shape // num_feat))
+            upscale, num_feat = get_pixelshuffle_params(state_dict, "upsample")
         elif upsampler == "pixelshuffledirect":
             num_feat = state_dict["upsample.0.weight"].shape[1]
             upscale = int(
                 math.sqrt(state_dict["upsample.0.weight"].shape[0] // in_chans)
             )
 
         qkv_bias = "layers.0.blocks.0.attn.qkv.bias" in state_dict
```

## spandrel/architectures/DAT/arch/DAT.py

```diff
@@ -191,16 +191,15 @@
         self.scale = qk_scale or head_dim**-0.5
 
         if idx == 0:
             H_sp, W_sp = self.split_size[0], self.split_size[1]
         elif idx == 1:
             W_sp, H_sp = self.split_size[0], self.split_size[1]
         else:
-            print("ERROR MODE", idx)
-            exit(0)
+            raise ValueError(f"ERROR MODE: {idx}")
         self.H_sp = H_sp
         self.W_sp = W_sp
 
         if self.position_bias:
             self.pos = DynamicPosBias(self.dim // 4, self.num_heads, residual=False)
             # generate mother-set
             position_bias_h = torch.arange(1 - self.H_sp, self.H_sp)
```

## spandrel/architectures/GRL/__init__.py

```diff
@@ -2,15 +2,20 @@
 
 import math
 from typing import Literal
 
 import torch
 from typing_extensions import override
 
-from spandrel.util import KeyCondition, get_scale_and_output_channels, get_seq_len
+from spandrel.util import (
+    KeyCondition,
+    get_pixelshuffle_params,
+    get_scale_and_output_channels,
+    get_seq_len,
+)
 
 from ...__helpers.canonicalize import remove_common_prefix
 from ...__helpers.model_descriptor import Architecture, ImageModelDescriptor, StateDict
 from .arch.grl import GRL
 
 _NON_PERSISTENT_BUFFERS = [
     "table_w",
@@ -46,26 +51,22 @@
 
 
 def _get_output_params(state_dict: StateDict, in_channels: int):
     out_channels: int
     upsampler: str
     upscale: int
 
-    num_out_feats = 64  # hard-coded
     if (
         "conv_before_upsample.0.weight" in state_dict
         and "upsample.up.0.weight" in state_dict
     ):
         upsampler = "pixelshuffle"
         out_channels = state_dict["conv_last.weight"].shape[0]
 
-        upscale = 1
-        for i in range(0, get_seq_len(state_dict, "upsample.up"), 2):
-            shape = state_dict[f"upsample.up.{i}.weight"].shape[0]
-            upscale *= int(math.sqrt(shape // num_out_feats))
+        upscale, _ = get_pixelshuffle_params(state_dict, "upsample.up")
     elif "upsample.up.0.weight" in state_dict:
         upsampler = "pixelshuffledirect"
         upscale, out_channels = get_scale_and_output_channels(
             state_dict["upsample.up.0.weight"].shape[0], in_channels
         )
     elif "conv_up1.weight" in state_dict:
         upsampler = "nearest+conv"
```

## spandrel/architectures/GRL/arch/grl.py

```diff
@@ -25,17 +25,15 @@
     bchw_to_blc,
     blc_to_bchw,
     calculate_mask,
     calculate_mask_all,
     get_relative_coords_table_all,
     get_relative_position_index_simple,
 )
-from .swin_v1_block import (
-    build_last_conv,
-)
+from .swin_v1_block import build_last_conv
 from .upsample import Upsample, UpsampleOneStep
 
 
 class TransformerStage(nn.Module):
     """Transformer stage.
     Args:
         dim (int): Number of input channels.
@@ -147,30 +145,30 @@
 
         self.conv = build_last_conv(conv_type, dim)
 
     def _init_weights(self):
         for n, m in self.named_modules():
             if self.init_method == "w":
                 if isinstance(m, (nn.Linear, nn.Conv2d)) and n.find("cpb_mlp") < 0:
-                    print("nn.Linear and nn.Conv2d weight initilization")
+                    # print("nn.Linear and nn.Conv2d weight initilization")
                     m.weight.data *= 0.1
             elif self.init_method == "l":
                 if isinstance(m, nn.LayerNorm):
-                    print("nn.LayerNorm initialization")
+                    # print("nn.LayerNorm initialization")
                     nn.init.constant_(m.bias, 0)
                     nn.init.constant_(m.weight, 0)
             elif self.init_method.find("t") >= 0:
                 scale = 0.1 ** (len(self.init_method) - 1) * int(self.init_method[-1])
                 if isinstance(m, nn.Linear) and n.find("cpb_mlp") < 0:
                     trunc_normal_(m.weight, std=scale)
                 elif isinstance(m, nn.Conv2d):
                     m.weight.data *= 0.1
-                print(
-                    "Initialization nn.Linear - trunc_normal; nn.Conv2d - weight rescale."
-                )
+                # print(
+                #     "Initialization nn.Linear - trunc_normal; nn.Conv2d - weight rescale."
+                # )
             else:
                 raise NotImplementedError(
                     f"Parameter initialization method {self.init_method} not implemented in TransformerStage."
                 )
 
     def forward(self, x, x_size, table_index_mask):
         res = x
@@ -553,25 +551,7 @@
                 x = x + self.conv_last(res)
             else:
                 x = self.conv_last(res)
 
         x = x / self.img_range + self.mean
 
         return x[:, :, : H * self.upscale, : W * self.upscale]
-
-    def flops(self):
-        pass
-
-    def convert_checkpoint(self, state_dict):
-        for k in list(state_dict.keys()):
-            if (
-                k.find("relative_coords_table") >= 0
-                or k.find("relative_position_index") >= 0
-                or k.find("attn_mask") >= 0
-                or k.find("model.table_") >= 0
-                or k.find("model.index_") >= 0
-                or k.find("model.mask_") >= 0
-                # or k.find(".upsample.") >= 0
-            ):
-                state_dict.pop(k)
-                print(k)
-        return state_dict
```

## spandrel/architectures/GRL/arch/mixed_attn_block.py

```diff
@@ -1,647 +1,28 @@
 from __future__ import annotations
 
 import math
-from abc import ABC
-from math import prod
 
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 
 from .config import GRLConfig
-from .ops import (
-    bchw_to_bhwc,
-    bchw_to_blc,
-    blc_to_bchw,
-    blc_to_bhwc,
-    calculate_mask,
-    calculate_mask_all,
-    get_relative_coords_table_all,
-    get_relative_position_index_simple,
-    window_partition,
-    window_reverse,
-)
+from .ops import bchw_to_bhwc, bchw_to_blc, blc_to_bchw, blc_to_bhwc
 
 
 class CPB_MLP(nn.Sequential):
     def __init__(self, in_channels, out_channels, channels=512):
         m = [
             nn.Linear(in_channels, channels, bias=True),
             nn.ReLU(inplace=True),
             nn.Linear(channels, out_channels, bias=False),
         ]
         super().__init__(*m)
 
 
-class AffineTransformWindow(nn.Module):
-    r"""Affine transformation of the attention map.
-    The window is a square window.
-    Supports attention between different window sizes
-    """
-
-    def __init__(
-        self,
-        num_heads,
-        input_resolution,
-        window_size,
-        pretrained_window_size=[0, 0],
-        shift_size=0,
-        anchor_window_down_factor=1,
-        args: GRLConfig = None,  # type: ignore
-    ):
-        super().__init__()
-        # print("AffineTransformWindow", args)
-        self.num_heads = num_heads
-        self.input_resolution = input_resolution
-        self.window_size = window_size
-        self.pretrained_window_size = pretrained_window_size
-        self.shift_size = shift_size
-        self.anchor_window_down_factor = anchor_window_down_factor
-        self.use_buffer = args.use_buffer
-
-        logit_scale = torch.log(10 * torch.ones((num_heads, 1, 1)))
-        self.logit_scale = nn.Parameter(logit_scale, requires_grad=True)
-
-        # mlp to generate continuous relative position bias
-        self.cpb_mlp = CPB_MLP(2, num_heads)
-        if self.use_buffer:
-            table = get_relative_coords_table_all(
-                window_size, pretrained_window_size, anchor_window_down_factor
-            )
-            index = get_relative_position_index_simple(
-                window_size, anchor_window_down_factor
-            )
-            self.register_buffer("relative_coords_table", table)
-            self.register_buffer("relative_position_index", index)
-
-            if self.shift_size > 0:
-                attn_mask = calculate_mask(
-                    input_resolution, self.window_size, self.shift_size
-                )
-            else:
-                attn_mask = None
-            self.register_buffer("attn_mask", attn_mask)
-
-    def forward(self, attn, x_size):
-        B_, _H, N, _ = attn.shape
-        device = attn.device
-        # logit scale
-        attn = attn * torch.clamp(self.logit_scale, max=math.log(1.0 / 0.01)).exp()
-
-        # relative position bias
-        if self.use_buffer:
-            table = self.relative_coords_table
-            index = self.relative_position_index
-        else:
-            table = get_relative_coords_table_all(
-                self.window_size,
-                self.pretrained_window_size,
-                self.anchor_window_down_factor,
-            ).to(device)
-            index = get_relative_position_index_simple(
-                self.window_size, self.anchor_window_down_factor
-            ).to(device)
-
-        bias_table = self.cpb_mlp(table)  # 2*Wh-1, 2*Ww-1, num_heads
-        bias_table = bias_table.view(-1, self.num_heads)
-
-        win_dim = prod(self.window_size)
-        bias = bias_table[index.view(-1)]
-        bias = bias.view(win_dim, win_dim, -1).permute(2, 0, 1).contiguous()
-        # nH, Wh*Ww, Wh*Ww
-        bias = 16 * torch.sigmoid(bias)
-        attn = attn + bias.unsqueeze(0)
-
-        # W-MSA/SW-MSA
-        if self.use_buffer:
-            mask = self.attn_mask
-            # during test and window shift, recalculate the mask
-            if self.input_resolution != x_size and self.shift_size > 0:
-                mask = calculate_mask(x_size, self.window_size, self.shift_size)
-                mask = mask.to(attn.device)
-        else:
-            if self.shift_size > 0:
-                mask = calculate_mask(x_size, self.window_size, self.shift_size)
-                mask = mask.to(attn.device)
-            else:
-                mask = None
-
-        # shift attention mask
-        if mask is not None:
-            nW = mask.shape[0]
-            mask = mask.unsqueeze(1).unsqueeze(0)
-            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask
-            attn = attn.view(-1, self.num_heads, N, N)
-
-        return attn
-
-
-class AffineTransformStripe(nn.Module):
-    r"""Affine transformation of the attention map.
-    The window is a stripe window. Supports attention between different window sizes
-    """
-
-    def __init__(
-        self,
-        num_heads,
-        input_resolution,
-        stripe_size,
-        stripe_groups,
-        stripe_shift,
-        pretrained_stripe_size=[0, 0],
-        anchor_window_down_factor=1,
-        window_to_anchor=True,
-        args: GRLConfig = None,  # type: ignore
-    ):
-        super().__init__()
-        self.num_heads = num_heads
-        self.input_resolution = input_resolution
-        self.stripe_size = stripe_size
-        self.stripe_groups = stripe_groups
-        self.pretrained_stripe_size = pretrained_stripe_size
-        # TODO: be careful when determining the pretrained_stripe_size
-        self.stripe_shift = stripe_shift
-        stripe_size, shift_size = self._get_stripe_info(input_resolution)
-        self.anchor_window_down_factor = anchor_window_down_factor
-        self.window_to_anchor = window_to_anchor
-        self.use_buffer = args.use_buffer
-
-        logit_scale = torch.log(10 * torch.ones((num_heads, 1, 1)))
-        self.logit_scale = nn.Parameter(logit_scale, requires_grad=True)
-
-        # mlp to generate continuous relative position bias
-        self.cpb_mlp = CPB_MLP(2, num_heads)
-        if self.use_buffer:
-            table = get_relative_coords_table_all(
-                stripe_size, pretrained_stripe_size, anchor_window_down_factor
-            )
-            index = get_relative_position_index_simple(
-                stripe_size, anchor_window_down_factor, window_to_anchor
-            )
-            self.register_buffer("relative_coords_table", table)
-            self.register_buffer("relative_position_index", index)
-
-            if self.stripe_shift:
-                attn_mask = calculate_mask_all(
-                    input_resolution,
-                    stripe_size,
-                    shift_size,
-                    anchor_window_down_factor,
-                    window_to_anchor,
-                )
-            else:
-                attn_mask = None
-            self.register_buffer("attn_mask", attn_mask)
-
-    def forward(self, attn, x_size):
-        B_, _H, N1, N2 = attn.shape
-        device = attn.device
-        # logit scale
-        attn = attn * torch.clamp(self.logit_scale, max=math.log(1.0 / 0.01)).exp()
-
-        # relative position bias
-        stripe_size, shift_size = self._get_stripe_info(x_size)
-        fixed_stripe_size = (
-            self.stripe_groups[0] is None and self.stripe_groups[1] is None
-        )
-        if not self.use_buffer or (
-            self.use_buffer
-            and self.input_resolution != x_size
-            and not fixed_stripe_size
-        ):
-            # during test and stripe size is not fixed.
-            pretrained_stripe_size = (
-                self.pretrained_stripe_size
-            )  # or stripe_size; Needs further pondering
-            table = get_relative_coords_table_all(
-                stripe_size, pretrained_stripe_size, self.anchor_window_down_factor
-            )
-            table = table.to(device)
-            index = get_relative_position_index_simple(
-                stripe_size, self.anchor_window_down_factor, self.window_to_anchor
-            ).to(device)
-        else:
-            table = self.relative_coords_table
-            index = self.relative_position_index
-        # The same table size-> 1, Wh+AWh-1, Ww+AWw-1, 2
-        # But different index size -> # Wh*Ww, AWh*AWw
-        # if N1 < N2:
-        #     index = index.transpose(0, 1)
-
-        bias_table = self.cpb_mlp(table).view(-1, self.num_heads)
-        # if not self.training:
-        #     print(bias_table.shape, index.max(), index.min())
-        bias = bias_table[index.view(-1)]
-        bias = bias.view(N1, N2, -1).permute(2, 0, 1).contiguous()
-        # nH, Wh*Ww, Wh*Ww
-        bias = 16 * torch.sigmoid(bias)
-        # print(N1, N2, attn.shape, bias.unsqueeze(0).shape)
-        attn = attn + bias.unsqueeze(0)
-
-        # W-MSA/SW-MSA
-        if self.use_buffer:
-            mask = self.attn_mask
-            # during test and window shift, recalculate the mask
-            if self.input_resolution != x_size and self.stripe_shift > 0:
-                mask = calculate_mask_all(
-                    x_size,
-                    stripe_size,
-                    shift_size,
-                    self.anchor_window_down_factor,
-                    self.window_to_anchor,
-                )
-                mask = mask.to(device)
-        else:
-            if self.stripe_shift > 0:
-                mask = calculate_mask_all(
-                    x_size,
-                    stripe_size,
-                    shift_size,
-                    self.anchor_window_down_factor,
-                    self.window_to_anchor,
-                )
-                mask = mask.to(attn.device)
-            else:
-                mask = None
-
-        # shift attention mask
-        if mask is not None:
-            nW = mask.shape[0]
-            mask = mask.unsqueeze(1).unsqueeze(0)
-            attn = attn.view(B_ // nW, nW, self.num_heads, N1, N2) + mask
-            attn = attn.view(-1, self.num_heads, N1, N2)
-
-        return attn
-
-    def _get_stripe_info(self, input_resolution):
-        stripe_size, shift_size = [], []
-        for s, g, d in zip(self.stripe_size, self.stripe_groups, input_resolution):
-            if g is None:
-                stripe_size.append(s)
-                shift_size.append(s // 2 if self.stripe_shift else 0)
-            else:
-                stripe_size.append(d // g)
-                shift_size.append(0 if g == 1 else d // (g * 2))
-        return stripe_size, shift_size
-
-
-class Attention(ABC, nn.Module):
-    def __init__(self):
-        super().__init__()
-
-    def attn(self, q, k, v, attn_transform, x_size, reshape=True):
-        # cosine attention map
-        B_, _, H, head_dim = q.shape
-        if self.euclidean_dist:
-            attn = torch.norm(q.unsqueeze(-2) - k.unsqueeze(-3), dim=-1)
-        else:
-            attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)
-        attn = attn_transform(attn, x_size)
-        # attention
-        attn = self.softmax(attn)
-        attn = self.attn_drop(attn)
-        x = attn @ v  # B_, H, N1, head_dim
-        if reshape:
-            x = x.transpose(1, 2).reshape(B_, -1, H * head_dim)
-        # B_, N, C
-        return x
-
-
-class WindowAttention(Attention):
-    r"""Window attention. QKV is the input to the forward method.
-    Args:
-        num_heads (int): Number of attention heads.
-        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
-        pretrained_window_size (tuple[int]): The height and width of the window in pre-training.
-    """
-
-    def __init__(
-        self,
-        input_resolution,
-        window_size: tuple[int, int],
-        num_heads,
-        window_shift=False,
-        attn_drop=0.0,
-        pretrained_window_size=[0, 0],
-        args: GRLConfig = None,  # type: ignore
-    ):
-        super().__init__()
-        self.input_resolution = input_resolution
-        self.window_size = window_size
-        self.pretrained_window_size = pretrained_window_size
-        self.num_heads = num_heads
-        self.shift_size = window_size[0] // 2 if window_shift else 0
-        self.euclidean_dist = args.euclidean_dist
-
-        self.attn_transform = AffineTransformWindow(
-            num_heads,
-            input_resolution,
-            window_size,
-            pretrained_window_size,
-            self.shift_size,
-            args=args,
-        )
-        self.attn_drop = nn.Dropout(attn_drop)
-        self.softmax = nn.Softmax(dim=-1)
-
-    def forward(self, qkv, x_size):
-        """
-        Args:
-            qkv: input QKV features with shape of (B, L, 3C)
-            x_size: use x_size to determine whether the relative positional bias table and index
-            need to be regenerated.
-        """
-        H, W = x_size
-        B, L, C = qkv.shape
-        qkv = qkv.view(B, H, W, C)
-
-        # cyclic shift
-        if self.shift_size > 0:
-            qkv = torch.roll(
-                qkv, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)
-            )
-
-        # partition windows
-        qkv = window_partition(qkv, self.window_size)  # nW*B, wh, ww, C
-        qkv = qkv.view(-1, prod(self.window_size), C)  # nW*B, wh*ww, C
-
-        B_, N, _ = qkv.shape
-        qkv = qkv.reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
-        q, k, v = qkv[0], qkv[1], qkv[2]
-
-        # attention
-        x = self.attn(q, k, v, self.attn_transform, x_size)
-
-        # merge windows
-        x = x.view(-1, *self.window_size, C // 3)
-        x = window_reverse(x, self.window_size, x_size)  # B, H, W, C/3
-
-        # reverse cyclic shift
-        if self.shift_size > 0:
-            x = torch.roll(x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
-        x = x.view(B, L, C // 3)
-
-        return x
-
-    def extra_repr(self) -> str:
-        return (
-            f"window_size={self.window_size}, shift_size={self.shift_size}, "
-            f"pretrained_window_size={self.pretrained_window_size}, num_heads={self.num_heads}"
-        )
-
-    def flops(self, N):
-        # calculate flops for 1 window with token length of N
-        flops = 0
-        # qkv = self.qkv(x)
-        flops += N * self.dim * 3 * self.dim
-        # attn = (q @ k.transpose(-2, -1))
-        flops += self.num_heads * N * (self.dim // self.num_heads) * N
-        #  x = (attn @ v)
-        flops += self.num_heads * N * N * (self.dim // self.num_heads)
-        # x = self.proj(x)
-        flops += N * self.dim * self.dim
-        return flops
-
-
-class StripeAttention(Attention):
-    r"""Stripe attention
-    Args:
-        stripe_size (tuple[int]): The height and width of the stripe.
-        num_heads (int): Number of attention heads.
-        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
-        pretrained_stripe_size (tuple[int]): The height and width of the stripe in pre-training.
-    """
-
-    def __init__(
-        self,
-        input_resolution,
-        stripe_size,
-        stripe_groups,
-        stripe_shift,
-        num_heads,
-        attn_drop=0.0,
-        pretrained_stripe_size=[0, 0],
-        args: GRLConfig = None,  # type: ignore
-    ):
-        super().__init__()
-        self.input_resolution = input_resolution
-        self.stripe_size = stripe_size  # Wh, Ww
-        self.stripe_groups = stripe_groups
-        self.stripe_shift = stripe_shift
-        self.num_heads = num_heads
-        self.pretrained_stripe_size = pretrained_stripe_size
-        self.euclidean_dist = args.euclidean_dist
-
-        self.attn_transform = AffineTransformStripe(
-            num_heads,
-            input_resolution,
-            stripe_size,
-            stripe_groups,
-            stripe_shift,
-            pretrained_stripe_size,
-            anchor_window_down_factor=1,
-            args=args,
-        )
-        self.attn_drop = nn.Dropout(attn_drop)
-        self.softmax = nn.Softmax(dim=-1)
-
-    def forward(self, qkv, x_size):
-        """
-        Args:
-            x: input features with shape of (B, L, C)
-            stripe_size: use stripe_size to determine whether the relative positional bias table and index
-            need to be regenerated.
-        """
-        H, W = x_size
-        B, L, C = qkv.shape
-        qkv = qkv.view(B, H, W, C)
-
-        running_stripe_size, running_shift_size = self.attn_transform._get_stripe_info(  # type: ignore
-            x_size
-        )
-        # cyclic shift
-        if self.stripe_shift:
-            qkv = torch.roll(
-                qkv,
-                shifts=(-running_shift_size[0], -running_shift_size[1]),
-                dims=(1, 2),
-            )
-
-        # partition windows
-        qkv = window_partition(qkv, running_stripe_size)  # nW*B, wh, ww, C
-        qkv = qkv.view(-1, prod(running_stripe_size), C)  # nW*B, wh*ww, C
-
-        B_, N, _ = qkv.shape
-        qkv = qkv.reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
-        q, k, v = qkv[0], qkv[1], qkv[2]
-
-        # attention
-        x = self.attn(q, k, v, self.attn_transform, x_size)
-
-        # merge windows
-        x = x.view(-1, *running_stripe_size, C // 3)
-        x = window_reverse(x, running_stripe_size, x_size)  # B H W C/3
-
-        # reverse the shift
-        if self.stripe_shift:
-            x = torch.roll(x, shifts=running_shift_size, dims=(1, 2))
-
-        x = x.view(B, L, C // 3)
-        return x
-
-    def extra_repr(self) -> str:
-        return (
-            f"stripe_size={self.stripe_size}, stripe_groups={self.stripe_groups}, stripe_shift={self.stripe_shift}, "
-            f"pretrained_stripe_size={self.pretrained_stripe_size}, num_heads={self.num_heads}"
-        )
-
-    def flops(self, N):
-        # calculate flops for 1 window with token length of N
-        flops = 0
-        # qkv = self.qkv(x)
-        flops += N * self.dim * 3 * self.dim
-        # attn = (q @ k.transpose(-2, -1))
-        flops += self.num_heads * N * (self.dim // self.num_heads) * N
-        #  x = (attn @ v)
-        flops += self.num_heads * N * N * (self.dim // self.num_heads)
-        # x = self.proj(x)
-        flops += N * self.dim * self.dim
-        return flops
-
-
-class AnchorStripeAttention(Attention):
-    r"""Stripe attention
-    Args:
-        stripe_size (tuple[int]): The height and width of the stripe.
-        num_heads (int): Number of attention heads.
-        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
-        pretrained_stripe_size (tuple[int]): The height and width of the stripe in pre-training.
-    """
-
-    def __init__(
-        self,
-        input_resolution,
-        stripe_size,
-        stripe_groups,
-        stripe_shift,
-        num_heads,
-        attn_drop=0.0,
-        pretrained_stripe_size=[0, 0],
-        anchor_window_down_factor=1,
-        args: GRLConfig = None,  # type: ignore
-    ):
-        super().__init__()
-        self.input_resolution = input_resolution
-        self.stripe_size = stripe_size  # Wh, Ww
-        self.stripe_groups = stripe_groups
-        self.stripe_shift = stripe_shift
-        self.num_heads = num_heads
-        self.pretrained_stripe_size = pretrained_stripe_size
-        self.anchor_window_down_factor = anchor_window_down_factor
-        self.euclidean_dist = args.euclidean_dist
-
-        self.attn_transform1 = AffineTransformStripe(
-            num_heads,
-            input_resolution,
-            stripe_size,
-            stripe_groups,
-            stripe_shift,
-            pretrained_stripe_size,
-            anchor_window_down_factor,
-            window_to_anchor=False,
-            args=args,
-        )
-
-        self.attn_transform2 = AffineTransformStripe(
-            num_heads,
-            input_resolution,
-            stripe_size,
-            stripe_groups,
-            stripe_shift,
-            pretrained_stripe_size,
-            anchor_window_down_factor,
-            window_to_anchor=True,
-            args=args,
-        )
-
-        self.attn_drop = nn.Dropout(attn_drop)
-        self.softmax = nn.Softmax(dim=-1)
-
-    def forward(self, qkv, anchor, x_size):
-        """
-        Args:
-            qkv: input features with shape of (B, L, C)
-            anchor:
-            x_size: use stripe_size to determine whether the relative positional bias table and index
-            need to be regenerated.
-        """
-        H, W = x_size
-        B, _L, C = qkv.shape
-        qkv = qkv.view(B, H, W, C)
-
-        stripe_size, shift_size = self.attn_transform1._get_stripe_info(x_size)  # type: ignore
-        anchor_stripe_size = [s // self.anchor_window_down_factor for s in stripe_size]
-        anchor_shift_size = [s // self.anchor_window_down_factor for s in shift_size]
-        # cyclic shift
-        if self.stripe_shift:
-            qkv = torch.roll(qkv, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))
-            anchor = torch.roll(
-                anchor,
-                shifts=(-anchor_shift_size[0], -anchor_shift_size[1]),
-                dims=(1, 2),
-            )
-
-        # partition windows
-        qkv = window_partition(qkv, stripe_size)  # nW*B, wh, ww, C
-        qkv = qkv.view(-1, prod(stripe_size), C)  # nW*B, wh*ww, C
-        anchor = window_partition(anchor, anchor_stripe_size)
-        anchor = anchor.view(-1, prod(anchor_stripe_size), C // 3)
-
-        B_, N1, _ = qkv.shape
-        N2 = anchor.shape[1]
-        qkv = qkv.reshape(B_, N1, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
-        q, k, v = qkv[0], qkv[1], qkv[2]
-        anchor = anchor.reshape(B_, N2, self.num_heads, -1).permute(0, 2, 1, 3)
-
-        # attention
-        x = self.attn(anchor, k, v, self.attn_transform1, x_size, False)
-        x = self.attn(q, anchor, x, self.attn_transform2, x_size)
-
-        # merge windows
-        x = x.view(B_, *stripe_size, C // 3)
-        x = window_reverse(x, stripe_size, x_size)  # B H' W' C
-
-        # reverse the shift
-        if self.stripe_shift:
-            x = torch.roll(x, shifts=shift_size, dims=(1, 2))
-
-        x = x.view(B, H * W, C // 3)
-        return x
-
-    def extra_repr(self) -> str:
-        return (
-            f"stripe_size={self.stripe_size}, stripe_groups={self.stripe_groups}, stripe_shift={self.stripe_shift}, "
-            f"pretrained_stripe_size={self.pretrained_stripe_size}, num_heads={self.num_heads}, anchor_window_down_factor={self.anchor_window_down_factor}"
-        )
-
-    def flops(self, N):
-        # calculate flops for 1 window with token length of N
-        flops = 0
-        # qkv = self.qkv(x)
-        flops += N * self.dim * 3 * self.dim
-        # attn = (q @ k.transpose(-2, -1))
-        flops += self.num_heads * N * (self.dim // self.num_heads) * N
-        #  x = (attn @ v)
-        flops += self.num_heads * N * N * (self.dim // self.num_heads)
-        # x = self.proj(x)
-        flops += N * self.dim * self.dim
-        return flops
-
-
 class SeparableConv(nn.Sequential):
     def __init__(
         self, in_channels, out_channels, kernel_size, stride, bias, args: GRLConfig
     ):
         m: list[torch.nn.Module] = [
             nn.Conv2d(
                 in_channels,
@@ -793,173 +174,14 @@
         else:
             for i, m in enumerate(self.body):
                 x = m(x, [s // 2**i for s in x_size])
             x = blc_to_bhwc(x, [s // 2 ** (i + 1) for s in x_size])  # type: ignore
         return x
 
 
-class MixedAttention(nn.Module):
-    r"""Mixed window attention and stripe attention
-    Args:
-        dim (int): Number of input channels.
-        stripe_size (tuple[int]): The height and width of the stripe.
-        num_heads (int): Number of attention heads.
-        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
-        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
-        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
-        pretrained_stripe_size (tuple[int]): The height and width of the stripe in pre-training.
-    """
-
-    def __init__(
-        self,
-        dim,
-        input_resolution,
-        num_heads_w,
-        num_heads_s,
-        window_size,
-        window_shift,
-        stripe_size,
-        stripe_groups,
-        stripe_shift,
-        qkv_bias=True,
-        qkv_proj_type="linear",
-        anchor_proj_type="separable_conv",
-        anchor_one_stage=True,
-        anchor_window_down_factor=1,
-        attn_drop=0.0,
-        proj_drop=0.0,
-        pretrained_window_size=[0, 0],
-        pretrained_stripe_size=[0, 0],
-        args: GRLConfig = None,  # type: ignore
-    ):
-        super().__init__()
-        self.dim = dim
-        self.input_resolution = input_resolution
-        self.use_anchor = anchor_window_down_factor > 1
-        self.args = args
-        # print(args)
-        self.qkv = QKVProjection(dim, qkv_bias, qkv_proj_type, args)
-        if self.use_anchor:
-            # anchor is only used for stripe attention
-            self.anchor = AnchorProjection(
-                dim, anchor_proj_type, anchor_one_stage, anchor_window_down_factor, args
-            )
-
-        self.window_attn = WindowAttention(
-            input_resolution,
-            window_size,
-            num_heads_w,
-            window_shift,
-            attn_drop,
-            pretrained_window_size,
-            args,
-        )
-
-        if self.args.double_window:
-            self.stripe_attn = WindowAttention(
-                input_resolution,
-                window_size,
-                num_heads_w,
-                window_shift,
-                attn_drop,
-                pretrained_window_size,
-                args,
-            )
-        else:
-            if self.use_anchor:
-                self.stripe_attn = AnchorStripeAttention(
-                    input_resolution,
-                    stripe_size,
-                    stripe_groups,
-                    stripe_shift,
-                    num_heads_s,
-                    attn_drop,
-                    pretrained_stripe_size,
-                    anchor_window_down_factor,
-                    args,
-                )
-            else:
-                if self.args.stripe_square:
-                    self.stripe_attn = StripeAttention(
-                        input_resolution,
-                        window_size,
-                        [None, None],
-                        window_shift,
-                        num_heads_s,
-                        attn_drop,
-                        pretrained_stripe_size,
-                        args,
-                    )
-                else:
-                    self.stripe_attn = StripeAttention(
-                        input_resolution,
-                        stripe_size,
-                        stripe_groups,
-                        stripe_shift,
-                        num_heads_s,
-                        attn_drop,
-                        pretrained_stripe_size,
-                        args,
-                    )
-        if self.args.out_proj_type == "linear":
-            self.proj = nn.Linear(dim, dim)
-        else:
-            self.proj = nn.Conv2d(dim, dim, 3, 1, 1)
-        self.proj_drop = nn.Dropout(proj_drop)
-
-    def forward(self, x, x_size):
-        """
-        Args:
-            x: input features with shape of (B, L, C)
-            stripe_size: use stripe_size to determine whether the relative positional bias table and index
-            need to be regenerated.
-        """
-        _B, _L, C = x.shape
-
-        # qkv projection
-        qkv = self.qkv(x, x_size)
-        qkv_window, qkv_stripe = torch.split(qkv, C * 3 // 2, dim=-1)
-        # anchor projection
-        if self.use_anchor:
-            anchor = self.anchor(x, x_size)
-
-        # attention
-        x_window = self.window_attn(qkv_window, x_size)
-        if self.use_anchor:
-            x_stripe = self.stripe_attn(qkv_stripe, anchor, x_size)  # type: ignore
-        else:
-            x_stripe = self.stripe_attn(qkv_stripe, x_size)
-        x = torch.cat([x_window, x_stripe], dim=-1)
-
-        # output projection
-        if self.args.out_proj_type == "linear":
-            x = self.proj(x)
-        else:
-            x = blc_to_bchw(x, x_size)
-            x = bchw_to_blc(self.proj(x))
-        x = self.proj_drop(x)
-        return x
-
-    def extra_repr(self) -> str:
-        return f"dim={self.dim}, input_resolution={self.input_resolution}"
-
-    def flops(self, N):
-        # calculate flops for 1 window with token length of N
-        flops = 0
-        # qkv = self.qkv(x)
-        flops += N * self.dim * 3 * self.dim
-        # attn = (q @ k.transpose(-2, -1))
-        flops += self.num_heads * N * (self.dim // self.num_heads) * N
-        #  x = (attn @ v)
-        flops += self.num_heads * N * N * (self.dim // self.num_heads)
-        # x = self.proj(x)
-        flops += N * self.dim * self.dim
-        return flops
-
-
 class ChannelAttention(nn.Module):
     """Channel attention used in RCAN.
     Args:
         num_feat (int): Channel number of intermediate features.
         reduction (int): Channel reduction factor. Default: 16.
     """
 
@@ -988,150 +210,7 @@
             nn.Conv2d(num_feat // compress_ratio, num_feat, 3, 1, 1),
             ChannelAttention(num_feat, reduction),
         )
 
     def forward(self, x, x_size):
         x = self.cab(blc_to_bchw(x, x_size).contiguous())
         return bchw_to_blc(x)
-
-
-class MixAttnTransformerBlock(nn.Module):
-    r"""Mix attention transformer block with shared QKV projection and output projection for mixed attention modules.
-    Args:
-        dim (int): Number of input channels.
-        input_resolution (tuple[int]): Input resulotion.
-        num_heads (int): Number of attention heads.
-        window_size (int): Window size.
-        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
-        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
-        drop (float, optional): Dropout rate. Default: 0.0
-        attn_drop (float, optional): Attention dropout rate. Default: 0.0
-        drop_path (float, optional): Stochastic depth rate. Default: 0.0
-        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
-        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
-        pretrained_stripe_size (int): Window size in pre-training.
-        attn_type (str, optional): Attention type. Default: cwhv.
-                    c: residual blocks
-                    w: window attention
-                    h: horizontal stripe attention
-                    v: vertical stripe attention
-    """
-
-    def __init__(
-        self,
-        dim,
-        input_resolution,
-        num_heads_w,
-        num_heads_s,
-        window_size=7,
-        window_shift=False,
-        stripe_size=[8, 8],
-        stripe_groups=[None, None],
-        stripe_shift=False,
-        stripe_type="H",
-        mlp_ratio=4.0,
-        qkv_bias=True,
-        qkv_proj_type="linear",
-        anchor_proj_type="separable_conv",
-        anchor_one_stage=True,
-        anchor_window_down_factor=1,
-        drop=0.0,
-        attn_drop=0.0,
-        drop_path=0.0,
-        act_layer=nn.GELU,
-        norm_layer=nn.LayerNorm,
-        pretrained_window_size=[0, 0],
-        pretrained_stripe_size=[0, 0],
-        res_scale=1.0,
-        args: GRLConfig = None,  # type: ignore
-    ):
-        super().__init__()
-        self.dim = dim
-        self.input_resolution = input_resolution
-        self.num_heads_w = num_heads_w
-        self.num_heads_s = num_heads_s
-        self.window_size = window_size
-        self.window_shift = window_shift
-        self.stripe_shift = stripe_shift
-        self.stripe_type = stripe_type
-        self.args = args
-        if self.stripe_type == "W":
-            self.stripe_size = stripe_size[::-1]
-            self.stripe_groups = stripe_groups[::-1]
-        else:
-            self.stripe_size = stripe_size
-            self.stripe_groups = stripe_groups
-        self.mlp_ratio = mlp_ratio
-        self.res_scale = res_scale
-
-        self.attn = MixedAttention(
-            dim,
-            input_resolution,
-            num_heads_w,
-            num_heads_s,
-            window_size,
-            window_shift,
-            self.stripe_size,
-            self.stripe_groups,
-            stripe_shift,
-            qkv_bias,
-            qkv_proj_type,
-            anchor_proj_type,
-            anchor_one_stage,
-            anchor_window_down_factor,
-            attn_drop,
-            drop,
-            pretrained_window_size,
-            pretrained_stripe_size,
-            args,
-        )
-        self.norm1 = norm_layer(dim)
-        if self.args.local_connection:
-            self.conv = CAB(dim)
-
-    #         self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
-
-    #         self.mlp = Mlp(
-    #             in_features=dim,
-    #             hidden_features=int(dim * mlp_ratio),
-    #             act_layer=act_layer,
-    #             drop=drop,
-    #         )
-    #         self.norm2 = norm_layer(dim)
-
-    def forward(self, x, x_size):
-        # Mixed attention
-        if self.args.local_connection:
-            x = (
-                x
-                + self.res_scale * self.drop_path(self.norm1(self.attn(x, x_size)))
-                + self.conv(x, x_size)
-            )
-        else:
-            x = x + self.res_scale * self.drop_path(self.norm1(self.attn(x, x_size)))
-        # FFN
-        x = x + self.res_scale * self.drop_path(self.norm2(self.mlp(x)))
-
-    #         return x
-
-    def extra_repr(self) -> str:
-        return (
-            f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads=({self.num_heads_w}, {self.num_heads_s}), "
-            f"window_size={self.window_size}, window_shift={self.window_shift}, "
-            f"stripe_size={self.stripe_size}, stripe_groups={self.stripe_groups}, stripe_shift={self.stripe_shift}, self.stripe_type={self.stripe_type}, "
-            f"mlp_ratio={self.mlp_ratio}, res_scale={self.res_scale}"
-        )
-
-
-#     def flops(self):
-#         flops = 0
-#         H, W = self.input_resolution
-#         # norm1
-#         flops += self.dim * H * W
-#         # W-MSA/SW-MSA
-#         nW = H * W / self.stripe_size[0] / self.stripe_size[1]
-#         flops += nW * self.attn.flops(self.stripe_size[0] * self.stripe_size[1])
-#         # mlp
-#         flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio
-#         # norm2
-#         flops += self.dim * H * W
-#         return flops
```

## spandrel/architectures/GRL/arch/mixed_attn_block_efficient.py

```diff
@@ -7,24 +7,16 @@
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from spandrel.util.timm import DropPath, to_2tuple
 
 from .config import GRLConfig
-from .mixed_attn_block import (
-    CAB,
-    CPB_MLP,
-    AnchorProjection,
-    QKVProjection,
-)
-from .ops import (
-    window_partition,
-    window_reverse,
-)
+from .mixed_attn_block import CAB, CPB_MLP, AnchorProjection, QKVProjection
+from .ops import window_partition, window_reverse
 from .swin_v1_block import Mlp
 
 
 class AffineTransform(nn.Module):
     r"""Affine transformation of the attention map.
     The window could be a square window or a stripe window. Supports attention between different window sizes
     """
```

## spandrel/architectures/GRL/arch/ops.py

```diff
@@ -9,19 +9,14 @@
 
 
 def bchw_to_bhwc(x: torch.Tensor) -> torch.Tensor:
     """Permutes a tensor from the shape (B, C, H, W) to (B, H, W, C)."""
     return x.permute(0, 2, 3, 1)
 
 
-def bhwc_to_bchw(x: torch.Tensor) -> torch.Tensor:
-    """Permutes a tensor from the shape (B, H, W, C) to (B, C, H, W)."""
-    return x.permute(0, 3, 1, 2)
-
-
 def bchw_to_blc(x: torch.Tensor) -> torch.Tensor:
     """Rearrange a tensor from the shape (B, C, H, W) to (B, L, C)."""
     return x.flatten(2).transpose(1, 2)
 
 
 def blc_to_bchw(x: torch.Tensor, x_size: tuple[int, int]) -> torch.Tensor:
     """Rearrange a tensor from the shape (B, L, C) to (B, C, H, W)."""
@@ -159,84 +154,25 @@
     attn_mask = attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(
         attn_mask == 0, 0.0
     )  # nW, Wh**Ww, AWh*AWw
 
     return attn_mask
 
 
-def calculate_win_mask(
-    input_resolution1, input_resolution2, window_size1, window_size2
-):
-    """
-    Use case: 2)
-    """
-    # calculate attention mask for SW-MSA
-
-    # mask of window1: nW, Wh**Ww
-    mask_windows1 = _fill_window(input_resolution1, window_size1)
-    # mask of window2: nW, AWh*AWw
-    mask_windows2 = _fill_window(input_resolution2, window_size2)
-
-    attn_mask = mask_windows1.unsqueeze(2) - mask_windows2.unsqueeze(1)
-    attn_mask = attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(
-        attn_mask == 0, 0.0
-    )  # nW, Wh**Ww, AWh*AWw
-
-    return attn_mask
-
-
 def _get_meshgrid_coords(
     start_coords: tuple[int, int] | list[int],
     end_coords: tuple[int, int] | list[int],
 ):
     coord_h = torch.arange(start_coords[0], end_coords[0])
     coord_w = torch.arange(start_coords[1], end_coords[1])
     coords = torch.stack(torch.meshgrid([coord_h, coord_w], indexing="ij"))  # 2, Wh, Ww
     coords = torch.flatten(coords, 1)  # 2, Wh*Ww
     return coords
 
 
-def get_relative_coords_table(
-    window_size: tuple[int, int] | list[int],
-    pretrained_window_size: tuple[int, int] | list[int] = [0, 0],
-    anchor_window_down_factor=1,
-):
-    """
-    Use case: 1)
-    """
-    # get relative_coords_table
-    ws = window_size
-    aws = [w // anchor_window_down_factor for w in window_size]
-    pws = pretrained_window_size
-    paws = [w // anchor_window_down_factor for w in pretrained_window_size]
-
-    ts = [(w1 + w2) // 2 for w1, w2 in zip(ws, aws)]
-    pts = [(w1 + w2) // 2 for w1, w2 in zip(pws, paws)]
-
-    # TODO: pretrained window size and pretrained anchor window size is only used here.
-    # TODO: Investigate whether it is really important to use this setting when finetuning large window size
-    # TODO: based on pretrained weights with small window size.
-
-    coord_h = torch.arange(-(ts[0] - 1), ts[0], dtype=torch.float32)
-    coord_w = torch.arange(-(ts[1] - 1), ts[1], dtype=torch.float32)
-    table = torch.stack(torch.meshgrid([coord_h, coord_w], indexing="ij")).permute(
-        1, 2, 0
-    )
-    table = table.contiguous().unsqueeze(0)  # 1, Wh+AWh-1, Ww+AWw-1, 2
-    if pts[0] > 0:
-        table[:, :, :, 0] /= pts[0] - 1
-        table[:, :, :, 1] /= pts[1] - 1
-    else:
-        table[:, :, :, 0] /= ts[0] - 1
-        table[:, :, :, 1] /= ts[1] - 1
-    table *= 8  # normalize to -8, 8
-    table = torch.sign(table) * torch.log2(torch.abs(table) + 1.0) / np.log2(8)
-    return table
-
-
 def get_relative_coords_table_all(
     window_size: tuple[int, int] | list[int],
     pretrained_window_size: tuple[int, int] | list[int] = [0, 0],
     anchor_window_down_factor=1,
 ):
     """
     Use case: 3)
@@ -280,96 +216,25 @@
         table[:, :, :, 1] /= ts_p[1]
     table *= 8  # normalize to -8, 8
     table = torch.sign(table) * torch.log2(torch.abs(table) + 1.0) / np.log2(8)
     # 1, Wh+AWh-1, Ww+AWw-1, 2
     return table
 
 
-def coords_diff(coords1, coords2, max_diff):
-    # The coordinates starts from (-start_coord[0], -start_coord[1])
-    coords = coords1[:, :, None] - coords2[:, None, :]  # 2, Wh*Ww, AWh*AWw
-    coords = coords.permute(1, 2, 0).contiguous()  # Wh*Ww, AWh*AWw, 2
-    coords[:, :, 0] += max_diff[0] - 1  # shift to start from 0
-    coords[:, :, 1] += max_diff[1] - 1
-    coords[:, :, 0] *= 2 * max_diff[1] - 1
-    idx = coords.sum(-1)  # Wh*Ww, AWh*AWw
-    return idx
-
-
-def get_relative_position_index(
-    window_size: tuple[int, int] | list[int],
-    anchor_window_down_factor=1,
-    window_to_anchor=True,
-):
-    """
-    Use case: 1)
-    """
-    # get pair-wise relative position index for each token inside the window
-    ws = window_size
-    aws = [w // anchor_window_down_factor for w in window_size]
-    coords_anchor_end = [(w1 + w2) // 2 for w1, w2 in zip(ws, aws)]
-    coords_anchor_start = [(w1 - w2) // 2 for w1, w2 in zip(ws, aws)]
-
-    coords = _get_meshgrid_coords((0, 0), window_size)  # 2, Wh*Ww
-    coords_anchor = _get_meshgrid_coords(coords_anchor_start, coords_anchor_end)
-    # 2, AWh*AWw
-
-    if window_to_anchor:
-        idx = coords_diff(coords, coords_anchor, max_diff=coords_anchor_end)
-    else:
-        idx = coords_diff(coords_anchor, coords, max_diff=coords_anchor_end)
-    return idx  # Wh*Ww, AWh*AWw or AWh*AWw, Wh*Ww
-
-
 def coords_diff_odd(coords1, coords2, start_coord, max_diff):
     # The coordinates starts from (-start_coord[0], -start_coord[1])
     coords = coords1[:, :, None] - coords2[:, None, :]  # 2, Wh*Ww, AWh*AWw
     coords = coords.permute(1, 2, 0).contiguous()  # Wh*Ww, AWh*AWw, 2
     coords[:, :, 0] += start_coord[0]  # shift to start from 0
     coords[:, :, 1] += start_coord[1]
     coords[:, :, 0] *= max_diff
     idx = coords.sum(-1)  # Wh*Ww, AWh*AWw
     return idx
 
 
-def get_relative_position_index_all(
-    window_size: tuple[int, int] | list[int],
-    anchor_window_down_factor=1,
-    window_to_anchor=True,
-):
-    """
-    Use case: 3)
-    Support all window shapes:
-        square window - square window
-        rectangular window - rectangular window
-        window - anchor
-        anchor - window
-        [8, 8] - [8, 8]
-        [4, 86] - [2, 43]
-    """
-    # get pair-wise relative position index for each token inside the window
-    ws = window_size
-    aws = [w // anchor_window_down_factor for w in window_size]
-    coords_anchor_start = [(w1 - w2) // 2 for w1, w2 in zip(ws, aws)]
-    coords_anchor_end = [s + w2 for s, w2 in zip(coords_anchor_start, aws)]
-
-    coords = _get_meshgrid_coords((0, 0), window_size)  # 2, Wh*Ww
-    coords_anchor = _get_meshgrid_coords(coords_anchor_start, coords_anchor_end)
-    # 2, AWh*AWw
-
-    max_horizontal_diff = aws[1] + ws[1] - 1
-    if window_to_anchor:
-        offset = [w2 + s - 1 for s, w2 in zip(coords_anchor_start, aws)]
-        idx = coords_diff_odd(coords, coords_anchor, offset, max_horizontal_diff)
-    else:
-        offset = [w1 - s - 1 for s, w1 in zip(coords_anchor_start, ws)]
-        idx = coords_diff_odd(coords_anchor, coords, offset, max_horizontal_diff)
-    return idx  # Wh*Ww, AWh*AWw or AWh*AWw, Wh*Ww
-
-
 def get_relative_position_index_simple(
     window_size: tuple[int, int] | list[int],
     anchor_window_down_factor=1,
     window_to_anchor=True,
 ):
     """
     Use case: 3)
@@ -388,185 +253,7 @@
     if window_to_anchor:
         offset = [w2 - 1 for w2 in aws]
         idx = coords_diff_odd(coords, coords_anchor, offset, max_horizontal_diff)
     else:
         offset = [w1 - 1 for w1 in ws]
         idx = coords_diff_odd(coords_anchor, coords, offset, max_horizontal_diff)
     return idx  # Wh*Ww, AWh*AWw or AWh*AWw, Wh*Ww
-
-
-# def get_relative_position_index(window_size):
-#     # This is a very early version
-#     # get pair-wise relative position index for each token inside the window
-#     coords = _get_meshgrid_coords(start_coords=(0, 0), end_coords=window_size)
-
-#     coords = coords[:, :, None] - coords[:, None, :]  # 2, Wh*Ww, Wh*Ww
-#     coords = coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
-#     coords[:, :, 0] += window_size[0] - 1  # shift to start from 0
-#     coords[:, :, 1] += window_size[1] - 1
-#     coords[:, :, 0] *= 2 * window_size[1] - 1
-#     idx = coords.sum(-1)  # Wh*Ww, Wh*Ww
-#     return idx
-
-
-def get_relative_win_position_index(
-    window_size: tuple[int, int] | list[int], anchor_window_size
-):
-    """
-    Use case: 2)
-    """
-    # get pair-wise relative position index for each token inside the window
-    ws = window_size
-    aws = anchor_window_size
-    coords_anchor_end = [(w1 + w2) // 2 for w1, w2 in zip(ws, aws)]
-    coords_anchor_start = [(w1 - w2) // 2 for w1, w2 in zip(ws, aws)]
-
-    coords = _get_meshgrid_coords((0, 0), window_size)  # 2, Wh*Ww
-    coords_anchor = _get_meshgrid_coords(coords_anchor_start, coords_anchor_end)
-    # 2, AWh*AWw
-    coords = coords[:, :, None] - coords_anchor[:, None, :]  # 2, Wh*Ww, AWh*AWw
-    coords = coords.permute(1, 2, 0).contiguous()  # Wh*Ww, AWh*AWw, 2
-    coords[:, :, 0] += coords_anchor_end[0] - 1  # shift to start from 0
-    coords[:, :, 1] += coords_anchor_end[1] - 1
-    coords[:, :, 0] *= 2 * coords_anchor_end[1] - 1
-    idx = coords.sum(-1)  # Wh*Ww, AWh*AWw
-    return idx
-
-
-# def get_relative_coords_table(window_size, pretrained_window_size):
-#     # This is a very early version
-#     # get relative_coords_table
-#     ws = window_size
-#     pws = pretrained_window_size
-#     coord_h = torch.arange(-(ws[0] - 1), ws[0], dtype=torch.float32)
-#     coord_w = torch.arange(-(ws[1] - 1), ws[1], dtype=torch.float32)
-#     table = torch.stack(torch.meshgrid([coord_h, coord_w], indexing='ij')).permute(1, 2, 0)
-#     table = table.contiguous().unsqueeze(0)  # 1, 2*Wh-1, 2*Ww-1, 2
-#     if pws[0] > 0:
-#         table[:, :, :, 0] /= pws[0] - 1
-#         table[:, :, :, 1] /= pws[1] - 1
-#     else:
-#         table[:, :, :, 0] /= ws[0] - 1
-#         table[:, :, :, 1] /= ws[1] - 1
-#     table *= 8  # normalize to -8, 8
-#     table = torch.sign(table) * torch.log2(torch.abs(table) + 1.0) / np.log2(8)
-#     return table
-
-
-def get_relative_win_coords_table(
-    window_size,
-    anchor_window_size,
-    pretrained_window_size=[0, 0],
-    pretrained_anchor_window_size=[0, 0],
-):
-    """
-    Use case: 2)
-    """
-    # get relative_coords_table
-    ws = window_size
-    aws = anchor_window_size
-    pws = pretrained_window_size
-    paws = pretrained_anchor_window_size
-
-    # TODO: pretrained window size and pretrained anchor window size is only used here.
-    # TODO: Investigate whether it is really important to use this setting when finetuning large window size
-    # TODO: based on pretrained weights with small window size.
-
-    table_size = [(wsi + awsi) // 2 for wsi, awsi in zip(ws, aws)]
-    table_size_pretrained = [(pwsi + pawsi) // 2 for pwsi, pawsi in zip(pws, paws)]
-    coord_h = torch.arange(-(table_size[0] - 1), table_size[0], dtype=torch.float32)
-    coord_w = torch.arange(-(table_size[1] - 1), table_size[1], dtype=torch.float32)
-    table = torch.stack(torch.meshgrid([coord_h, coord_w], indexing="ij")).permute(
-        1, 2, 0
-    )
-    table = table.contiguous().unsqueeze(0)  # 1, Wh+AWh-1, Ww+AWw-1, 2
-    if table_size_pretrained[0] > 0:
-        table[:, :, :, 0] /= table_size_pretrained[0] - 1
-        table[:, :, :, 1] /= table_size_pretrained[1] - 1
-    else:
-        table[:, :, :, 0] /= table_size[0] - 1
-        table[:, :, :, 1] /= table_size[1] - 1
-    table *= 8  # normalize to -8, 8
-    table = torch.sign(table) * torch.log2(torch.abs(table) + 1.0) / np.log2(8)
-    return table
-
-
-if __name__ == "__main__":
-    table = get_relative_coords_table_all((4, 86), anchor_window_down_factor=2)
-    table = table.view(-1, 2)
-    index1 = get_relative_position_index_all((4, 86), 2, False)
-    index2 = get_relative_position_index_simple((4, 86), 2, False)
-    print(index2)
-    index3 = get_relative_position_index_all((4, 86), 2)
-    index4 = get_relative_position_index_simple((4, 86), 2)
-    print(index4)
-    print(
-        table.shape,
-        index2.shape,
-        index2.max(),
-        index2.min(),
-        index4.shape,
-        index4.max(),
-        index4.min(),
-        torch.allclose(index1, index2),
-        torch.allclose(index3, index4),
-    )
-
-    table = get_relative_coords_table_all((4, 86), anchor_window_down_factor=1)
-    table = table.view(-1, 2)
-    index1 = get_relative_position_index_all((4, 86), 1, False)
-    index2 = get_relative_position_index_simple((4, 86), 1, False)
-    # print(index1)
-    index3 = get_relative_position_index_all((4, 86), 1)
-    index4 = get_relative_position_index_simple((4, 86), 1)
-    # print(index2)
-    print(
-        table.shape,
-        index2.shape,
-        index2.max(),
-        index2.min(),
-        index4.shape,
-        index4.max(),
-        index4.min(),
-        torch.allclose(index1, index2),
-        torch.allclose(index3, index4),
-    )
-
-    table = get_relative_coords_table_all((8, 8), anchor_window_down_factor=2)
-    table = table.view(-1, 2)
-    index1 = get_relative_position_index_all((8, 8), 2, False)
-    index2 = get_relative_position_index_simple((8, 8), 2, False)
-    # print(index1)
-    index3 = get_relative_position_index_all((8, 8), 2)
-    index4 = get_relative_position_index_simple((8, 8), 2)
-    # print(index2)
-    print(
-        table.shape,
-        index2.shape,
-        index2.max(),
-        index2.min(),
-        index4.shape,
-        index4.max(),
-        index4.min(),
-        torch.allclose(index1, index2),
-        torch.allclose(index3, index4),
-    )
-
-    table = get_relative_coords_table_all((8, 8), anchor_window_down_factor=1)
-    table = table.view(-1, 2)
-    index1 = get_relative_position_index_all((8, 8), 1, False)
-    index2 = get_relative_position_index_simple((8, 8), 1, False)
-    # print(index1)
-    index3 = get_relative_position_index_all((8, 8), 1)
-    index4 = get_relative_position_index_simple((8, 8), 1)
-    # print(index2)
-    print(
-        table.shape,
-        index2.shape,
-        index2.max(),
-        index2.min(),
-        index4.shape,
-        index4.max(),
-        index4.min(),
-        torch.allclose(index1, index2),
-        torch.allclose(index3, index4),
-    )
```

## spandrel/architectures/GRL/arch/swin_v1_block.py

```diff
@@ -1,21 +1,12 @@
-from math import prod
-
-import torch
 import torch.nn as nn
 
-from spandrel.util.timm import DropPath, to_2tuple, trunc_normal_
+from spandrel.util.timm import to_2tuple
 
-from .ops import (
-    bchw_to_blc,
-    blc_to_bchw,
-    calculate_mask,
-    window_partition,
-    window_reverse,
-)
+from .ops import bchw_to_blc, blc_to_bchw
 
 
 class Mlp(nn.Module):
     """MLP as used in Vision Transformer, MLP-Mixer and related networks"""
 
     def __init__(
         self,
@@ -41,424 +32,14 @@
         x = self.act(x)
         x = self.drop1(x)
         x = self.fc2(x)
         x = self.drop2(x)
         return x
 
 
-class WindowAttentionV1(nn.Module):
-    r"""Window based multi-head self attention (W-MSA) module with relative position bias.
-    It supports both of shifted and non-shifted window.
-    Args:
-        dim (int): Number of input channels.
-        window_size (tuple[int]): The height and width of the window.
-        num_heads (int): Number of attention heads.
-        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
-        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
-        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
-        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
-    """
-
-    def __init__(
-        self,
-        dim,
-        window_size,
-        num_heads,
-        qkv_bias=True,
-        qk_scale=None,
-        attn_drop=0.0,
-        proj_drop=0.0,
-        use_pe=True,
-    ):
-        super().__init__()
-        self.dim = dim
-        self.window_size = window_size  # Wh, Ww
-        self.num_heads = num_heads
-        head_dim = dim // num_heads
-        self.scale = qk_scale or head_dim**-0.5
-        self.use_pe = use_pe
-
-        if self.use_pe:
-            # define a parameter table of relative position bias
-            ws = self.window_size
-            table = torch.zeros((2 * ws[0] - 1) * (2 * ws[1] - 1), num_heads)
-            self.relative_position_bias_table = nn.Parameter(table)
-            # 2*Wh-1 * 2*Ww-1, nH
-            trunc_normal_(self.relative_position_bias_table, std=0.02)
-
-            self.get_relative_position_index(self.window_size)
-
-        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
-        self.attn_drop = nn.Dropout(attn_drop)
-        self.proj = nn.Linear(dim, dim)
-
-        self.proj_drop = nn.Dropout(proj_drop)
-
-        self.softmax = nn.Softmax(dim=-1)
-
-    def get_relative_position_index(self, window_size):
-        # get pair-wise relative position index for each token inside the window
-        coord_h = torch.arange(window_size[0])
-        coord_w = torch.arange(window_size[1])
-        coords = torch.stack(torch.meshgrid([coord_h, coord_w]))  # 2, Wh, Ww
-        coords = torch.flatten(coords, 1)  # 2, Wh*Ww
-        coords = coords[:, :, None] - coords[:, None, :]  # 2, Wh*Ww, Wh*Ww
-        coords = coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
-        coords[:, :, 0] += window_size[0] - 1  # shift to start from 0
-        coords[:, :, 1] += window_size[1] - 1
-        coords[:, :, 0] *= 2 * window_size[1] - 1
-        relative_position_index = coords.sum(-1)  # Wh*Ww, Wh*Ww
-        self.register_buffer("relative_position_index", relative_position_index)
-
-    def forward(self, x, mask=None):
-        """
-        Args:
-            x: input features with shape of (num_windows*B, N, C)
-            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
-        """
-        B_, N, C = x.shape
-
-        # qkv projection
-        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
-        q, k, v = qkv[0], qkv[1], qkv[2]
-
-        # attention map
-        q = q * self.scale
-        attn = q @ k.transpose(-2, -1)
-
-        # positional encoding
-        if self.use_pe:
-            win_dim = prod(self.window_size)
-            bias = self.relative_position_bias_table[
-                self.relative_position_index.view(-1)
-            ]
-            bias = bias.view(win_dim, win_dim, -1).permute(2, 0, 1).contiguous()
-            # nH, Wh*Ww, Wh*Ww
-            attn = attn + bias.unsqueeze(0)
-
-        # shift attention mask
-        if mask is not None:
-            nW = mask.shape[0]
-            mask = mask.unsqueeze(1).unsqueeze(0)
-            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask
-            attn = attn.view(-1, self.num_heads, N, N)
-
-        # attention
-        attn = self.softmax(attn)
-        attn = self.attn_drop(attn)
-        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
-
-        # output projection
-        x = self.proj(x)
-        x = self.proj_drop(x)
-        return x
-
-    def extra_repr(self) -> str:
-        return f"dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}"
-
-    def flops(self, N):
-        # calculate flops for 1 window with token length of N
-        flops = 0
-        # qkv = self.qkv(x)
-        flops += N * self.dim * 3 * self.dim
-        # attn = (q @ k.transpose(-2, -1))
-        flops += self.num_heads * N * (self.dim // self.num_heads) * N
-        #  x = (attn @ v)
-        flops += self.num_heads * N * N * (self.dim // self.num_heads)
-        # x = self.proj(x)
-        flops += N * self.dim * self.dim
-        return flops
-
-
-class WindowAttentionWrapperV1(WindowAttentionV1):
-    def __init__(self, shift_size, input_resolution, **kwargs):
-        super().__init__(**kwargs)
-        self.shift_size = shift_size
-        self.input_resolution = input_resolution
-
-        if self.shift_size > 0:
-            attn_mask = calculate_mask(input_resolution, self.window_size, shift_size)
-        else:
-            attn_mask = None
-        self.register_buffer("attn_mask", attn_mask)
-
-    def forward(self, x, x_size):  # type: ignore
-        H, W = x_size
-        B, _L, C = x.shape
-        x = x.view(B, H, W, C)
-
-        # cyclic shift
-        if self.shift_size > 0:
-            x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
-
-        # partition windows
-        x = window_partition(x, self.window_size)  # nW*B, wh, ww, C
-        x = x.view(-1, prod(self.window_size), C)  # nW*B, wh*ww, C
-
-        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size
-        if self.input_resolution == x_size:
-            attn_mask = self.attn_mask
-        else:
-            attn_mask = calculate_mask(x_size, self.window_size, self.shift_size)
-            attn_mask = attn_mask.to(x.device)
-
-        # attention
-        x = super().forward(x, mask=attn_mask)
-        # nW*B, wh*ww, C
-
-        # merge windows
-        x = x.view(-1, *self.window_size, C)
-        x = window_reverse(x, self.window_size, x_size)  # B, H, W, C
-
-        # reverse cyclic shift
-        if self.shift_size > 0:
-            x = torch.roll(x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
-        x = x.view(B, H * W, C)
-
-        return x
-
-
-class SwinTransformerBlockV1(nn.Module):
-    r"""Swin Transformer Block.
-    Args:
-        dim (int): Number of input channels.
-        input_resolution (tuple[int]): Input resulotion.
-        num_heads (int): Number of attention heads.
-        window_size (int): Window size.
-        shift_size (int): Shift size for SW-MSA.
-        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
-        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
-        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
-        drop (float, optional): Dropout rate. Default: 0.0
-        attn_drop (float, optional): Attention dropout rate. Default: 0.0
-        drop_path (float, optional): Stochastic depth rate. Default: 0.0
-        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
-        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
-    """
-
-    def __init__(
-        self,
-        dim,
-        input_resolution,
-        num_heads,
-        window_size=7,
-        shift_size=0,
-        mlp_ratio=4.0,
-        qkv_bias=True,
-        qk_scale=None,
-        drop=0.0,
-        attn_drop=0.0,
-        drop_path=0.0,
-        act_layer=nn.GELU,
-        norm_layer=nn.LayerNorm,
-        use_pe=True,
-        res_scale=1.0,
-    ):
-        super().__init__()
-        self.dim = dim
-        self.input_resolution = input_resolution
-        self.num_heads = num_heads
-        self.window_size = window_size
-        self.shift_size = shift_size
-        self.mlp_ratio = mlp_ratio
-        if min(self.input_resolution) <= self.window_size:
-            # if window size is larger than input resolution, we don't partition windows
-            self.shift_size = 0
-            self.window_size = min(self.input_resolution)
-        assert (
-            0 <= self.shift_size < self.window_size
-        ), "shift_size must in 0-window_size"
-        self.res_scale = res_scale
-
-        self.norm1 = norm_layer(dim)
-        self.attn = WindowAttentionWrapperV1(
-            shift_size=self.shift_size,
-            input_resolution=self.input_resolution,
-            dim=dim,
-            window_size=to_2tuple(self.window_size),
-            num_heads=num_heads,
-            qkv_bias=qkv_bias,
-            qk_scale=qk_scale,
-            attn_drop=attn_drop,
-            proj_drop=drop,
-            use_pe=use_pe,
-        )
-
-        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
-
-        self.norm2 = norm_layer(dim)
-        self.mlp = Mlp(
-            in_features=dim,
-            hidden_features=int(dim * mlp_ratio),
-            act_layer=act_layer,
-            drop=drop,
-        )
-
-    def forward(self, x, x_size):
-        # Window attention
-        x = x + self.res_scale * self.drop_path(self.attn(self.norm1(x), x_size))
-        # FFN
-        x = x + self.res_scale * self.drop_path(self.mlp(self.norm2(x)))
-
-        return x
-
-    def extra_repr(self) -> str:
-        return (
-            f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, "
-            f"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}, res_scale={self.res_scale}"
-        )
-
-    def flops(self):
-        flops = 0
-        H, W = self.input_resolution
-        # norm1
-        flops += self.dim * H * W
-        # W-MSA/SW-MSA
-        nW = H * W / self.window_size / self.window_size
-        flops += nW * self.attn.flops(self.window_size * self.window_size)
-        # mlp
-        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio
-        # norm2
-        flops += self.dim * H * W
-        return flops
-
-
-class PatchMerging(nn.Module):
-    r"""Patch Merging Layer.
-    Args:
-        input_resolution (tuple[int]): Resolution of input feature.
-        dim (int): Number of input channels.
-        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
-    """
-
-    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
-        super().__init__()
-        self.input_resolution = input_resolution
-        self.dim = dim
-        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
-        self.norm = norm_layer(4 * dim)
-
-    def forward(self, x):
-        """
-        x: B, H*W, C
-        """
-        H, W = self.input_resolution
-        B, L, C = x.shape
-        assert L == H * W, "input feature has wrong size"
-        assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."
-
-        x = x.view(B, H, W, C)
-
-        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
-        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
-        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
-        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
-        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
-        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C
-
-        x = self.norm(x)
-        x = self.reduction(x)
-
-        return x
-
-    def extra_repr(self) -> str:
-        return f"input_resolution={self.input_resolution}, dim={self.dim}"
-
-    def flops(self):
-        H, W = self.input_resolution
-        flops = H * W * self.dim
-        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim
-        return flops
-
-
-class PatchEmbed(nn.Module):
-    r"""Image to Patch Embedding
-    Args:
-        img_size (int): Image size.  Default: 224.
-        patch_size (int): Patch token size. Default: 4.
-        in_chans (int): Number of input image channels. Default: 3.
-        embed_dim (int): Number of linear projection output channels. Default: 96.
-        norm_layer (nn.Module, optional): Normalization layer. Default: None
-    """
-
-    def __init__(
-        self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None
-    ):
-        super().__init__()
-        img_size = to_2tuple(img_size)
-        patch_size = to_2tuple(patch_size)
-        patches_resolution = [
-            img_size[0] // patch_size[0],
-            img_size[1] // patch_size[1],
-        ]
-        self.img_size = img_size
-        self.patch_size = patch_size
-        self.patches_resolution = patches_resolution
-        self.num_patches = patches_resolution[0] * patches_resolution[1]
-
-        self.in_chans = in_chans
-        self.embed_dim = embed_dim
-
-        if norm_layer is not None:
-            self.norm = norm_layer(embed_dim)
-        else:
-            self.norm = None
-
-    def forward(self, x):
-        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C
-        if self.norm is not None:
-            x = self.norm(x)
-        return x
-
-    def flops(self):
-        flops = 0
-        H, W = self.img_size
-        if self.norm is not None:
-            flops += H * W * self.embed_dim
-        return flops
-
-
-class PatchUnEmbed(nn.Module):
-    r"""Image to Patch Unembedding
-    Args:
-        img_size (int): Image size.  Default: 224.
-        patch_size (int): Patch token size. Default: 4.
-        in_chans (int): Number of input image channels. Default: 3.
-        embed_dim (int): Number of linear projection output channels. Default: 96.
-        norm_layer (nn.Module, optional): Normalization layer. Default: None
-    """
-
-    def __init__(
-        self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None
-    ):
-        super().__init__()
-        img_size = to_2tuple(img_size)
-        patch_size = to_2tuple(patch_size)
-        patches_resolution = [
-            img_size[0] // patch_size[0],
-            img_size[1] // patch_size[1],
-        ]
-        self.img_size = img_size
-        self.patch_size = patch_size
-        self.patches_resolution = patches_resolution
-        self.num_patches = patches_resolution[0] * patches_resolution[1]
-
-        self.in_chans = in_chans
-        self.embed_dim = embed_dim
-
-    def forward(self, x, x_size):
-        B, _HW, _C = x.shape
-        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C
-        return x
-
-    def flops(self):
-        flops = 0
-        return flops
-
-
 class Linear(nn.Linear):
     def __init__(self, in_features, out_features, bias=True):
         super().__init__(in_features, out_features, bias)
 
     def forward(self, x):  # type: ignore
         _B, _C, H, W = x.shape
         x = bchw_to_blc(x)
@@ -482,124 +63,7 @@
     elif conv_type == "1conv1x1":
         block = nn.Conv2d(dim, dim, 1, 1, 0)
     elif conv_type == "linear":
         block = Linear(dim, dim)
     else:
         raise ValueError(f"Unsupported conv_type {conv_type}")
     return block
-
-
-# class BasicLayer(nn.Module):
-#     """A basic Swin Transformer layer for one stage.
-#     Args:
-#         dim (int): Number of input channels.
-#         input_resolution (tuple[int]): Input resolution.
-#         depth (int): Number of blocks.
-#         num_heads (int): Number of attention heads.
-#         window_size (int): Local window size.
-#         mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
-#         qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
-#         qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
-#         drop (float, optional): Dropout rate. Default: 0.0
-#         attn_drop (float, optional): Attention dropout rate. Default: 0.0
-#         drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
-#         norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
-#         downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
-#         args: Additional arguments
-#     """
-
-#     def __init__(
-#         self,
-#         dim,
-#         input_resolution,
-#         depth,
-#         num_heads,
-#         window_size,
-#         mlp_ratio=4.0,
-#         qkv_bias=True,
-#         qk_scale=None,
-#         drop=0.0,
-#         attn_drop=0.0,
-#         drop_path=0.0,
-#         norm_layer=nn.LayerNorm,
-#         downsample=None,
-#         args=None,
-#     ):
-
-#         super().__init__()
-#         self.dim = dim
-#         self.input_resolution = input_resolution
-#         self.depth = depth
-
-#         # build blocks
-#         self.blocks = nn.ModuleList(
-#             [
-#                 _parse_block(
-#                     dim=dim,
-#                     input_resolution=input_resolution,
-#                     num_heads=num_heads,
-#                     window_size=window_size,
-#                     shift_size=0
-#                     if args.no_shift
-#                     else (0 if (i % 2 == 0) else window_size // 2),
-#                     mlp_ratio=mlp_ratio,
-#                     qkv_bias=qkv_bias,
-#                     qk_scale=qk_scale,
-#                     drop=drop,
-#                     attn_drop=attn_drop,
-#                     drop_path=drop_path[i]
-#                     if isinstance(drop_path, list)
-#                     else drop_path,
-#                     norm_layer=norm_layer,
-#                     stripe_type="H" if (i % 2 == 0) else "W",
-#                     args=args,
-#                 )
-#                 for i in range(depth)
-#             ]
-#         )
-#         # self.blocks = nn.ModuleList(
-#         #     [
-#         #         STV1Block(
-#         #             dim=dim,
-#         #             input_resolution=input_resolution,
-#         #             num_heads=num_heads,
-#         #             window_size=window_size,
-#         #             shift_size=0 if (i % 2 == 0) else window_size // 2,
-#         #             mlp_ratio=mlp_ratio,
-#         #             qkv_bias=qkv_bias,
-#         #             qk_scale=qk_scale,
-#         #             drop=drop,
-#         #             attn_drop=attn_drop,
-#         #             drop_path=drop_path[i]
-#         #             if isinstance(drop_path, list)
-#         #             else drop_path,
-#         #             norm_layer=norm_layer,
-#         #         )
-#         #         for i in range(depth)
-#         #     ]
-#         # )
-
-#         # patch merging layer
-#         if downsample is not None:
-#             self.downsample = downsample(
-#                 input_resolution, dim=dim, norm_layer=norm_layer
-#             )
-#         else:
-#             self.downsample = None
-
-#     def forward(self, x, x_size):
-#         for blk in self.blocks:
-#             x = blk(x, x_size)
-#         if self.downsample is not None:
-#             x = self.downsample(x)
-#         return x
-
-#     def extra_repr(self) -> str:
-#         return f"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}"
-
-#     def flops(self):
-#         flops = 0
-#         for blk in self.blocks:
-#             flops += blk.flops()
-#         if self.downsample is not None:
-#             flops += self.downsample.flops()
-#         return flops
```

## spandrel/architectures/HAT/__init__.py

```diff
@@ -1,12 +1,12 @@
 import math
 
 from typing_extensions import override
 
-from spandrel.util import KeyCondition, get_seq_len
+from spandrel.util import KeyCondition, get_pixelshuffle_params, get_seq_len
 
 from ...__helpers.model_descriptor import (
     Architecture,
     ImageModelDescriptor,
     SizeRequirements,
     StateDict,
 )
@@ -56,20 +56,29 @@
 
 
 class HATArch(Architecture[HAT]):
     def __init__(self) -> None:
         super().__init__(
             id="HAT",
             detect=KeyCondition.has_all(
+                "relative_position_index_SA",
+                "conv_first.weight",
                 "layers.0.residual_group.blocks.0.norm1.weight",
                 "layers.0.residual_group.blocks.0.conv_block.cab.0.weight",
-                "conv_last.weight",
-                "conv_first.weight",
+                "layers.0.residual_group.blocks.0.conv_block.cab.2.weight",
+                "layers.0.residual_group.blocks.0.conv_block.cab.3.attention.1.weight",
+                "layers.0.residual_group.blocks.0.conv_block.cab.3.attention.3.weight",
                 "layers.0.residual_group.blocks.0.mlp.fc1.bias",
-                "relative_position_index_SA",
+                "layers.0.residual_group.blocks.0.mlp.fc2.weight",
+                "layers.0.residual_group.overlap_attn.relative_position_bias_table",
+                "layers.0.residual_group.overlap_attn.qkv.weight",
+                "layers.0.residual_group.overlap_attn.proj.weight",
+                "layers.0.residual_group.overlap_attn.mlp.fc1.weight",
+                "layers.0.residual_group.overlap_attn.mlp.fc2.weight",
+                "conv_last.weight",
             ),
         )
 
     @override
     def load(self, state_dict: StateDict) -> ImageModelDescriptor[HAT]:
         img_size = 64
         patch_size = 1
@@ -88,35 +97,23 @@
         drop_rate = 0.0  # cannot be deduced from state dict
         attn_drop_rate = 0.0  # cannot be deduced from state dict
         drop_path_rate = 0.1  # cannot be deduced from state dict
         ape = False
         patch_norm = True
         upscale = 2
         img_range = 1.0  # cannot be deduced from state dict
-        upsampler = ""
+        upsampler = "pixelshuffle"  # it's the only possible value
         resi_connection = "1conv"
         num_feat = 64
 
         in_chans = state_dict["conv_first.weight"].shape[1]
         embed_dim = state_dict["conv_first.weight"].shape[0]
 
-        if "conv_last.weight" in state_dict:
-            # upscaling model
-            upsampler = "pixelshuffle"
-            num_feat = state_dict["conv_last.weight"].shape[1]
-
-            upscale = 1
-            for i in range(0, get_seq_len(state_dict, "upsample"), 2):
-                shape = state_dict[f"upsample.{i}.weight"].shape[0]
-                upscale *= int(math.sqrt(shape // num_feat))
-        else:
-            # 1x model
-            upsampler = ""
-
-            upscale = 1
+        num_feat = state_dict["conv_last.weight"].shape[1]
+        upscale, _ = get_pixelshuffle_params(state_dict, "upsample", num_feat)
 
         window_size = int(math.sqrt(state_dict["relative_position_index_SA"].shape[0]))
         overlap_ratio = _get_overlap_ratio(
             window_size,
             with_overlap=int(
                 math.sqrt(state_dict["relative_position_index_OCA"].shape[1])
             ),
```

## spandrel/architectures/HAT/arch/HAT.py

```diff
@@ -1,11 +1,12 @@
 # HAT from https://github.com/XPixelGroup/HAT/blob/main/hat/archs/hat_arch.py
 from __future__ import annotations
 
 import math
+from typing import Literal
 
 import torch
 import torch.nn as nn
 from einops import rearrange
 
 from spandrel.util import store_hyperparameters
 from spandrel.util.timm import to_2tuple, trunc_normal_
@@ -371,54 +372,14 @@
         # FFN
         x = shortcut + self.drop_path(attn_x) + conv_x * self.conv_scale
         x = x + self.drop_path(self.mlp(self.norm2(x)))
 
         return x
 
 
-class PatchMerging(nn.Module):
-    r"""Patch Merging Layer.
-
-    Args:
-        input_resolution (tuple[int]): Resolution of input feature.
-        dim (int): Number of input channels.
-        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
-    """
-
-    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
-        super().__init__()
-        self.input_resolution = input_resolution
-        self.dim = dim
-        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
-        self.norm = norm_layer(4 * dim)
-
-    def forward(self, x):
-        """
-        x: b, h*w, c
-        """
-        h, w = self.input_resolution
-        b, seq_len, c = x.shape
-        assert seq_len == h * w, "input feature has wrong size"
-        assert h % 2 == 0 and w % 2 == 0, f"x size ({h}*{w}) are not even."
-
-        x = x.view(b, h, w, c)
-
-        x0 = x[:, 0::2, 0::2, :]  # b h/2 w/2 c
-        x1 = x[:, 1::2, 0::2, :]  # b h/2 w/2 c
-        x2 = x[:, 0::2, 1::2, :]  # b h/2 w/2 c
-        x3 = x[:, 1::2, 1::2, :]  # b h/2 w/2 c
-        x = torch.cat([x0, x1, x2, x3], -1)  # b h/2 w/2 4*c
-        x = x.view(b, -1, 4 * c)  # b h/2*w/2 4*c
-
-        x = self.norm(x)
-        x = self.reduction(x)
-
-        return x
-
-
 class OCAB(nn.Module):
     # overlapping cross-attention block
 
     def __init__(
         self,
         dim,
         input_resolution,
@@ -914,15 +875,15 @@
         drop_path_rate=0.1,
         norm_layer=nn.LayerNorm,
         ape=False,
         patch_norm=True,
         use_checkpoint=False,
         upscale=1,
         img_range=1.0,
-        upsampler="",
+        upsampler: Literal["pixelshuffle"] = "pixelshuffle",
         resi_connection="1conv",
         num_feat=64,
     ):
         super().__init__()
 
         self.window_size = window_size
         self.shift_size = window_size // 2
@@ -1127,22 +1088,14 @@
         attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
         attn_mask = attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(
             attn_mask == 0, 0.0
         )
 
         return attn_mask
 
-    @torch.jit.ignore  # type: ignore
-    def no_weight_decay(self):
-        return {"absolute_pos_embed"}
-
-    @torch.jit.ignore  # type: ignore
-    def no_weight_decay_keywords(self):
-        return {"relative_position_bias_table"}
-
     def check_image_size(self, x):
         return pad_to_multiple(x, self.window_size, mode="reflect")
 
     def forward_features(self, x):
         x_size = (x.shape[2], x.shape[3])
 
         # Calculate attention mask and relative position index in advance to speed up inference.
```

## spandrel/architectures/KBNet/__init__.py

```diff
@@ -87,15 +87,15 @@
         )
 
         return ImageModelDescriptor(
             model,
             state_dict,
             architecture=self,
             purpose="Restoration",
-            tags=["L"],
+            tags=["L", f"{dim}dim"],
             supports_half=False,
             supports_bfloat16=True,
             scale=1,
             input_channels=in_nc,
             output_channels=out_nc,
             size_requirements=SizeRequirements(multiple_of=16),
         )
@@ -113,20 +113,20 @@
         width = state_dict["intro.weight"].shape[0]
 
         middle_blk_num = get_seq_len(state_dict, "middle_blks")
 
         enc_count = get_seq_len(state_dict, "encoders")
         enc_blk_nums = [1] * enc_count
         for i in range(enc_count):
-            enc_blk_nums[i] = get_seq_len(state_dict, "encoders." + str(i))
+            enc_blk_nums[i] = get_seq_len(state_dict, f"encoders.{i}")
 
         dec_count = get_seq_len(state_dict, "decoders")
         dec_blk_nums = [1] * dec_count
         for i in range(dec_count):
-            dec_blk_nums[i] = get_seq_len(state_dict, "decoders." + str(i))
+            dec_blk_nums[i] = get_seq_len(state_dict, f"decoders.{i}")
 
         # in code: ffn_ch = int(c * ffn_scale)
         temp_c = state_dict["middle_blks.0.conv4.weight"].shape[1]
         temp_ffn_ch = state_dict["middle_blks.0.conv4.weight"].shape[0]
         ffn_scale = temp_ffn_ch / temp_c
 
         # kernel size is 3 for lightweight and 5 otherwise
@@ -144,15 +144,19 @@
         )
 
         return ImageModelDescriptor(
             model,
             state_dict,
             architecture=self,
             purpose="Restoration",
-            tags=["S"],
+            tags=[
+                "S",
+                f"{width}w",
+                *(["lightweight"] if lightweight else []),
+            ],
             supports_half=False,
             supports_bfloat16=True,
             scale=1,
             input_channels=img_channel,
             output_channels=img_channel,
         )
```

## spandrel/architectures/RGT/__init__.py

```diff
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
 import math
 
 from typing_extensions import override
 
-from spandrel.util import KeyCondition, get_seq_len
+from spandrel.util import KeyCondition, get_pixelshuffle_params, get_seq_len
 
 from ...__helpers.model_descriptor import (
     Architecture,
     ImageModelDescriptor,
     SizeRequirements,
     StateDict,
 )
@@ -59,15 +59,20 @@
                 "layers.0.blocks.0.attn.attns.0.rpe_biases",
                 "layers.0.blocks.0.attn.attns.0.relative_position_index",
                 "layers.0.blocks.0.attn.attns.0.pos.pos_proj.weight",
                 "layers.0.blocks.0.mlp.fc1.weight",
                 "layers.0.blocks.0.mlp.fc2.weight",
                 "layers.0.blocks.0.norm2.weight",
                 "norm.weight",
-                "conv_after_body.weight",
+                KeyCondition.has_any(
+                    # 1conv
+                    "conv_after_body.weight",
+                    # 3conv
+                    "conv_after_body.0.weight",
+                ),
                 "conv_before_upsample.0.weight",
                 "conv_last.weight",
             ),
         )
 
     @override
     def load(self, state_dict: StateDict) -> ImageModelDescriptor[RGT]:
@@ -124,21 +129,15 @@
             if d >= 2:
                 c_ratio = (
                     state_dict[f"layers.{i}.blocks.1.attn.conv.weight"].shape[0]
                     / state_dict[f"layers.{i}.blocks.1.attn.conv.weight"].shape[1]
                 )
                 break
 
-        upscale = 1
-        for i in range(0, 10, 2):
-            key = f"upsample.{i}.weight"
-            if key in state_dict:
-                shape = state_dict[key].shape
-                num_feat = shape[1]
-                upscale *= math.isqrt(shape[0] // num_feat)
+        upscale, _ = get_pixelshuffle_params(state_dict, "upsample")
 
         split_size = _get_split_size(state_dict)
 
         model = RGT(
             img_size=img_size,
             in_chans=in_chans,
             embed_dim=embed_dim,
```

## spandrel/architectures/RGT/arch/rgt.py

```diff
@@ -161,16 +161,15 @@
         self.scale = qk_scale or head_dim**-0.5
 
         if idx == 0:
             H_sp, W_sp = self.split_size[0], self.split_size[1]
         elif idx == 1:
             W_sp, H_sp = self.split_size[0], self.split_size[1]
         else:
-            print("ERROR MODE", idx)
-            exit(0)
+            raise ValueError(f"ERROR MODE: {idx}")
         self.H_sp = H_sp
         self.W_sp = W_sp
 
         if self.position_bias:
             self.pos = DynamicPosBias(self.dim // 4, self.num_heads, residual=False)
             # generate mother-set
             position_bias_h = torch.arange(1 - self.H_sp, self.H_sp)
```

## spandrel/architectures/RestoreFormer/__init__.py

```diff
@@ -1,7 +1,9 @@
+import torch
+from torchvision.transforms.functional import normalize as tv_normalize
 from typing_extensions import override
 
 from spandrel.util import KeyCondition, get_seq_len
 
 from ...__helpers.model_descriptor import (
     Architecture,
     ImageModelDescriptor,
@@ -76,20 +78,26 @@
             resolution=resolution,
             z_channels=z_channels,
             double_z=double_z,
             enable_mid=enable_mid,
             head_size=head_size,
         )
 
+        def call(model: RestoreFormer, x: torch.Tensor) -> torch.Tensor:
+            x = tv_normalize(x, [0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
+            result = model(x)[0]
+            return (result + 1) / 2
+
         return ImageModelDescriptor(
             model,
             state_dict,
             architecture=self,
             purpose="FaceSR",
             tags=[],
             supports_half=False,
             supports_bfloat16=True,
-            scale=8,
+            scale=1,
             input_channels=in_channels,
             output_channels=out_ch,
-            size_requirements=SizeRequirements(minimum=16),
+            size_requirements=SizeRequirements(multiple_of=32),
+            call_fn=call,
         )
```

## spandrel/architectures/RestoreFormer/arch/restoreformer_arch.py

```diff
@@ -1,11 +1,10 @@
 # type: ignore
 """Modified from https://github.com/wzhouxiff/RestoreFormer"""
 
-import numpy as np
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from spandrel.util import store_hyperparameters
 
 
@@ -440,20 +439,20 @@
         self.in_channels = in_channels
         self.give_pre_end = give_pre_end
         self.enable_mid = enable_mid
 
         # compute in_ch_mult, block_in and curr_res at lowest res
         block_in = ch * ch_mult[self.num_resolutions - 1]
         curr_res = resolution // 2 ** (self.num_resolutions - 1)
-        self.z_shape = (1, z_channels, curr_res, curr_res)
-        print(
-            "Working with z of shape {} = {} dimensions.".format(
-                self.z_shape, np.prod(self.z_shape)
-            )
-        )
+        # self.z_shape = (1, z_channels, curr_res, curr_res)
+        # print(
+        #     "Working with z of shape {} = {} dimensions.".format(
+        #         self.z_shape, np.prod(self.z_shape)
+        #     )
+        # )
 
         # z to block_in
         self.conv_in = torch.nn.Conv2d(
             z_channels, block_in, kernel_size=3, stride=1, padding=1
         )
 
         # middle
@@ -503,15 +502,15 @@
         self.norm_out = Normalize(block_in)
         self.conv_out = torch.nn.Conv2d(
             block_in, out_ch, kernel_size=3, stride=1, padding=1
         )
 
     def forward(self, z):
         # assert z.shape[1:] == self.z_shape[1:]
-        self.last_z_shape = z.shape
+        # self.last_z_shape = z.shape
 
         # timestep embedding
         temb = None
 
         # z to block_in
         h = self.conv_in(z)
 
@@ -566,15 +565,15 @@
         self.in_channels = in_channels
         self.give_pre_end = give_pre_end
         self.enable_mid = enable_mid
 
         # compute in_ch_mult, block_in and curr_res at lowest res
         block_in = ch * ch_mult[self.num_resolutions - 1]
         curr_res = resolution // 2 ** (self.num_resolutions - 1)
-        self.z_shape = (1, z_channels, curr_res, curr_res)
+        # self.z_shape = (1, z_channels, curr_res, curr_res)
         # print(
         #     "Working with z of shape {} = {} dimensions.".format(
         #         self.z_shape, np.prod(self.z_shape)
         #     )
         # )
 
         # z to block_in
```

## spandrel/architectures/Swin2SR/__init__.py

```diff
@@ -1,12 +1,12 @@
 import math
 
 from typing_extensions import override
 
-from spandrel.util import KeyCondition, get_seq_len
+from spandrel.util import KeyCondition, get_pixelshuffle_params, get_seq_len
 
 from ...__helpers.model_descriptor import (
     Architecture,
     ImageModelDescriptor,
     SizeRequirements,
     StateDict,
 )
@@ -98,19 +98,15 @@
         elif upsampler == "nearest+conv":
             upscale = 4  # only supports 4x
         elif upsampler == "pixelshuffledirect":
             upscale = int(
                 math.sqrt(state_dict["upsample.0.weight"].shape[0] // in_chans)
             )
         else:
-            num_feat = 64  # hard-coded constant
-            upscale = 1
-            for i in range(0, get_seq_len(state_dict, "upsample"), 2):
-                shape = state_dict[f"upsample.{i}.weight"].shape[0]
-                upscale *= int(math.sqrt(shape // num_feat))
+            upscale, _ = get_pixelshuffle_params(state_dict, "upsample")
 
         window_size = int(
             math.sqrt(
                 state_dict[
                     "layers.0.residual_group.blocks.0.attn.relative_position_index"
                 ].shape[0]
             )
```

## spandrel/architectures/SwinIR/__init__.py

```diff
@@ -1,13 +1,13 @@
 import math
 
 from torch import nn
 from typing_extensions import override
 
-from spandrel.util import KeyCondition, get_seq_len
+from spandrel.util import KeyCondition, get_pixelshuffle_params, get_seq_len
 
 from ...__helpers.model_descriptor import (
     Architecture,
     ImageModelDescriptor,
     SizeRequirements,
     StateDict,
 )
@@ -80,23 +80,15 @@
             upsample_keys = [
                 x for x in state_dict if "conv_up" in x and "bias" not in x
             ]
 
             for _upsample_key in upsample_keys:
                 upscale *= 2
         elif upsampler == "pixelshuffle":
-            upsample_keys = [
-                x
-                for x in state_dict
-                if "upsample" in x and "conv" not in x and "bias" not in x
-            ]
-            for upsample_key in upsample_keys:
-                shape = state_dict[upsample_key].shape[0]
-                upscale *= math.sqrt(shape // num_feat)
-            upscale = int(upscale)
+            upscale, num_feat = get_pixelshuffle_params(state_dict, "upsample")
         elif upsampler == "pixelshuffledirect":
             upscale = int(
                 math.sqrt(state_dict["upsample.0.bias"].shape[0] // num_out_ch)
             )
 
         embed_dim = state_dict["conv_first.weight"].shape[0]
```

## spandrel/architectures/Uformer/arch/Uformer.py

```diff
@@ -49,15 +49,14 @@
         flops = 0
         # fc1
         flops += H * W * self.dim * self.hidden_dim
         # dwconv
         flops += H * W * self.hidden_dim * 3 * 3
         # fc2
         flops += H * W * self.hidden_dim * self.dim
-        print("LeFF:{%.2f}" % (flops / 1e9))
         return flops
 
 
 def conv(in_channels, out_channels, kernel_size, bias=False, stride=1):
     return nn.Conv2d(
         in_channels,
         out_channels,
@@ -414,15 +413,14 @@
         x = self.pointwise(x)
         return x
 
     def flops(self, HW):
         flops = 0
         flops += HW * self.in_channels * self.kernel_size**2 / self.stride**2
         flops += HW * self.in_channels * self.out_channels
-        print("SeqConv2d:{%.2f}" % (flops / 1e9))
         return flops
 
 
 ######## Embedding for q,k,v ########
 class ConvProjection(nn.Module):
     def __init__(
         self,
@@ -624,15 +622,14 @@
 
         flops += nW * self.num_heads * N * (self.dim // self.num_heads) * N
         #  x = (attn @ v)
         flops += nW * self.num_heads * N * N * (self.dim // self.num_heads)
 
         # x = self.proj(x)
         flops += nW * N * self.dim * self.dim
-        print("W-MSA:{%.2f}" % (flops / 1e9))
         return flops
 
 
 ########### self-attention #############
 class Attention(nn.Module):
     def __init__(
         self,
@@ -707,15 +704,14 @@
 
         flops += self.num_heads * q_num * (self.dim // self.num_heads) * kv_num
         #  x = (attn @ v)
         flops += self.num_heads * q_num * (self.dim // self.num_heads) * kv_num
 
         # x = self.proj(x)
         flops += q_num * self.dim * self.dim
-        print("MCA:{%.2f}" % (flops / 1e9))
         return flops
 
 
 #########################################
 ########### feed-forward network #############
 class Mlp(nn.Module):
     def __init__(
@@ -747,15 +743,14 @@
 
     def flops(self, H, W):
         flops = 0
         # fc1
         flops += H * W * self.in_features * self.hidden_features
         # fc2
         flops += H * W * self.hidden_features * self.out_features
-        print("MLP:{%.2f}" % (flops / 1e9))
         return flops
 
 
 class LeFF(nn.Module):
     def __init__(
         self, dim=32, hidden_dim=128, act_layer=nn.GELU, drop=0.0, use_eca=False
     ):
@@ -802,15 +797,14 @@
         flops = 0
         # fc1
         flops += H * W * self.dim * self.hidden_dim
         # dwconv
         flops += H * W * self.hidden_dim * 3 * 3
         # fc2
         flops += H * W * self.hidden_dim * self.dim
-        print("LeFF:{%.2f}" % (flops / 1e9))
         # eca
         if hasattr(self.eca, "flops"):
             flops += self.eca.flops()
         return flops
 
 
 #########################################
@@ -878,15 +872,14 @@
         out = self.conv(x).flatten(2).transpose(1, 2).contiguous()  # B H*W C
         return out
 
     def flops(self, H, W):
         flops = 0
         # conv
         flops += H / 2 * W / 2 * self.in_channel * self.out_channel * 4 * 4
-        print("Downsample:{%.2f}" % (flops / 1e9))
         return flops
 
 
 # Upsample Block
 class Upsample(nn.Module):
     def __init__(self, in_channel, out_channel):
         super().__init__()
@@ -904,15 +897,14 @@
         out = self.deconv(x).flatten(2).transpose(1, 2).contiguous()  # B H*W C
         return out
 
     def flops(self, H, W):
         flops = 0
         # conv
         flops += H * 2 * W * 2 * self.in_channel * self.out_channel * 2 * 2
-        print("Upsample:{%.2f}" % (flops / 1e9))
         return flops
 
 
 # Input Projection
 class InputProj(nn.Module):
     def __init__(
         self,
@@ -950,15 +942,14 @@
     def flops(self, H, W):
         flops = 0
         # conv
         flops += H * W * self.in_channel * self.out_channel * 3 * 3
 
         if self.norm is not None:
             flops += H * W * self.out_channel
-        print("Input_proj:{%.2f}" % (flops / 1e9))
         return flops
 
 
 # Output Projection
 class OutputProj(nn.Module):
     def __init__(
         self,
@@ -1001,15 +992,14 @@
     def flops(self, H, W):
         flops = 0
         # conv
         flops += H * W * self.in_channel * self.out_channel * 3 * 3
 
         if self.norm is not None:
             flops += H * W * self.out_channel
-        print("Output_proj:{%.2f}" % (flops / 1e9))
         return flops
 
 
 #########################################
 ########### LeWinTransformer #############
 class LeWinTransformerBlock(nn.Module):
     def __init__(
```

## spandrel/util/__init__.py

```diff
@@ -107,14 +107,43 @@
 
     raise AssertionError(
         f"Expected output channels to be either 1, 3, or 4."
         f" Could not find a pair (scale, out_nc) such that `scale**2 * out_nc = {x}`"
     )
 
 
+def get_pixelshuffle_params(
+    state_dict: Mapping[str, object],
+    upsample_key: str = "upsample",
+    default_nf: int = 64,
+) -> tuple[int, int]:
+    """
+    This will detect the upscale factor and number of features of a pixelshuffle module in the state dict.
+
+    A pixelshuffle module is a sequence of alternating up convolutions and pixelshuffle.
+    The class of this module is commonyl called `Upsample`.
+    Examples of such modules can be found in most SISR architectures, such as SwinIR, HAT, RGT, and many more.
+    """
+    upscale = 1
+    num_feat = default_nf
+
+    for i in range(0, 10, 2):
+        key = f"{upsample_key}.{i}.weight"
+        if key not in state_dict:
+            break
+
+        tensor = state_dict[key]
+        # we'll assume that the state dict contains tensors
+        shape: tuple[int, ...] = tensor.shape  # type: ignore
+        num_feat = shape[1]
+        upscale *= math.isqrt(shape[0] // num_feat)
+
+    return upscale, num_feat
+
+
 def store_hyperparameters(*, extra_parameters: Mapping[str, object] = {}):
     """
     Stores the hyperparameters of a class in a `hyperparameters` attribute.
     """
 
     def get_arg_defaults(spec: inspect.FullArgSpec) -> dict[str, Any]:
         defaults = {}
@@ -166,13 +195,14 @@
         cls.__init__ = new_init
         return cls
 
     return inner
 
 
 __all__ = [
-    "KeyCondition",
     "get_first_seq_index",
-    "get_seq_len",
+    "get_pixelshuffle_params",
     "get_scale_and_output_channels",
+    "get_seq_len",
+    "KeyCondition",
     "store_hyperparameters",
 ]
```

## Comparing `spandrel-0.3.1.dist-info/METADATA` & `spandrel-0.3.2.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: spandrel
-Version: 0.3.1
+Version: 0.3.2
 Summary: Give your project support for a variety of PyTorch model architectures, including auto-detecting model architecture from just .pth files. spandrel gives you arch support.
 Author: chaiNNer team
 License: MIT
 Keywords: spandrel,pytorch architecture,pytorch arch,model arch,model architecture
 Classifier: Development Status :: 2 - Pre-Alpha
 Classifier: Intended Audience :: Developers
 Classifier: Topic :: Software Development :: Libraries
@@ -123,19 +123,20 @@
 - [FeMaSR](https://github.com/chaofengc/FeMaSR) (+) | [Models](https://github.com/chaofengc/FeMaSR/releases/tag/v0.1-pretrain_models)
 - [GRL](https://github.com/ofsoundof/GRL-Image-Restoration) | [Models](https://github.com/ofsoundof/GRL-Image-Restoration/releases/tag/v1.0.0)
 - [DITN](https://github.com/yongliuy/DITN) | [Models](https://drive.google.com/drive/folders/1XpHW27H5j2S4IH8t4lccgrgHkIjqrS-X)
 - [MM-RealSR](https://github.com/TencentARC/MM-RealSR) | [Models](https://github.com/TencentARC/MM-RealSR/releases/tag/v1.0.0)
 - [SPAN](https://github.com/hongyuanyu/SPAN) | [Models](https://drive.google.com/file/d/1iYUA2TzKuxI0vzmA-UXr_nB43XgPOXUg/view?usp=sharing)
 - [Real-CUGAN](https://github.com/bilibili/ailab/tree/main/Real-CUGAN) | [Models](https://drive.google.com/drive/folders/1jAJyBf2qKe2povySwsGXsVMnzVyQzqDD), [Pro Models](https://drive.google.com/drive/folders/1hfT4WwnNUaS43ErrgXk0J1R5Ik8s5NVo)
 - [CRAFT](https://github.com/AVC2-UESTC/CRAFT-SR) | [Models](https://drive.google.com/file/d/13wAmc93BPeBUBQ24zUZOuUpdBFG2aAY5/view?usp=sharing)
-- [SAFMN](https://github.com/sunny2109/SAFMN) | [Models](https://drive.google.com/drive/folders/12O_xgwfgc76DsYbiClYnl6ErCDrsi_S9?usp=share_link)
+- [SAFMN](https://github.com/sunny2109/SAFMN) | [Models](https://drive.google.com/drive/folders/12O_xgwfgc76DsYbiClYnl6ErCDrsi_S9?usp=share_link), [JPEG model](https://github.com/sunny2109/SAFMN/releases/tag/v0.1.1)
 - [RGT](https://github.com/zhengchen1999/RGT) | [RGT Models](https://drive.google.com/drive/folders/1zxrr31Kp2D_N9a-OUAPaJEn_yTaSXTfZ?usp=drive_link), [RGT-S Models](https://drive.google.com/drive/folders/1j46WHs1Gvyif1SsZXKy1Y1IrQH0gfIQ1?usp=drive_link)
 - [DCTLSA](https://github.com/zengkun301/DCTLSA) | [Models](https://github.com/zengkun301/DCTLSA/tree/main/pretrained)
 - [ATD](https://github.com/LabShuHangGU/Adaptive-Token-Dictionary) | [Models](https://drive.google.com/drive/folders/1D3BvTS1xBcaU1mp50k3pBzUWb7qjRvmB?usp=sharing)
 - [AdaCode](https://github.com/kechunl/AdaCode) | [Models](https://github.com/kechunl/AdaCode/releases/tag/v0-pretrain_models)
+- [DRCT](https://github.com/ming053l/DRCT)
 
 #### Face Restoration
 
 - [GFPGAN](https://github.com/TencentARC/GFPGAN) | [1.2](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.2.pth), [1.3](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth), [1.4](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/GFPGANv1.4.pth)
 - [RestoreFormer](https://github.com/wzhouxiff/RestoreFormer) | [Model](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/RestoreFormer.pth)
 - [CodeFormer](https://github.com/sczhou/CodeFormer) (+) | [Model](https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth)
 
@@ -153,14 +154,15 @@
 - [Restormer](https://github.com/swz30/Restormer) (+) | [Models](https://github.com/swz30/Restormer/releases/tag/v1.0)
 - [FFTformer](https://github.com/kkkls/FFTformer) | [Models](https://github.com/kkkls/FFTformer/releases/tag/pretrain_model)
 - [M3SNet](https://github.com/Tombs98/M3SNet) (+) | [Models](https://drive.google.com/drive/folders/1y4BEX7LagtXVO98ZItSbJJl7WWM3gnbD)
 - [MPRNet](https://github.com/swz30/MPRNet) (+) | [Deblurring](https://drive.google.com/file/d/1QwQUVbk6YVOJViCsOKYNykCsdJSVGRtb/view?usp=sharing), [Deraining](https://drive.google.com/file/d/1O3WEJbcat7eTY6doXWeorAbQ1l_WmMnM/view?usp=sharing), [Denoising](https://drive.google.com/file/d/1LODPt9kYmxwU98g96UrRA0_Eh5HYcsRw/view?usp=sharing)
 - [MIRNet2](https://github.com/swz30/MIRNetv2) (+) | [Models](https://github.com/swz30/MIRNetv2/releases/tag/v1.0.0) (SR not supported)
 - [DnCNN, FDnCNN](https://github.com/cszn/DPIR) | [Models](https://github.com/cszn/KAIR/releases/tag/v1.0)
 - [DRUNet](https://github.com/cszn/DPIR) | [Models](https://github.com/cszn/KAIR/releases/tag/v1.0)
+- [IPT](https://github.com/huawei-noah/Pretrained-IPT) | [Models](https://drive.google.com/drive/folders/1MVSdUX0YBExauG0fFz4ANiWTrq9xZEj7?usp=sharing)
 
 #### DeJPEG
 
 - [FBCNN](https://github.com/jiaxi-jiang/FBCNN) | [Models](https://github.com/jiaxi-jiang/FBCNN/releases/tag/v1.0)
 
 #### Colorization
```

## Comparing `spandrel-0.3.1.dist-info/RECORD` & `spandrel-0.3.2.dist-info/RECORD`

 * *Files 11% similar despite different names*

```diff
@@ -1,29 +1,31 @@
-spandrel/__init__.py,sha256=Y56oc9ML3-s51Z0So7pUah8MrO2pWjOANtZt2JyhgFI,1136
+spandrel/__init__.py,sha256=jbcnkExMtVJRBhCqSvmAqBxw_14_gCCkojf_gn36DcY,1136
 spandrel/__helpers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 spandrel/__helpers/canonicalize.py,sha256=CxYNBAK-9l0Xpapky652Ii7VwcYt8LoEVeWsb83b9ow,1421
-spandrel/__helpers/loader.py,sha256=KC71_aTK9fCvS8FhWt-aKAfUcpClaQP1Dxcuui_dAh8,3498
-spandrel/__helpers/main_registry.py,sha256=t2LrFPYkvzqP_DCQ5LcT4IqmI21jEGgUc43kQJ9pXLg,2360
+spandrel/__helpers/loader.py,sha256=UPwpmrjtk2SsKxvLEddI-ZWSZwcIuQHgt2xgNfSu9cc,3801
+spandrel/__helpers/main_registry.py,sha256=FSvxRpZFy14_nn7LjtpGTPv7uUWtvRzzjGJky7EDZPo,2558
 spandrel/__helpers/model_descriptor.py,sha256=MRfnOcmMTFkFhxsvLLZGcJ4ZurkkauDlNLBJ5drDgE0,19466
 spandrel/__helpers/registry.py,sha256=mf-VFANwRYl0CYR5j_jCLA_NSjgfI5BfkPBPXo7liSA,5853
 spandrel/__helpers/size_req.py,sha256=wd027UPsOuJ2pVVjXjh85mIzLTYC9bH8EmhRLq1JHVM,3026
 spandrel/__helpers/unpickler.py,sha256=YA12ATt4jmjK_K3_v7g_cjU9X5Bi9SINEyvQYMgFcBs,976
 spandrel/architectures/__init__.py,sha256=wfh0afLbLNyLCOeCYGPx6k092dQ-nkmtuckY0CVXIug,117
-spandrel/architectures/ATD/__init__.py,sha256=eZ6es32_lTaf8PPKB0uEEzeOcg_vO-XTkyiXTaGw0NY,6706
-spandrel/architectures/ATD/arch/atd_arch.py,sha256=dSEROIgEUtIRBo0wTL79j6krG2y-Ucesm3etQ-RKNhM,40948
+spandrel/architectures/ATD/__init__.py,sha256=QaAlIce1PY6yOrK4hPQ-q-wGCltInK5RErz5U7_QN9k,6537
+spandrel/architectures/ATD/arch/atd_arch.py,sha256=TFlw8HrTqpeaKnsJud6gq98pXVB21gSG0O_hSfdcXnk,41329
 spandrel/architectures/CRAFT/__init__.py,sha256=fgLgHrnNhRnbu7sM5Gu8Ou0h8HBSn6FHc_TBD-shuIs,4084
-spandrel/architectures/CRAFT/arch/CRAFT.py,sha256=G9Eh_bmLVnEzW2q-QmC0NGnO_LY_7sWpNCHD7hRWlB4,30821
+spandrel/architectures/CRAFT/arch/CRAFT.py,sha256=VQp57kOCkWyExHGFPOYjtkARfmRpfIqAEfRNLdxmrbQ,30815
 spandrel/architectures/Compact/__init__.py,sha256=RzP7yCP9oHoXqcL5vQVnGbcXeV2Z2wRPHIQKGOE8fRw,1635
 spandrel/architectures/Compact/arch/SRVGG.py,sha256=UrulTEGfzj4gdlGkXjCGyuEn7NrAb7fdnUNGyDjoK5s,2835
-spandrel/architectures/DAT/__init__.py,sha256=YsmhgfOaSZVC_NCpDQhi-vxX5-CZofWgM8IIyVeY0MI,4839
-spandrel/architectures/DAT/arch/DAT.py,sha256=CdMbhVqd2jq5UNwjLGN-ow5zXcmQCp_JgGe54BVZFV4,36851
+spandrel/architectures/DAT/__init__.py,sha256=2rHSoyA32-T0EEUtAAyK7l5HcYyAMB0ttppsCp2H-io,6629
+spandrel/architectures/DAT/arch/DAT.py,sha256=2jW3OXQCQIea2erx6_SmUyBq6B5XgwqWcA9WiN2Wo9w,36845
 spandrel/architectures/DCTLSA/__init__.py,sha256=6-fkmpMrJ3fErp4dyxk6hLsNhbxaoG5TNJOx9yfypuE,2773
 spandrel/architectures/DCTLSA/arch/dctlsa.py,sha256=ysQDE4CW1ip97gOmxSMoiRC9Hx9mulab27A8Q0P_UQ8,15247
 spandrel/architectures/DITN/__init__.py,sha256=1W1wCxdbrrfkC3y1FHaVyJz3SyfUUYxraF_p4qpq90g,2660
 spandrel/architectures/DITN/arch/DITN_Real.py,sha256=-MfzAgI8eKfo6vGuZ_l5RT-yi85QWk4-e5cXB0YfFi4,9484
+spandrel/architectures/DRCT/__init__.py,sha256=anz2bU4bqdg7eYmK7CtydIj9TI3QHBKjQeZYBQMbjfM,5144
+spandrel/architectures/DRCT/arch/drct_arch.py,sha256=yxEzUIfuCHfyQKQhEhqX3kLaSyO6jH9Tl0EoQm7b4lQ,28533
 spandrel/architectures/DRUNet/__init__.py,sha256=8nm3A4ic3wnVBqoFzGHne2Wl0xjfV3MmT-xlDYYIhbE,3163
 spandrel/architectures/DRUNet/arch/network_unet.py,sha256=mD-0dFWB-KFn0DgZU_YuhDoMDZvBdS0ArLcrtlWvPu0,3630
 spandrel/architectures/DnCNN/__init__.py,sha256=2SBr0NV1Tftf8A506tV5HjoeTBt_yRWCbCcRQlwJDYA,3849
 spandrel/architectures/DnCNN/arch/network_dncnn.py,sha256=8wUp_D0ZwK8NRHTDXl3cztd-fttNEbb4dBm2NIPSBhg,2889
 spandrel/architectures/ESRGAN/__init__.py,sha256=7HL37LKKzo6JY1HTD3700rbfudLCNUlsDqK0QjjxuTM,7553
 spandrel/architectures/ESRGAN/arch/RRDB.py,sha256=MND164-1Q3MZZ3dE_jdI_xLX_BcxhkEpUMMjrl4qR-w,4456
 spandrel/architectures/FBCNN/__init__.py,sha256=M-mnbRYkIi8Hld0XmF0n-94cuU_I7QQzslrvCK7t7rs,2578
@@ -36,25 +38,29 @@
 spandrel/architectures/GFPGAN/arch/gfpgan_bilinear_arch.py,sha256=btGvnRcmA4EPqsm3xhkkSI2o9IwWL17lrMUy1CQODJM,14296
 spandrel/architectures/GFPGAN/arch/gfpganv1_arch.py,sha256=b_JRK2W7yMeB9IrBMEsVrdDaplrD-A_voC0YtQk_qCo,19614
 spandrel/architectures/GFPGAN/arch/gfpganv1_clean_arch.py,sha256=78_oSp-tWNmItg8DQjXXbvDIqlcircah19kBW-xlPq4,13677
 spandrel/architectures/GFPGAN/arch/stylegan2_arch.py,sha256=WLKPj3wMCsE-ZqrDCZQgiID9-mnZ9T-UTNovmIPbEXo,28263
 spandrel/architectures/GFPGAN/arch/stylegan2_bilinear_arch.py,sha256=VVztMx2F7RQf_nLt4lnPwJjjmuqgOggB8kiqRh2cyVY,23081
 spandrel/architectures/GFPGAN/arch/stylegan2_clean_arch.py,sha256=zgy_JYh_b85HDqkRJKdkIGZE6f_78B9O-exGA2wyGHk,16045
 spandrel/architectures/GFPGAN/arch/upfirdn2d.py,sha256=OIs2L8svz8szh7wrjxN_KgRU-0VChvXGWoBNqJONzs0,5589
-spandrel/architectures/GRL/__init__.py,sha256=CSKoMd-X2KZc4KuJcGHpinbmv4ku6t991HaukGtAImo,13523
+spandrel/architectures/GRL/__init__.py,sha256=h8Xubf2mhi96zCs-85p54lSFw83sXHDXeNeAquqMw18,13385
 spandrel/architectures/GRL/arch/config.py,sha256=YiMgmGspwWj4_EY9rgU4m6GMWky9Ec12jP8j3i2kKOk,1225
-spandrel/architectures/GRL/arch/grl.py,sha256=N4-wBpUymcJz0d3OUVtQb87FOsSLsIaP6adNp81wsUo,26006
-spandrel/architectures/GRL/arch/mixed_attn_block.py,sha256=2uspa-LVNpLqAPvEVHlV506rbtNwlLnL1GjnRBW7D1g,39569
-spandrel/architectures/GRL/arch/mixed_attn_block_efficient.py,sha256=OgY4CfRzZuwW52E3M__koIil1iLPvtOjCpJpj7JPoKA,20122
-spandrel/architectures/GRL/arch/ops.py,sha256=r2JLW1KRElXOOjYjMKr1L2m_mYIrh5iFw1d-swCeXrA,19820
-spandrel/architectures/GRL/arch/swin_v1_block.py,sha256=vJ7Iz3D0DcaN1Y8qx5OwPXUP8YEM_Ri4C1D02XwtLq4,20723
+spandrel/architectures/GRL/arch/grl.py,sha256=w55_XLI1p9EDHXmrY7j4eqd2LMvjachLVSomu-Usics,25424
+spandrel/architectures/GRL/arch/mixed_attn_block.py,sha256=hyOQk5Lx_rwxQsjBypBng80Nrq2Kv5ECemWo2D9CiUA,7043
+spandrel/architectures/GRL/arch/mixed_attn_block_efficient.py,sha256=j-NCLxWdaAaml4R58um8TQR4MlsUdiFldoyE_YygntM,20088
+spandrel/architectures/GRL/arch/ops.py,sha256=pPIPQZASlQj8DaDOEc2C4hZ0nEW1MZQzgEHmKdxL77Y,8423
+spandrel/architectures/GRL/arch/swin_v1_block.py,sha256=Yk4eWtddA9PHFiSe2A9jTNefte5SkPAObEObjP083ro,2021
 spandrel/architectures/GRL/arch/upsample.py,sha256=2N11fI6-_x_btrB7vm9t4IYc0y8misZFxXB5js6wlF4,1575
-spandrel/architectures/HAT/__init__.py,sha256=toEXiVgHbOri7X3Tc49yMfLcjPNdrJV9y_6M_UW8DIc,7782
-spandrel/architectures/HAT/arch/HAT.py,sha256=-QByvT9VRaONY8_Y92B-Tj5qa-FTLXHt1JLP40DkUoM,40884
-spandrel/architectures/KBNet/__init__.py,sha256=OTzhhli-QHMGX__wmK6qudkP3RLataB9InJj9hdIUfg,5399
+spandrel/architectures/HAT/__init__.py,sha256=mLweW3F6g_OeJRkkv0mqAev8uv1rMOZLSUm8I1UNrp4,8179
+spandrel/architectures/HAT/arch/HAT.py,sha256=CKl7bcsPyXUbhiOaJouXnZCcnLw8-3gXbyo7B5IumKs,39467
+spandrel/architectures/IPT/__init__.py,sha256=sthm-JBb_XpKLenCQ_8ltet4JH8d7tSdLogEo5zpbAQ,5362
+spandrel/architectures/IPT/arch/common.py,sha256=SoNAFE7LcUYG2aspWmyy9lUE3ETL1-Lhfm9HOcBXs_Q,2645
+spandrel/architectures/IPT/arch/ipt.py,sha256=iUfcDaguz-CntKy-XjwoTzmmYc1jZNc32M749Wp_LH0,11879
+spandrel/architectures/IPT/arch/model.py,sha256=AxwIG_hXTe7qqPvwGvsq8BOsL92MWWzepMihx68au0A,9255
+spandrel/architectures/KBNet/__init__.py,sha256=jOikiXK84-r7zkYrrFrjFp5rSu2wAN7hGQTrl0eQDzM,5521
 spandrel/architectures/KBNet/arch/kb_utils.py,sha256=cKU-a0Qn7dW5hHsGhCvQxUaEvfqipyTjmcohej1T9LY,4410
 spandrel/architectures/KBNet/arch/kbnet_l.py,sha256=as1yCID7oFrHV8ti-uFdfZevHATIJMacrhEWNa450nE,11639
 spandrel/architectures/KBNet/arch/kbnet_s.py,sha256=KTbMmuNAADilTAIDMu3slCTBrUUl89kkEla7tBBDJn0,8357
 spandrel/architectures/LaMa/__init__.py,sha256=2WkYgZQtlZ4q1-QSsB1rDx3CNhEXVHyjsxcYuQdFJ3U,1484
 spandrel/architectures/LaMa/arch/LaMa.py,sha256=Krf94KRxANehSDizijPX8vXsq-dPwqnzElpyMx_Av3s,20902
 spandrel/architectures/MMRealSR/__init__.py,sha256=xydrckxw2DnUq3ssKtebi5hUXYkid9zY0yxAzpgpq4c,6225
 spandrel/architectures/MMRealSR/arch/mmrealsr_arch.py,sha256=fY3ungLFsNwGqJEG0FTKh6IB_5fIVugUujQf0i-ml5k,23377
@@ -67,40 +73,42 @@
 spandrel/architectures/OmniSR/arch/ChannelAttention.py,sha256=DXvuY-ahJ58ty52CAXux8ctvk2DhcXbnbwdzevnGSso,3106
 spandrel/architectures/OmniSR/arch/OSA.py,sha256=YzjtWUQFrs1rHsco_dKymhK24nvE-b8GZj-Wckw3vsc,15054
 spandrel/architectures/OmniSR/arch/OSAG.py,sha256=s8SFztqA3dFyw7V-iXJuiNjvzgNZ-VVpLqdNIVhjD60,1714
 spandrel/architectures/OmniSR/arch/OmniSR.py,sha256=WcICMPe1AtrEaw7fw0AcGnZFECO8o0xO4WYgEn6zJVo,2666
 spandrel/architectures/OmniSR/arch/esa.py,sha256=zzEIFOzQ9S7E_D1XiSmk0sLJSOYi0WHTUlOc0RdDqEc,8315
 spandrel/architectures/OmniSR/arch/layernorm.py,sha256=bAllwJzSrqHlCXt9cftrXjj0hfuUEjp5izXUphbqWvo,2275
 spandrel/architectures/OmniSR/arch/pixelshuffle.py,sha256=jIjiKer0lcfD9flZ65Zy98rLDJkdJiOW9mmpZ7SvA9U,850
-spandrel/architectures/RGT/__init__.py,sha256=r4kQYwIM_Ue0x8VIF6WxWFn7ojHxu3thltjSf937WYI,5937
-spandrel/architectures/RGT/arch/rgt.py,sha256=Jp2pnz9smVxlrdKtSQJyhe39XBf7FRSj5ZsazFGoRQY,29903
+spandrel/architectures/RGT/__init__.py,sha256=rBCg54wNaE7cf7-jLbOKlwE57Gmd5G0gdDSHLrsAVc4,5925
+spandrel/architectures/RGT/arch/rgt.py,sha256=Y7_BMs7itVCMKZrU-QcRAZ0PUgwNbNGfoxWCwWxEokU,29897
 spandrel/architectures/RealCUGAN/__init__.py,sha256=uJVQm35UOFTl9StAAmwRXBP1ozRYPaIvxPhBG2g9OZc,3651
 spandrel/architectures/RealCUGAN/arch/upcunet_v3.py,sha256=TS3lf2efMaQAtOqdXrHYzJirXX8Fd1xjv9_6kXBbuxw,14795
-spandrel/architectures/RestoreFormer/__init__.py,sha256=Q-XI3lxwNYR7tpPlBJORHRfgCYDe2W5bIGue-8K1xTU,3086
-spandrel/architectures/RestoreFormer/arch/restoreformer_arch.py,sha256=QhVavGAzvlCeRHPeCzWfkve2O7GrvZOw3eZsrPHU-AA,23887
+spandrel/architectures/RestoreFormer/__init__.py,sha256=6DEPQ2RA8fycLxIX6R4taNNG9mWVzJjSqeacB5e_vK8,3410
+spandrel/architectures/RestoreFormer/arch/restoreformer_arch.py,sha256=ZlvrLgGF_eBRrRiWBt8Z4SixuDpObQycRdlp9iuQJYE,23884
 spandrel/architectures/SAFMN/__init__.py,sha256=-D2_tTavVgjt1Atk9a5qIs2cvkmAZKa-T8oSKS6hyQc,2100
 spandrel/architectures/SAFMN/arch/safmn.py,sha256=FDb1DwzA5-zNXHjGhQ27jpHWt-xD0Hud1PTbhFjQgFI,5019
+spandrel/architectures/SAFMNBCIE/__init__.py,sha256=mAikogocTj9Iu54Q5e5iOnApeWNCBsIgr52hp6clP-o,2596
+spandrel/architectures/SAFMNBCIE/arch/safmn_bcie.py,sha256=x1f8BQ2VurANL3VNbxjAn6MOz_bDsOYRJhqfg7WubjQ,4168
 spandrel/architectures/SCUNet/__init__.py,sha256=jdP_kK-NxieKK-Xlaiu1tES3NpLr_vbO-eZ8f2IhC5U,1904
 spandrel/architectures/SCUNet/arch/SCUNet.py,sha256=WYZAsjw94KH922v7esG3ftSlEMOg6yBsshX8E7KgvqU,13786
 spandrel/architectures/SPAN/__init__.py,sha256=3fNjQWKTdFzZvv6zA-hgehjQJJ3Yyj-5PTkxwkVzDFA,2289
 spandrel/architectures/SPAN/arch/span.py,sha256=m5hXlUak8hoZsxyIz9BzjgQLkwTpMwKM6QOE28DIQuY,9234
 spandrel/architectures/SwiftSRGAN/__init__.py,sha256=aJVGCf_eyU8I5ovrZt5dXDtCFmMF9LdUlro2f3KBOXI,1733
 spandrel/architectures/SwiftSRGAN/arch/SwiftSRGAN.py,sha256=IOOndnoBIOuynDtyijIN1c_xNVMpDfFAueC8ioaMVbc,4104
-spandrel/architectures/Swin2SR/__init__.py,sha256=wXO8PR1Cx7uvXzlg2huR3RH6muRBXSiT86AHAy8bu1o,6726
+spandrel/architectures/Swin2SR/__init__.py,sha256=wfWe8ZvjfgXK9ljvapuiiJmKOTM8ZwAc8Ojsres0SI8,6551
 spandrel/architectures/Swin2SR/arch/Swin2SR.py,sha256=cWH-jvjJjQQleJ1ziKhB0g2kqYFY2uImyPLGPjILyKw,46536
-spandrel/architectures/SwinIR/__init__.py,sha256=2wc7Wdfh0vDSj19aHQqkbLMPb-y0dE_3iRisqy1wDgE,6331
+spandrel/architectures/SwinIR/__init__.py,sha256=_cIEt1jh1B0hy_fhahr31LYC49TskNcJBFx7byD4-bc,6067
 spandrel/architectures/SwinIR/arch/SwinIR.py,sha256=mwfAWgNPkqvYJAxViP0UCGGT1RyvrEnyxy_DvjaN5Sc,38906
 spandrel/architectures/Uformer/__init__.py,sha256=dIzS2WHhGkb6aMujkgodVdpLFh1APPH0mPd-ViBVrlw,5727
-spandrel/architectures/Uformer/arch/Uformer.py,sha256=q1Loz2wMqNhFjLdzMv20eLz050dY5CpQJp8SXE3idLQ,55152
+spandrel/architectures/Uformer/arch/Uformer.py,sha256=qrM0naYnDnnus_CV0QmP8F8XulBET7n6gyTb3jf57dc,54675
 spandrel/architectures/__arch_helpers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 spandrel/architectures/__arch_helpers/block.py,sha256=4NwY4AjNWKlBQovbzKzlEBJMwKZfFoBvK3mu6CoAvo8,13495
 spandrel/architectures/__arch_helpers/dpir_basic_block.py,sha256=LG-XW8KLKU1yWElyzn2KGWXc03FfTF68LUPenasznao,10601
 spandrel/architectures/__arch_helpers/padding.py,sha256=azwwAAZ1fPaK37IG9TEQDbNcwOZXoa1xeGapV5oUI0k,770
-spandrel/util/__init__.py,sha256=6oGP0xyBEFmiyOSj6O-zXOwirGfOnSRFM30r2-rccPE,5431
+spandrel/util/__init__.py,sha256=irRfy2qfHqgpVRWGpQOzVsJym4xOZ88WSvViPdqVu-A,6433
 spandrel/util/timm/__drop.py,sha256=qslYFOof8yBrjVBAEoSAocWOYDIZYJChEpFvxeqZr18,7368
 spandrel/util/timm/__helpers.py,sha256=478WrzOnSDpaad97neragwGczLBIO3C6JfCEkcF1dik,788
 spandrel/util/timm/__init__.py,sha256=JIG2mLkdHSaCFYU4ucmjyhy5zI2tuIhHSUheam9TnOc,314
 spandrel/util/timm/__weight_init.py,sha256=GrJiLolNRvps3py643cLdVB86keFoOT55bA5dkPtITI,5088
-spandrel-0.3.1.dist-info/METADATA,sha256=ViQHouMKlUNIuyZgMYZV0vktw7Pzt3UMFhII000Zs90,14005
-spandrel-0.3.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-spandrel-0.3.1.dist-info/top_level.txt,sha256=bNYky20zJCBLFqnR9qneKKk45heuhs9rCRGOrt3CFas,9
-spandrel-0.3.1.dist-info/RECORD,,
+spandrel-0.3.2.dist-info/METADATA,sha256=9ERTTiuVFZpnY3lTjSYG2ler4vbMh951oIjzLqassvE,14270
+spandrel-0.3.2.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+spandrel-0.3.2.dist-info/top_level.txt,sha256=bNYky20zJCBLFqnR9qneKKk45heuhs9rCRGOrt3CFas,9
+spandrel-0.3.2.dist-info/RECORD,,
```

