# Comparing `tmp/lazylinop-1.9.0-py3-none-any.whl.zip` & `tmp/lazylinop-1.9.0a0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,71 +1,71 @@
-Zip file size: 137307 bytes, number of entries: 69
--rw-r--r--  2.0 unx      176 b- defN 24-May-05 19:40 lazylinop/__init__.py
--rw-r--r--  2.0 unx     8275 b- defN 24-May-05 19:40 lazylinop/check_op.py
--rw-r--r--  2.0 unx    53134 b- defN 24-May-05 19:40 lazylinop/lazylinop.py
--rw-r--r--  2.0 unx      370 b- defN 24-May-05 19:40 lazylinop/basicops/__init__.py
--rw-r--r--  2.0 unx     1508 b- defN 24-May-05 19:40 lazylinop/basicops/add.py
--rw-r--r--  2.0 unx     2098 b- defN 24-May-05 19:40 lazylinop/basicops/blockdiag.py
--rw-r--r--  2.0 unx     1852 b- defN 24-May-05 19:40 lazylinop/basicops/cat.py
--rw-r--r--  2.0 unx    11518 b- defN 24-May-05 19:40 lazylinop/basicops/diag.py
--rw-r--r--  2.0 unx     3287 b- defN 24-May-05 19:40 lazylinop/basicops/eye.py
--rw-r--r--  2.0 unx     3600 b- defN 24-May-05 19:40 lazylinop/basicops/kron.py
--rw-r--r--  2.0 unx     2261 b- defN 24-May-05 19:40 lazylinop/basicops/ones.py
--rw-r--r--  2.0 unx    42194 b- defN 24-May-05 19:40 lazylinop/basicops/pad.py
--rw-r--r--  2.0 unx     1343 b- defN 24-May-05 19:40 lazylinop/basicops/zeros.py
--rw-r--r--  2.0 unx      264 b- defN 24-May-05 19:40 lazylinop/polynomial/__init__.py
--rw-r--r--  2.0 unx    52587 b- defN 24-May-05 19:40 lazylinop/polynomial/polynomial.py
--rw-r--r--  2.0 unx      100 b- defN 24-May-05 19:40 lazylinop/wip/__init__.py
--rw-r--r--  2.0 unx     1443 b- defN 24-May-05 19:40 lazylinop/wip/code_optimization.py
--rw-r--r--  2.0 unx     1447 b- defN 24-May-05 19:40 lazylinop/wip/linear_algebra.py
--rw-r--r--  2.0 unx    56577 b- defN 24-May-05 19:40 lazylinop/wip/special_matrices.py
--rw-r--r--  2.0 unx       85 b- defN 24-May-05 19:40 lazylinop/wip/basicops/__init__.py
--rw-r--r--  2.0 unx    12477 b- defN 24-May-05 19:40 lazylinop/wip/basicops/anti_diag.py
--rw-r--r--  2.0 unx     4066 b- defN 24-May-05 19:40 lazylinop/wip/basicops/average.py
--rw-r--r--  2.0 unx     2206 b- defN 24-May-05 19:40 lazylinop/wip/basicops/mean.py
--rw-r--r--  2.0 unx      298 b- defN 24-May-05 19:40 lazylinop/wip/linalg/__init__.py
--rw-r--r--  2.0 unx     4486 b- defN 24-May-05 19:40 lazylinop/wip/linalg/coshm.py
--rw-r--r--  2.0 unx     2816 b- defN 24-May-05 19:40 lazylinop/wip/linalg/cosm.py
--rw-r--r--  2.0 unx     4037 b- defN 24-May-05 19:40 lazylinop/wip/linalg/expm.py
--rw-r--r--  2.0 unx     5461 b- defN 24-May-05 19:40 lazylinop/wip/linalg/khatri_rao.py
--rw-r--r--  2.0 unx     4295 b- defN 24-May-05 19:40 lazylinop/wip/linalg/logm.py
--rw-r--r--  2.0 unx     4388 b- defN 24-May-05 19:40 lazylinop/wip/linalg/sinhm.py
--rw-r--r--  2.0 unx     2823 b- defN 24-May-05 19:40 lazylinop/wip/linalg/sinm.py
--rw-r--r--  2.0 unx     1582 b- defN 24-May-05 19:40 lazylinop/wip/linalg/spectral_norm.py
--rw-r--r--  2.0 unx     3578 b- defN 24-May-05 19:40 lazylinop/wip/linalg/sqrtm.py
--rw-r--r--  2.0 unx      253 b- defN 24-May-05 19:40 lazylinop/wip/parallel/__init__.py
--rw-r--r--  2.0 unx     8328 b- defN 24-May-05 19:40 lazylinop/wip/parallel/mpilop.py
--rw-r--r--  2.0 unx     4897 b- defN 24-May-05 19:40 lazylinop/wip/parallel/pmatmat.py
--rw-r--r--  2.0 unx     4250 b- defN 24-May-05 19:40 lazylinop/wip/parallel/pmatmat_mpi.py
--rw-r--r--  2.0 unx     5902 b- defN 24-May-05 19:40 lazylinop/wip/parallel/pmatmat_process.py
--rw-r--r--  2.0 unx     3306 b- defN 24-May-05 19:40 lazylinop/wip/parallel/pmatmat_thread.py
--rw-r--r--  2.0 unx       23 b- defN 24-May-05 19:40 lazylinop/wip/random/__init__.py
--rw-r--r--  2.0 unx     6721 b- defN 24-May-05 19:40 lazylinop/wip/random/rand.py
--rw-r--r--  2.0 unx      501 b- defN 24-May-05 19:40 lazylinop/wip/signal/__init__.py
--rw-r--r--  2.0 unx     3659 b- defN 24-May-05 19:40 lazylinop/wip/signal/anti_eye.py
--rw-r--r--  2.0 unx     4752 b- defN 24-May-05 19:40 lazylinop/wip/signal/bc.py
--rw-r--r--  2.0 unx     4117 b- defN 24-May-05 19:40 lazylinop/wip/signal/bc2d.py
--rw-r--r--  2.0 unx    33110 b- defN 24-May-05 19:40 lazylinop/wip/signal/convolve.py
--rw-r--r--  2.0 unx    12402 b- defN 24-May-05 19:40 lazylinop/wip/signal/convolve2d.py
--rw-r--r--  2.0 unx    10537 b- defN 24-May-05 19:40 lazylinop/wip/signal/dct.py
--rw-r--r--  2.0 unx     3013 b- defN 24-May-05 19:40 lazylinop/wip/signal/decimate.py
--rw-r--r--  2.0 unx    10239 b- defN 24-May-05 19:40 lazylinop/wip/signal/ds_mconv.py
--rw-r--r--  2.0 unx     9694 b- defN 24-May-05 19:40 lazylinop/wip/signal/dsconvolve.py
--rw-r--r--  2.0 unx    11453 b- defN 24-May-05 19:40 lazylinop/wip/signal/dst.py
--rw-r--r--  2.0 unx    11717 b- defN 24-May-05 19:40 lazylinop/wip/signal/dwt1d.py
--rw-r--r--  2.0 unx    18373 b- defN 24-May-05 19:40 lazylinop/wip/signal/dwt2d.py
--rw-r--r--  2.0 unx    10924 b- defN 24-May-05 19:40 lazylinop/wip/signal/fft.py
--rw-r--r--  2.0 unx     2714 b- defN 24-May-05 19:40 lazylinop/wip/signal/fft2.py
--rw-r--r--  2.0 unx     2629 b- defN 24-May-05 19:40 lazylinop/wip/signal/flip.py
--rw-r--r--  2.0 unx     7242 b- defN 24-May-05 19:40 lazylinop/wip/signal/fwht.py
--rw-r--r--  2.0 unx      269 b- defN 24-May-05 19:40 lazylinop/wip/signal/is_power_of_two.py
--rw-r--r--  2.0 unx     3057 b- defN 24-May-05 19:40 lazylinop/wip/signal/mslices.py
--rw-r--r--  2.0 unx     2147 b- defN 24-May-05 19:40 lazylinop/wip/signal/oa.py
--rw-r--r--  2.0 unx     2958 b- defN 24-May-05 19:40 lazylinop/wip/signal/scatter_and_gather_windows.py
--rw-r--r--  2.0 unx     8565 b- defN 24-May-05 19:40 lazylinop/wip/signal/slices.py
--rw-r--r--  2.0 unx     4291 b- defN 24-May-05 19:40 lazylinop/wip/signal/stft.py
--rw-r--r--  2.0 unx     1434 b- defN 24-May-05 19:41 lazylinop-1.9.0.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx     8483 b- defN 24-May-05 19:41 lazylinop-1.9.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-May-05 19:41 lazylinop-1.9.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 24-May-05 19:41 lazylinop-1.9.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     5989 b- defN 24-May-05 19:41 lazylinop-1.9.0.dist-info/RECORD
-69 files, 524683 bytes uncompressed, 127817 bytes compressed:  75.6%
+Zip file size: 138601 bytes, number of entries: 69
+-rw-r--r--  2.0 unx      178 b- defN 24-Apr-30 12:04 lazylinop/__init__.py
+-rw-r--r--  2.0 unx     8275 b- defN 24-Apr-30 12:04 lazylinop/check_op.py
+-rw-r--r--  2.0 unx    52918 b- defN 24-Apr-30 12:04 lazylinop/lazylinop.py
+-rw-r--r--  2.0 unx      370 b- defN 24-Apr-30 12:04 lazylinop/basicops/__init__.py
+-rw-r--r--  2.0 unx     1508 b- defN 24-Apr-30 12:04 lazylinop/basicops/add.py
+-rw-r--r--  2.0 unx     2255 b- defN 24-Apr-30 12:04 lazylinop/basicops/blockdiag.py
+-rw-r--r--  2.0 unx     1852 b- defN 24-Apr-30 12:04 lazylinop/basicops/cat.py
+-rw-r--r--  2.0 unx    11699 b- defN 24-Apr-30 12:04 lazylinop/basicops/diag.py
+-rw-r--r--  2.0 unx     3449 b- defN 24-Apr-30 12:04 lazylinop/basicops/eye.py
+-rw-r--r--  2.0 unx     3792 b- defN 24-Apr-30 12:04 lazylinop/basicops/kron.py
+-rw-r--r--  2.0 unx     2309 b- defN 24-Apr-30 12:04 lazylinop/basicops/ones.py
+-rw-r--r--  2.0 unx    42464 b- defN 24-Apr-30 12:04 lazylinop/basicops/pad.py
+-rw-r--r--  2.0 unx     1573 b- defN 24-Apr-30 12:04 lazylinop/basicops/zeros.py
+-rw-r--r--  2.0 unx      264 b- defN 24-Apr-30 12:04 lazylinop/polynomial/__init__.py
+-rw-r--r--  2.0 unx    53006 b- defN 24-Apr-30 12:04 lazylinop/polynomial/polynomial.py
+-rw-r--r--  2.0 unx      100 b- defN 24-Apr-30 12:04 lazylinop/wip/__init__.py
+-rw-r--r--  2.0 unx     1443 b- defN 24-Apr-30 12:04 lazylinop/wip/code_optimization.py
+-rw-r--r--  2.0 unx     1447 b- defN 24-Apr-30 12:04 lazylinop/wip/linear_algebra.py
+-rw-r--r--  2.0 unx    57938 b- defN 24-Apr-30 12:04 lazylinop/wip/special_matrices.py
+-rw-r--r--  2.0 unx       85 b- defN 24-Apr-30 12:04 lazylinop/wip/basicops/__init__.py
+-rw-r--r--  2.0 unx    12658 b- defN 24-Apr-30 12:04 lazylinop/wip/basicops/anti_diag.py
+-rw-r--r--  2.0 unx     4066 b- defN 24-Apr-30 12:04 lazylinop/wip/basicops/average.py
+-rw-r--r--  2.0 unx     2206 b- defN 24-Apr-30 12:04 lazylinop/wip/basicops/mean.py
+-rw-r--r--  2.0 unx      298 b- defN 24-Apr-30 12:04 lazylinop/wip/linalg/__init__.py
+-rw-r--r--  2.0 unx     4792 b- defN 24-Apr-30 12:04 lazylinop/wip/linalg/coshm.py
+-rw-r--r--  2.0 unx     2816 b- defN 24-Apr-30 12:04 lazylinop/wip/linalg/cosm.py
+-rw-r--r--  2.0 unx     4233 b- defN 24-Apr-30 12:04 lazylinop/wip/linalg/expm.py
+-rw-r--r--  2.0 unx     5587 b- defN 24-Apr-30 12:04 lazylinop/wip/linalg/khatri_rao.py
+-rw-r--r--  2.0 unx     4667 b- defN 24-Apr-30 12:04 lazylinop/wip/linalg/logm.py
+-rw-r--r--  2.0 unx     4694 b- defN 24-Apr-30 12:04 lazylinop/wip/linalg/sinhm.py
+-rw-r--r--  2.0 unx     2823 b- defN 24-Apr-30 12:04 lazylinop/wip/linalg/sinm.py
+-rw-r--r--  2.0 unx     1582 b- defN 24-Apr-30 12:04 lazylinop/wip/linalg/spectral_norm.py
+-rw-r--r--  2.0 unx     3754 b- defN 24-Apr-30 12:04 lazylinop/wip/linalg/sqrtm.py
+-rw-r--r--  2.0 unx      253 b- defN 24-Apr-30 12:04 lazylinop/wip/parallel/__init__.py
+-rw-r--r--  2.0 unx     8328 b- defN 24-Apr-30 12:04 lazylinop/wip/parallel/mpilop.py
+-rw-r--r--  2.0 unx     4897 b- defN 24-Apr-30 12:04 lazylinop/wip/parallel/pmatmat.py
+-rw-r--r--  2.0 unx     4250 b- defN 24-Apr-30 12:04 lazylinop/wip/parallel/pmatmat_mpi.py
+-rw-r--r--  2.0 unx     5902 b- defN 24-Apr-30 12:04 lazylinop/wip/parallel/pmatmat_process.py
+-rw-r--r--  2.0 unx     3306 b- defN 24-Apr-30 12:04 lazylinop/wip/parallel/pmatmat_thread.py
+-rw-r--r--  2.0 unx       23 b- defN 24-Apr-30 12:04 lazylinop/wip/random/__init__.py
+-rw-r--r--  2.0 unx     6721 b- defN 24-Apr-30 12:04 lazylinop/wip/random/rand.py
+-rw-r--r--  2.0 unx      501 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/__init__.py
+-rw-r--r--  2.0 unx     3796 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/anti_eye.py
+-rw-r--r--  2.0 unx     4084 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/bc.py
+-rw-r--r--  2.0 unx     4117 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/bc2d.py
+-rw-r--r--  2.0 unx    34045 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/convolve.py
+-rw-r--r--  2.0 unx    12692 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/convolve2d.py
+-rw-r--r--  2.0 unx    11141 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/dct.py
+-rw-r--r--  2.0 unx     3272 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/decimate.py
+-rw-r--r--  2.0 unx    10179 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/ds_mconv.py
+-rw-r--r--  2.0 unx     9636 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/dsconvolve.py
+-rw-r--r--  2.0 unx    12181 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/dst.py
+-rw-r--r--  2.0 unx    12014 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/dwt1d.py
+-rw-r--r--  2.0 unx    18663 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/dwt2d.py
+-rw-r--r--  2.0 unx    11167 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/fft.py
+-rw-r--r--  2.0 unx     2714 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/fft2.py
+-rw-r--r--  2.0 unx     2854 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/flip.py
+-rw-r--r--  2.0 unx     7186 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/fwht.py
+-rw-r--r--  2.0 unx      269 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/is_power_of_two.py
+-rw-r--r--  2.0 unx     3315 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/mslices.py
+-rw-r--r--  2.0 unx     2438 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/oa.py
+-rw-r--r--  2.0 unx     3230 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/scatter_and_gather_windows.py
+-rw-r--r--  2.0 unx     9463 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/slices.py
+-rw-r--r--  2.0 unx     4291 b- defN 24-Apr-30 12:04 lazylinop/wip/signal/stft.py
+-rw-r--r--  2.0 unx     1434 b- defN 24-Apr-30 12:04 lazylinop-1.9.0a0.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     8485 b- defN 24-Apr-30 12:04 lazylinop-1.9.0a0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-30 12:04 lazylinop-1.9.0a0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 24-Apr-30 12:04 lazylinop-1.9.0a0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     5999 b- defN 24-Apr-30 12:04 lazylinop-1.9.0a0.dist-info/RECORD
+69 files, 534049 bytes uncompressed, 129091 bytes compressed:  75.8%
```

## zipnote {}

```diff
@@ -186,23 +186,23 @@
 
 Filename: lazylinop/wip/signal/slices.py
 Comment: 
 
 Filename: lazylinop/wip/signal/stft.py
 Comment: 
 
-Filename: lazylinop-1.9.0.dist-info/LICENSE.txt
+Filename: lazylinop-1.9.0a0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: lazylinop-1.9.0.dist-info/METADATA
+Filename: lazylinop-1.9.0a0.dist-info/METADATA
 Comment: 
 
-Filename: lazylinop-1.9.0.dist-info/WHEEL
+Filename: lazylinop-1.9.0a0.dist-info/WHEEL
 Comment: 
 
-Filename: lazylinop-1.9.0.dist-info/top_level.txt
+Filename: lazylinop-1.9.0a0.dist-info/top_level.txt
 Comment: 
 
-Filename: lazylinop-1.9.0.dist-info/RECORD
+Filename: lazylinop-1.9.0a0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## lazylinop/__init__.py

```diff
@@ -1,7 +1,7 @@
 from .check_op import check_op
 from . import lazylinop
 from .lazylinop import *
 from .basicops import *
 from .polynomial import *
 from .wip import signal
-__version__ = '1.9.0'
+__version__ = '1.9.0a0'
```

## lazylinop/lazylinop.py

```diff
@@ -136,18 +136,17 @@
             check_matfunc(f, fn)
 
         if matvec is None and matmat is None:
             raise ValueError('At least a matvec or a matmat function must be'
                              ' passed to the constructor.')
 
         def _matmat(M, _matvec, shape):
-            # M is always 2d
             nonlocal dtype
-            if M.shape[1] == 1:
-                return _matvec(M.ravel()).reshape(-1, 1)
+            if len(M.shape) == 1:
+                return _matvec(M)
             first_col = _matvec(M[:, 0])
             dtype = first_col.dtype
             out = np.empty((shape[0], M.shape[1]), dtype=dtype)
             out[:, 0] = first_col
             for i in range(1, M.shape[1]):
                 out[:, i] = _matvec(M[:, i])
             return out
@@ -667,36 +666,31 @@
             ``self @ op``.
 
         """
         from scipy.sparse import issparse
         self._sanitize_matmul(op)
         sanitize_op(op)
         if isinstance(op, np.ndarray) or issparse(op):
-            if op.ndim == 1:
-                # hasattr(op, 'reshape') == True
-                # because op is a np.ndarray
-                # (scipy matrix => ndim != 1)
+            if op.ndim == 1 and self._root_obj is not None:
                 res = self.lambdas['@'](op.reshape(op.size, 1)).ravel()
             elif op.ndim > 2:
                 from itertools import product
                 # op.ndim > 2
                 dtype = _binary_dtype(self.dtype, op.dtype)
                 res = np.empty((*op.shape[:-2], self.shape[0], op.shape[-1]),
                                dtype=dtype)
                 idl = [list(range(op.shape[i])) for i in range(op.ndim-2)]
                 for t in product(*idl):
                     tr = (*t, slice(0, res.shape[-2]), slice(0, res.shape[-1]))
                     to = (*t, slice(0, op.shape[-2]), slice(0, op.shape[-1]))
                     R = self.lambdas['@'](op.__getitem__(to))
                     res.__setitem__(tr, R)
                 # parallelization would not necessarily be faster because
-                # successive 2d multiplications might themselves be
-                # parallelized
+                # successive matrix products are themselves parallelized
             else:
-                # op.ndim == 2
                 res = self.lambdas['@'](op)
         else:
             if not LazyLinOp.islazylinop(op):
                 op = LazyLinOp._create_from_op(op)
             lambdas = {'@': lambda o: self @ (op @ o),
                        'H': lambda: op.H @ self.H,
                        'T': lambda: op.T @ self.T,
```

## lazylinop/basicops/blockdiag.py

```diff
@@ -48,23 +48,29 @@
         coffsets += [coffsets[i] + ops[i].shape[0]]
         if i == 0:
             dtype = ops[0].dtype
         else:
             dtype = binary_dtype(dtype, ops[i].dtype)
 
     def matmat(x, lmul, offsets):
-        # x is always 2d
+        if len(x.shape) == 1:
+            x_is_1d = True
+            x = x.reshape(x.size, 1)
+        else:
+            x_is_1d = False
         Ps = [None for _ in range(len(ops))]
         n = len(ops)
         # x can only be a numpy array or a scipy mat
         # hence Ps[i] is a numpy array whatever are ops
         for i, A in enumerate(ops):
             Ps[i] = lmul(A, x[offsets[i]:offsets[i+1]])
         S = Ps[0]
         for i in range(1, n):
             S = np.vstack((S, Ps[i]))
+        if x_is_1d:
+            S = S.ravel()
         return S
 
     return LazyLinOp((coffsets[-1], roffsets[-1]), matmat=lambda x:
                      matmat(x, lAx, roffsets),
                      rmatmat=lambda x: matmat(x, lAHx, coffsets),
                      dtype=dtype)
```

## lazylinop/basicops/diag.py

```diff
@@ -133,24 +133,30 @@
 
         def matmat(x, v, k):
             if issparse(x):
                 # because elementwise mul for scipy sparse
                 # matrix is not immediate
                 return spd @ x
             v = v.reshape(v.size, 1)
-            # x is always 2d
+            if len(x.shape) == 1:
+                x_is_1d = True
+                x = x.reshape(x.size, 1)
+            else:
+                x_is_1d = False
             if k > 0:
                 y = v * x[k:k+v.size]
                 y = np.vstack((y, np.zeros((k, x.shape[1]), dtype=v.dtype)))
             elif k < 0:
                 y = v * x[:v.size]
                 y = np.vstack((np.zeros((abs(k), x.shape[1]), dtype=v.dtype),
                                y))
             else:  # k == 0
                 y = v * x[:v.size]
+            if x_is_1d:
+                y = y.ravel()
             return y
         return LazyLinOp((m, m), matmat=lambda x: matmat(x, v, k),
                          rmatmat=lambda x: matmat(x, np.conj(v), -k),
                          dtype=v.dtype)
     elif v.ndim == 2:
         # extraction of op diagonal
         op = v
```

## lazylinop/basicops/eye.py

```diff
@@ -64,20 +64,24 @@
         .. seealso::
             `scipy.sparse.eye <https://docs.scipy.org/doc/scipy/reference/
             generated/scipy.sparse.eye.html>`_,
             `numpy.eye <https://numpy.org/devdocs/reference/generated/
             numpy.eye.html>`_.
     """
     def matmat(x, m, n, k):
-        # x is always 2d
         nonlocal dtype
         out_dtype = binary_dtype(dtype, x.dtype)
         # if eye is the identity just return x
         if k == 0 and m == n:
             return x
+        if len(x.shape) == 1:
+            x = x.reshape(x.size, 1)
+            x_1dim = True
+        else:
+            x_1dim = False
         minmn = min(m, n)
         if issparse(x):
             x = x.tocsr()
             _zeros = szeros
             _vstack = svstack
         else:
             _zeros = np.zeros
@@ -93,12 +97,14 @@
         mul = x[k: k + limk, :]
         if neg_k:
             mul = _vstack((nz, mul))
         if mul.shape[0] < m:
             z = _zeros((m - mul.shape[0], mul.shape[1]), dtype=out_dtype)
             t = (mul, z)
             mul = _vstack(t)
+        if x_1dim:
+            mul = mul.reshape(-1)
         return mul.astype(out_dtype)
     n = n if n is not None else m
     return LazyLinOp((m, n), matmat=lambda x: matmat(x, m, n, k),
                      rmatmat=lambda x: matmat(x, n, m, -k),
                      dtype=dtype)
```

## lazylinop/basicops/kron.py

```diff
@@ -56,19 +56,22 @@
     op1, op2 = aslazylinops(op1, op2)
 
     def _kron(op1, op2, shape, op):
 
         if isinstance(op, np.ndarray):
             op = np.asfortranarray(op)
 
-        # op is always 2d
-
         if (hasattr(op, 'reshape') and
            hasattr(op, '__matmul__') and hasattr(op, '__getitem__')):
 
+            if len(op.shape) == 1:
+                op = op.reshape((op.size, 1))
+                one_dim = True
+            else:
+                one_dim = False
             dtype = binary_dtype(binary_dtype(op1.dtype, op2.dtype), op.dtype)
             res = np.empty((shape[0], op.shape[1]), dtype=dtype)
 
             def out_col(j, ncols):
                 for j in range(j, min(j + ncols, op.shape[1])):
                     op_mat = op[:, j].reshape((op1.shape[1], op2.shape[1]))
                     # Do we multiply from left to right or from right to left?
@@ -80,15 +83,16 @@
                     if ltor < rtol:
                         res[:, j] = ((op1 @ op_mat) @ op2.T).reshape(shape[0])
                     else:
                         res[:, j] = (op1 @ (op_mat @ op2.T)).reshape(shape[0])
 
             ncols = op.shape[1]
             out_col(0, ncols)
-
+            if one_dim:
+                res = res.ravel()
         else:
             raise TypeError('op must possess reshape, __matmul__ and'
                             ' __getitem__ attributes to be multiplied by a'
                             ' Kronecker LazyLinOp (use toarray on the'
                             ' latter to multiply by the former)')
         return res
```

## lazylinop/basicops/ones.py

```diff
@@ -63,20 +63,22 @@
 
     m, n = shape
 
     if dtype is None:
         dtype = 'int'
 
     def mul(nrows, ncols, op):
-        # op is always 2d
         out_dtype = binary_dtype(dtype, op.dtype)
 
         # op is a np array or scipy matrix (see LazyLinOp.__matmul__)
         # so it has a sum method
         s = op.sum(axis=0)
         ret = vstack([s for _ in range(nrows)]).astype(out_dtype)
 
-        return ret
+        if op.ndim == 1:
+            return ret.ravel()
+        else:
+            return ret
 
     return LazyLinOp((m, n), matmat=lambda x: mul(m, n, x),
                      rmatmat=lambda x: mul(n, m, x),
                      dtype=dtype)
```

## lazylinop/basicops/pad.py

```diff
@@ -1095,34 +1095,42 @@
     if n <= 0:
         return eye(X * L, n=X * L, k=0)
 
     offset0 = int('before' in add) * n
     offset1 = 1 - int('after' in add)
 
     def _matmat(x):
-        # x is always 2d
         # x shape compatibility (number of rows) is tested beforehand by
         # LazyLinOp class
+        if len(x.shape) == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
         # add n zeros to each block
         y = np.zeros((offset0 + X * L + (X - offset1) * n, x.shape[1]),
                      dtype=x.dtype)
         for i in range(X):
             y[(offset0 + i * (L + n)):(offset0 + i * (L + n) + L), :] = (
                 x[(i * L):((i + 1) * L), :]
             )
-        return y
+        return y.ravel() if is_1d else y
 
     def _rmatmat(x):
-        # x is always 2d
+        if len(x.shape) == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
         # keep L elements every n + 1 elements
         y = np.zeros((X * L, x.shape[1]), dtype=x.dtype)
         for i in range(X):
             y[(i * L):((i + 1) * L), :] = (
                 x[(offset0 + i * (L + n)):(offset0 + i * (L + n) + L), :]
             )
-        return y
+        return y.ravel() if is_1d else y
 
     return LazyLinOp(
         shape=(offset0 + X * L + (X - offset1) * n, X * L),
         matmat=lambda x: _matmat(x),
         rmatmat=lambda x: _rmatmat(x)
     )
```

## lazylinop/basicops/zeros.py

```diff
@@ -32,14 +32,18 @@
     if dtype is None:
         dtype = 'float'
 
     def _matmat(op, shape):
         nonlocal dtype
         dtype = binary_dtype(dtype, op.dtype)
         # shape[1] == op.shape[0] (because of LazyLinOp)
-        # op.ndim == 2
-        return np.zeros((shape[0], op.shape[1]), dtype=dtype)
+        # op.ndim > 2 can't happen because of LazyLinOp def
+        # op a LazyLinOp can't happen either for the same reason
+        if op.ndim == 2:
+            return np.zeros((shape[0], op.shape[1]), dtype=dtype)
+        else:  # op.ndim == 1: (see LazyLinOp.__matmul__)
+            return np.zeros((shape[0],))
     return LazyLinOp(shape, matmat=lambda x:
                      _matmat(x, shape),
                      rmatmat=lambda x: _matmat(x, (shape[1],
                                                    shape[0])),
                      dtype=dtype)
```

## lazylinop/polynomial/polynomial.py

```diff
@@ -927,25 +927,29 @@
 
     if c.shape[0] == 0:
         raise Exception("List of coefficients has zero size.")
 
     def _matmat(L, x, c):
         # x can't be a LazyLinOp here because it's handle before in
         # LazyLinOp.__matmul__
-        # x is always 2d
+        if x.ndim == 1:
+            x1d = True
+            x = x.reshape(-1, 1)
+        else:
+            x1d = False
         out = (
             x * c[-1] if c[-1] != 0
             else np.zeros(x.shape,
                           dtype=binary_dtype(c.dtype, x.dtype))
         )
         for i in range(len(c) - 2, -1, -1):
             out = L @ out
             if c[i] != 0:
                 out += x * c[i]
-        return out
+        return out.ravel() if x1d else out
 
     return LazyLinOp(
         shape=L.shape,
         matmat=lambda x: _matmat(L, x, c),
         rmatmat=lambda x: _matmat(L.T.conj(), x, c)
     )
 
@@ -1000,23 +1004,28 @@
         raise Exception("roots must be a 1d array.")
 
     R = r.shape[0]
     if R == 0:
         raise Exception("List of roots has zero size.")
 
     def _matmat(roots, L, x):
-        # x is always 2d
+        if x.ndim == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
+
         Lx = L @ x if roots[-1] == 0 else (L @ x - roots[-1] * x)
         nr = len(roots)
 
         for i in range(nr - 2, -1, -1):
             r = roots[i]
             Lx = L @ Lx if r == 0.0 else (L @ Lx - r * Lx)
 
-        return Lx
+        return Lx.ravel() if is_1d else Lx
 
     return LazyLinOp(
         shape=L.shape,
         matmat=lambda x: _matmat(r, L, x),
         rmatmat=lambda x: _matmat(r, L.T.conj(), x)
     )
 
@@ -1375,20 +1384,25 @@
             The function alpha(k, L, bk') that computes alpha(k, L) @ bk'
             (alpha(k, L) is generally alpha(k) * L except for Laguerre).
         beta_func:
             The function (- beta(k)).
         phi1_func:
             The function phi1(L, bk) that computes phi1(L) @ bk.
     """
-    # x is always 2d
+
+    if x.ndim == 1:
+        is_1d = True
+        x = x.reshape(x.shape[0], 1)
+    else:
+        is_1d = False
     if c.shape[0] == 1:
-        return c[0] * x
+        return (c[0] * x).ravel() if is_1d else c[0] * x
     elif c.shape[0] == 2:
         y = c[0] * x + c[1] * phi1_func(L, x)
-        return y
+        return y.ravel() if is_1d else y
     else:
         # Clenshaw algorithm
         # alpha_k = alpha_func(k) * L
         # beta_k  = beta_func(k)
         # phi0 = eye(N, n=N)
         # phi1 = L
         # phi_{k + 1} = alpha_k * phi_k + beta_k * phi_{k - 1}
@@ -1403,15 +1417,15 @@
                 ((beta_func(k + 1) * b2) if beta_func(k + 1) != 1 else b2)
             )
             b2 = b1
             b1 = bk
         # phi0 is always 1/Id
         y = c[0] * x + phi1_func(L, b1) - (
             (beta_func(1) * b2) if beta_func(1) != 1 else b2)
-        return y
+        return y.ravel() if is_1d else y
 
 
 def power(L, n):
     r"""Constructs the n-th power :math:`L^n` of linear operator :math:`L`.
     Matrix representation of :math:`L` must be square.
     :octicon:`alert-fill;1em;sd-text-danger` In some cases
     :code:`power(L,n) @ x` can be least efficient than
```

## lazylinop/wip/special_matrices.py

```diff
@@ -95,15 +95,14 @@
 
     if x.ndim != 1:
         raise ValueError("x expects 1d-array.")
 
     M, N = x.shape[0], len(f)
 
     def _matmat(x, f, X, adjoint):
-        # X is always 2d
 
         def _1d(x, f, X, adjoint):
             if X.ndim != 1:
                 raise Exception("batch size must be equal to 1.")            
             y = np.empty(
                 N if adjoint else M,
                 dtype=np.complex_ if x.dtype.kind == 'c' or X.dtype.kind == 'c' else (x[0] * X[0]).dtype
@@ -134,15 +133,15 @@
                         y[i, b] = np.array([f[i](x[j]) for j in range(M)]) @ X[:, b]
             else:
                 for b in prange(batch_size):
                     for i in range(M):
                         y[i, b] = np.array([f[j](x[i]) for j in range(N)]) @ X[:, b]
             return y
 
-        return _1d(x, f, X.ravel(), adjoint).reshape(-1, 1) if X.shape[1] == 1 else _2d(x, f, X, adjoint)
+        return _1d(x, f, X, adjoint) if X.ndim == 1 else _2d(x, f, X, adjoint)
 
     return LazyLinOp(
         shape=(M, N),
         matmat=lambda X: _matmat(x, f, X, False),
         rmatmat=lambda X: _matmat(x, f, X, True)
     )
 
@@ -173,32 +172,36 @@
         raise ValueError("a expects a 1d array.")
     if a.shape[0] < 2:
         raise ValueError("# of coefficients must be at least >= 2.")
     if a[0] == 0.0:
         raise ValueError("The first coefficient a[0] must be != 0.")
 
     def _matmat(a, x, H):
-        # x is always 2d
-        batch_size = x.shape[1]
-
+        if x.ndim == 1:
+            x = x.reshape(x.shape[0], 1)
+            batch_size = 1
+            is_1d = True
+        else:
+            batch_size = x.shape[1]
+            is_1d = False
         N = a.shape[0]
         if 'complex' in a.dtype.str:
             y = np.empty((N - 1, batch_size), dtype=np.complex_)
         else:
             y = np.empty((N - 1, batch_size), dtype=(a[0] * x[0]).dtype)
         if H:
             # conjugate and transpose
             for b in range(batch_size):
                 y[:, b] = np.divide(np.multiply(a[1: ], x[0, b]), -a[0])
                 np.add(y[:(N - 2), b], x[1:(N - 1), b], out=y[:(N - 2), b])
         else:
             for b in range(batch_size):
                 y[0, b] = np.divide(a[1:], -a[0]) @ x[:, b]
                 y[1:(N - 1), b] = x[:(N - 2), b]
-        return y
+        return y.ravel() if is_1d else y
 
     return LazyLinOp(
         shape=(a.shape[0] - 1, a.shape[0] - 1),
         matmat=lambda x: _matmat(a, x, False),
         rmatmat=lambda x: _matmat(a, x, True)
     )
 
@@ -227,18 +230,23 @@
         See also `scipy.linalg.fiedler <https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.fiedler.html>`_.
         See also `Wikipedia <https://en.wikipedia.org/wiki/Algebraic_connectivity>`_.
     """
     if a.shape[0] == 0:
         raise ValueError("a is empty.")
 
     def _matmat(a, x):
-        # x is always 2d
+
+        if x.ndim == 1:
+            x = x.reshape(x.shape[0], 1)
+            is_1d = True
+        else:
+            is_1d = False
         batch_size = x.shape[1]
 
-        @njit(parallel=batch_size > 1, cache=True)
+        @njit(parallel=not is_1d, cache=True)
         def _bf(a, x):
             N = a.shape[0]
             __T = max(2, _T // 2)
             tmp_acc = np.full(__T, 0.0 * (a[0] * x[0, 0]))
             ai = np.full(__T, 0.0 * a[0])
             y = np.full((N, batch_size), 0.0 * (a[0] * x[0, 0]))
             BperT = int(np.ceil(batch_size / __T))
@@ -251,29 +259,29 @@
                         # (L + D + U) @ x where U = L^T
                         # L is a lower triangular matrix such that L[i, i] = 0
                         # and D is a diagonal matrix such that D[i, i] = 0.
                         # L @ x + (x^T @ L)^T
                         for j in range(N):
                             tmp_acc[t] += np.absolute(ai[t] - a[j]) * x[j, b]
                         y[i, b] = tmp_acc[t]
-            return y
+            return y.ravel() if is_1d else y
 
         def _no_bf(a, x):
             N = a.shape[0]
             y = np.full((N, batch_size), 0 * (a[0] * x[0, 0]))
             for i in range(N):
                 # (L + D + U) @ x where U = L^T
                 # L is a lower triangular matrix such that L[i, i] = 0
                 # and D is a diagonal matrix such that D[i, i] = 0.
                 # L @ x + (x^T @ L)^T
                 if i < (N - 1):
                     y[i, :] += np.absolute(np.subtract(a[i], a[(i + 1):])) @ x[(i + 1):, :]
                 if i > 0:
                     y[i, :] += np.absolute(np.subtract(a[i], a[:i])) @ x[:i, :]
-            return y
+            return y.ravel() if is_1d else y
 
         return _bf(a, x) if use_numba else _no_bf(a, x)
 
     return LazyLinOp(
         shape=(a.shape[0], a.shape[0]),
         matmat=lambda x: _matmat(a, x),
         rmatmat=lambda x: _matmat(a, x)
@@ -552,21 +560,28 @@
     """
 
     norm = np.sqrt(np.dot(v, v))
     if norm == 0.0:
         raise ValueError("The norm of vector v is zero.")
 
     def _matmat(v, x):
-        # x is always 2d
+        if x.ndim == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
         L = x.shape[0]
         batch_size = x.shape[1]
         y = np.empty(x.shape, dtype=x.dtype)
         for i in range(batch_size):
             y[:, i] = np.subtract(v[:, i], np.multiply(2.0, v @ (v.T.conj() @ x[:, i])))
-        return y
+        if is_1d:
+            return y.ravel()
+        else:
+            return y
 
     return LazyLinOp(
         shape=(v.shape[0], v.shape[0]),
         matmat=lambda x: _matmat(np.multiply(1.0 / norm, v), x),
         rmatmat=lambda x: _matmat(np.multiply(1.0 / norm, v), x)
     )
 
@@ -601,15 +616,14 @@
     References:
         See also `Lehmer matrix <https://en.wikipedia.org/wiki/Lehmer_matrix>`_.
     """
     if n < 2:
         raise ValueError("n must be >= 2.")
 
     def _matmat(n, x):
-        # x is always 2d
         nb.config.DISABLE_JIT = 0 if use_numba else 1
 
         # (L + D + U) @ x where U = L^T and D = Id
         # L is a lower triangular matrix such that L[i, i] = 0.
         # L @ x + x + (x^T @ L)^T
 
         @njit(parallel=False, cache=True)
@@ -669,17 +683,17 @@
                     for j in range(i + 1, x.shape[0]):
                         norm = min(i, j) / max(i, j)
                         y[i, b] += x[j, b] * norm
                         y[j, b] += x[i, b] * norm
             return y
 
         if bf:
-            return _bf1d(n, x.ravel()).reshape(-1, 1) if x.shape[1] == 1 else _bf2d(n, x)
+            return _bf1d(n, x) if x.ndim == 1 else _bf2d(n, x)
         else:
-            return _1d(n, x.ravel()).reshape(-1, 1) if x.shape[1] == 1 else _2d(n, x)
+            return _1d(n, x) if x.ndim == 1 else _2d(n, x)
 
     return LazyLinOp(
         shape=(n, n),
         matmat=lambda x: _matmat(n, x),
         rmatmat=lambda x: _matmat(n, x)
     )
 
@@ -818,16 +832,21 @@
         See also `scipy.linalg.pascal <https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.pascal.html>`_.
         See also `Pascal matrix <https://en.wikipedia.org/wiki/Pascal_matrix>`_.
     """
     if not kind in ['symmetric', 'lower', 'upper']:
         raise ValueError("kind is either 'symmetric', 'lower' or 'upper'.")
 
     def _matmat(n, x, kind):
-        # x is always 2d
-        batch_size = x.shape[1]
+        if x.ndim == 1:
+            x = x.reshape(x.shape[0], 1)
+            batch_size = 1
+            is_1d = True
+        else:
+            batch_size = x.shape[1]
+            is_1d = False
         # for large n entries of the Pascal matrix
         # become very big ! TODO something about it
         if n <= 160:
             y = np.empty((n, batch_size), dtype=x.dtype)
             Mx = np.empty(n, dtype=x.dtype)
         else:
             y = np.empty((n, batch_size), dtype=object)
@@ -876,15 +895,15 @@
                         # np.copyto(Mx, Dl @ y[:, b])
                         np.copyto(Mx, np.append([0.0], np.multiply(seq, y[:(n - 1), b])))
                     for i in range(1, n, 1):
                         factor /= i
                         np.add(y[:, b], np.multiply(factor, Mx), out=y[:, b])
                         # np.copyto(Mx, Dl @ Mx)
                         np.copyto(Mx, np.append([0.0], np.multiply(seq, Mx[:(n - 1)])))
-        return y
+        return y.ravel() if is_1d else y
 
     if kind == 'lower':
         kindT = 'upper'
     elif kind == 'upper':
         kindT = 'lower'
     else:
         kindT = kind
@@ -945,17 +964,21 @@
     References:
         See also `Redheffer matrix <https://en.wikipedia.org/wiki/Redheffer_matrix>`_.
     """
     if n < 2:
         raise ValueError("n must be >= 2.")
 
     def _matmat(n, x, adjoint):
-        # x is always 2d
         nb.config.DISABLE_JIT = 0 if use_numba else 1
 
+        if x.ndim == 1:
+            x = x.reshape(x.shape[0], 1)
+            is_1d = True
+        else:
+            is_1d = False
         batch_size = x.shape[1]
 
         BperT = int(np.ceil(batch_size / _T))
 
         # diagonal of Redheffer matrix is 1
         # first column as-well-as first row is 1
 
@@ -967,15 +990,15 @@
                 for i in range(1, n):
                     y[np.arange(i, n, i + 1), :] += x[i, :]
             else:
                 y = np.empty((n, batch_size), dtype=x.dtype)
                 y[0, :] = np.sum(x, axis=0)
                 for i in range(1, n):
                     y[i, :] = np.add(x[0, :], np.sum(x[np.arange(i, n, i + 1), :], axis=0))
-            return y
+            return y.ravel() if is_1d else y
 
         @njit(parallel=True, cache=True)
         def _bf2d(n, x):
             if x.ndim != 2:
                 raise Exception("batch size must be greater than 1.")
             batch_size = x.shape[1]
             y = np.empty((n, batch_size), dtype=x.dtype)
@@ -993,15 +1016,15 @@
                 for t in prange(_T):
                     for b in range(t * BperT, min(batch_size, (t + 1) * BperT)):
                         for i in range(n):
                             y[i, b] = x[0, b]
                             for j in range(i, n, i + 1):
                                 y[i, b] += x[j, b]
                         y[0, b] -= x[0, b]
-            return y
+            return y.ravel() if is_1d else y
 
         return _bf2d(n, x) if use_numba else _2d(n, x)
 
     return LazyLinOp(
         shape=(n, n),
         matmat=lambda x: _matmat(n, x, False),
         rmatmat=lambda x: _matmat(n, x, True)
@@ -1097,23 +1120,28 @@
     """
     if a.ndim != 2:
         raise ValueError("Argument a expects 2d array.")
     if a.shape[0] != a.shape[1]:
         return ValueError("# of rows and # of columns are differents.")
 
     def _matmat(a, x, adjoint):
-        # x is always 2d
 
         if islazylinop(x):
             # TODO: do better than that
             H = sp.linalg.hessenberg(np.eye(x.shape[0], M=x.shape[0], k=0) @ a)[what[mode]]
         else:
             H = sp.linalg.hessenberg(a, calc_q=False)
 
-        batch_size = x.shape[1]
+        if x.ndim == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+            batch_size = 1
+        else:
+            is_1d = False
+            batch_size = x.shape[1]
 
         y = np.empty((a.shape[0], batch_size), dtype=(a[0, 0] * x[0]).dtype)
 
         # Hessenberg matrix first sub-diagonal has non-zero entries
         if adjoint:
             for b in range(batch_size):
                 y[0, b] = H[:2, 0] @ x[:2, b]
@@ -1126,15 +1154,15 @@
             for b in range(batch_size):
                 y[0, b] = H[0, :] @ x[:, b]
                 if a.shape[0] >= 2:
                     y[1, b] = H[1, :] @ x[:, b]
                 if a.shape[0] > 2:
                     for i in range(2, a.shape[0]):
                         y[i, b] = H[i, (i - 1):] @ x[(i - 1):, b]
-        return y
+        return y.ravel() if is_1d else y
 
     return LazyLinOp(
         shape=(a.shape[0], a.shape[0]),
         matmat=lambda x: _matmat(a, x, False),
         # rmatmat=lambda x: _matmat(a.T.conj(), x, False)
         rmatmat=lambda x: _matmat(a, x, True)
     )
@@ -1185,34 +1213,42 @@
         cq = np.copy(cq[:1])
     Md = M - 1
     Nd = N - 1
     if M == 0 or N == 0:
         raise ValueError("List of coefficients should have at least one element.")
 
     def _matmat(cp, cq, x):
-        # x is always 2d
+        if x.ndim == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
         batch_size = x.shape[1]
         y = np.empty((Md + Nd, batch_size), dtype=x.dtype)
         for b in range(batch_size):
             for n in range(Nd):
                 y[n, b] = cp[::-1] @ x[n:(n + Md + 1), b]
             for m in range(Md):
                 y[Nd + m, b] = cq[::-1] @ x[m:(m + Nd + 1), b]
-        return y
+        return y.ravel() if is_1d else y
 
     def _rmatmat(cp, cq, x):
-        # x is always 2d
+        if x.ndim == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
         batch_size = x.shape[1]
         y = np.zeros((Md + Nd, batch_size), dtype=x.dtype)
         for b in range(batch_size):
             for n in range(Nd):
                 y[n:(n + Md + 1), b] += np.multiply(cp[::-1], x[n, b])
             for m in range(Md):
                 y[m:(m + Nd + 1), b] += np.multiply(cq[::-1], x[Nd + m, b])
-        return y
+        return y.ravel() if is_1d else y
 
     return LazyLinOp(
         shape=(Md + Nd, Md + Nd),
         matmat=lambda x: _matmat(cp, cq, x),
         rmatmat=lambda x: _rmatmat(cp, cq, x)
     )
 
@@ -1240,15 +1276,20 @@
 
     Examples:
 
     References:
     """
 
     def _matmat(a, x, factor, H):
-        # x is always 2d
+
+        if x.ndim == 1:
+            x = x.reshape(x.shape[0], 1)
+            is_1d = True
+        else:
+            is_1d = False
         batch_size = x.shape[1]
         BperT = int(np.ceil(batch_size / _T))
 
         if islazylinop(a):
             a = np.eye(a.shape[0], M=a.shape[0], k=0) @ a
 
         M, N = a.shape
@@ -1267,15 +1308,15 @@
                 y = np.full((M, batch_size),  0 * (a[0, 0] * x[0, 0]))
                 if factor == 'lower':
                     for i in range(M):
                         y[i, :] = a[i, :(i + 1)] @ x[:(i + 1), :]
                 else:
                     for i in range(M):
                         y[i, :] = a[i, i:N] @ x[i:N, :]
-            return y
+            return y.ravel() if is_1d else y
 
         @njit(parallel=True, cache=True)
         def _bf(a, x, factor, H):
             tmp = np.full(_T, 0 * (a[0, 0] * x[0, 0]))
             y = np.full((M, batch_size), 0 * (a[0, 0] * x[0, 0]))
             if H:
                 # conjugate transpose
@@ -1308,15 +1349,15 @@
                     for t in prange(_T):
                         for b in range(t * BperT, min(batch_size, (t + 1) * BperT)):
                             for i in range(M):
                                 tmp[t] = 0 * (a[0, 0] * x[0, 0])
                                 for j in range(i, N, 1):
                                     tmp[t] += a[i, j] * x[j, b]
                                 y[i, b] = tmp[t]
-            return y
+            return y.ravel() if is_1d else y
 
         return _bf(a, x, factor, H) if use_numba else _no_bf(a, x, factor, H)
 
     return LazyLinOp(
         shape=a.shape,
         matmat=lambda x: _matmat(a, x, factor, False),
         rmatmat=lambda x: _matmat(a, x, factor, True)
@@ -1365,29 +1406,33 @@
         raise ValueError("x must be a 1d-array.")
     if N is None:
         N = len(x)
     if N != int(N) or N < 1:
         raise ValueError("N expects an integer value >= 1.")
 
     def _matmat(x, X, H):
-        # X is always 2d
+        if X.ndim == 1:
+            X = X.reshape(X.shape[0], 1)
+            is_1d = True
+        else:
+            is_1d = False
         batch_size = X.shape[1]
 
         y = np.empty(
             (N if H else x.shape[0], batch_size),
             dtype=np.complex_ if x.dtype.kind == 'c' or X.dtype.kind == 'c' else (x[0] * X[0, 0]).dtype
         )
         if H:
             # conjugate and transpose
             for i in range(N):
                 y[i, :] = np.power(x, np.full(x.shape[0], i)) @ X
         else:
             for i in range(x.shape[0]):
                 y[i, :] = np.power(np.full(N, x[i]), np.arange(0, N)) @ X
-        return y
+        return y.ravel() if is_1d else y
 
     return LazyLinOp(
         shape=(x.shape[0], N),
         matmat=lambda X: _matmat(x, X, False),
         rmatmat=lambda X: _matmat(x, X, True)
     )
```

## lazylinop/wip/basicops/anti_diag.py

```diff
@@ -129,33 +129,39 @@
     if isinstance(v, np.ndarray) and v.ndim == 1:
         # building antidiagonal op
         m = v.size + abs(k)
         # spd is just in case x is sparse in matmat
         spd = [None]  # lazy instantiation
 
         def matmat(x, v, k):
-            # x is always 2d
             if issparse(x):
                 # because elementwise mul for scipy sparse
                 # matrix is not immediate
 
                 if spd[0] is None:
                     spd[0] = _sp_anti_diag(v, k)
                 return spd[0] @ x
 
             v = v.reshape(v.size, 1)
+            if len(x.shape) == 1:
+                x_is_1d = True
+                x = x.reshape(x.size, 1)
+            else:
+                x_is_1d = False
             if k > 0:
                 y = v * x[- 1 - k:-1 - k - v.size:-1]
                 y = np.vstack((y, np.zeros((k, x.shape[1]), dtype=v.dtype)))
             elif k < 0:
                 y = v * x[-1:-1 - v.size:-1]
                 y = np.vstack((np.zeros((abs(k), x.shape[1]), dtype=v.dtype),
                                y))
             else:  # k == 0
                 y = v * x[-1:-1 - v.size:-1]
+            if x_is_1d:
+                y = y.ravel()
             return y
         return LazyLinOp((m, m), matmat=lambda x: matmat(x, v, k),
                          rmatmat=lambda x: matmat(x, np.conj(v[::-1]), k),
                          dtype=v.dtype)
     elif v.ndim == 2:
         # extraction of op antidiagonal
         op = v
```

## lazylinop/wip/linalg/coshm.py

```diff
@@ -55,23 +55,27 @@
         if islazylinop(L):
             L = np.eye(L.shape[0], M=L.shape[0]) @ L
         if use_numba:
             C = sp.linalg.coshm(scale * L) @ X
             nb.config.DISABLE_JIT = 0 if use_numba else 1
             @njit(nopython=True, parallel=True, cache=True)
             def _matmat(C, x):
-                # x is always 2d
+                if x.ndim == 1:
+                    is_1d = True
+                    x = x.reshape(x.shape[0], 1)
+                else:
+                    is_1d = False
                 batch_size = x.shape[1]
                 if use_numba:
                     y = np.empty((C.shape[0], batch_size), dtype=x.dtype)
                     for b in prange(batch_size):
                         y[:, b] = C @ X[:, b]
                 else:
                     y = C @ X
-                return y
+                return y.ravel() if is_1d else y
             return LazyLinOp(
                 shape=L.shape,
                 matmat=lambda X: _matmat(C, X),
                 rmatmat=lambda X: _matmat(C.T.conj(), X)
             )
         else:
             return LazyLinOp(
@@ -79,17 +83,21 @@
                 matmat=lambda X: sp.linalg.coshm(scale * L) @ X,
                 rmatmat=lambda X: sp.linalg.coshm(scale * L.T.conj()) @ X
             )
     elif backend == 'serie':
         if nmax < 1:
             raise ValueError("nmax must be >= 1.")
         def _matmat(L, x):
-            # x is always 2d
             if L.shape[1] != x.shape[0]:
                 raise ValueError("L @ x does not work because # of columns of L is not equal to the # of rows of x.")
+            if x.ndim == 1:
+                is_1d = True
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             batch_size = x.shape[1]
             y = np.copy(x)
             # Taylor expansion
             # exp(scale * L) ~= Id + scale * L + (scale * L) ** 2 / 2 + ...
             # exp(-scale * L) ~= Id - scale * L + (scale * L) ** 2 / 2 + ...
             # cosh(scale * L) ~= Id + (scale * L) ** 2 / 2 + ...
             if nmax > 1:
@@ -101,15 +109,15 @@
                     np.copyto(Lx, L @ x[:, b])
                     for i in range(1, nmax):
                         if (i % 2) == 0:
                             np.add(y[:, b], np.multiply(0.5 * (pfactor + mfactor), Lx), out=y[:, b])
                         pfactor *= scale / (i + 1)
                         mfactor *= -scale / (i + 1)
                         np.copyto(Lx, L @ Lx)
-            return y
+            return y.ravel() if is_1d else y
         return LazyLinOp(
             shape=L.shape,
             matmat=lambda X: _matmat(L, X),
             rmatmat=lambda X: _matmat(L.T.conj(), X)
         )
     else:
         raise ValueError("backend value is either 'scipy' or 'serie'.")
```

## lazylinop/wip/linalg/expm.py

```diff
@@ -58,23 +58,28 @@
         if islazylinop(L):#type(L) is np.ndarray:
             raise ValueError("If L is a 2d array, backend must be 'scipy'.")
         if use_numba:
             M = sp.linalg.expm(scale * L)
             nb.config.DISABLE_JIT = 0 if use_numba else 1
             @njit(nopython=True, parallel=True, cache=True)
             def _matmat(M, x):
-                # x is always 2d
+                if x.ndim == 1:
+                    is_1d = True
+                    batch_size = 1
+                    x = x.reshape(x.shape[0], 1)
+                else:
+                    is_1d = False
                 batch_size = x.shape[1]
                 if use_numba:
                     y = np.empty((M.shape[0], batch_size), dtype=x.dtype)
                     for b in prange(batch_size):
                         y[:, b] = M @ X[:, b]
                 else:
                     y = M @ X
-                return y
+                return y.ravel() if is_1d else y
             return LazyLinOp(
                 shape=L.shape,
                 matmat=lambda X: _matmat(M, X),
                 rmatmat=lambda X: _matmat(M.T.conj(), X)
             )
         else:
             return LazyLinOp(
```

## lazylinop/wip/linalg/khatri_rao.py

```diff
@@ -60,19 +60,18 @@
         raise ValueError("number of columns differs.")
 
     shape = (Ma * Mb, Na) if column else (Ma, Na * Nb)
 
     # Compute number of operations for lazylinop (B @ diag(x) @ A.T)
     # and for SciPy and return the best method
     def _nops(A, B, x):
-        # x is always 2d
         m, k = B.shape
         k, n = x.shape[0], x.shape[0]
         n, p = A.T.shape
-        batch_size = x.shape[1]
+        batch_size = 1 if x.ndim == 1 else x.shape[1]
         # # Left to right or right to left multiplication ?
         # ltor = (m * k * n + m * n * p + k + m * p) * batch_size + k ** 2
         # rtol = (m * k * p + k * n * p + k + m * p) * batch_size + k ** 2
         # # SciPy computes the Khatri-Rao matrix K and then computes K @ X
         # nops = A.shape[0] * B.shape[0] * A.shape[1] * (1 + batch_size)
         # print(nops / max(ltor, rtol), 'lazylinop' if nops >  max(ltor, rtol) else 'scipy')
         # return 'lazylinop' if nops > min(ltor, rtol) else 'scipy'
@@ -81,19 +80,23 @@
         # print('batch size={0:d} {1:s}'.format(batch_size, 'lazylinop' if (A.shape[0] * B.shape[0] * A.shape[1]) > ((k ** 2 + m * p) * batch_size) else 'scipy'))
         return 'lazylinop' if (A.shape[0] * B.shape[0] * A.shape[1]) > ((k ** 2 + m * p) * batch_size) else 'scipy'
 
     # Because NumPy/SciPy uses parallel computation of the @
     # there is no reasons to define a matvec and run batch of
     # matvec in parallel as matmat.
     def _matmat(A, B, x, column):
-        # x is always 2d
         Ma, Na = A.shape[0], A.shape[1]
         Mb, Nb = B.shape[0], B.shape[1]
         if islazylinop(x):
             x = np.eye(x.shape[0], M=x.shape[0], k=0) @ x
+        if x.ndim == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
         batch_size = x.shape[1]
         Y = np.full((Ma * Mb if column else Ma, batch_size), 0.0 * (A[0, 0] * B[0, 0] * x[0, 0]))
         if column:
             # We use (A c* B) @ x = vec(B @ diag(x) @ A^T)
             # and a ravel with order='F' (does not work with Numba)
             for i in range(batch_size):
                 m, k = B.shape
@@ -111,15 +114,15 @@
                     Y[:, i] = ((B @ D) @ A.T).ravel(order='F')
                 else:
                     Y[:, i] = (B @ (D @ A.T)).ravel(order='F')
         else:
             for i in range(batch_size):
                 for r in range(Ma):
                     Y[r, i] = A[r, :] @ (B[r, :] @ x[:, i].reshape(A.shape[1], B.shape[1]).T).T
-        return Y
+        return Y.ravel() if is_1d else Y
         
     # We use (A c* B)^T = A^T r* B^T to compute the adjoint.
     return LazyLinOp(
         shape=shape,
         matmat=lambda x: sp.linalg.khatri_rao(
             np.eye(A.shape[0], M=A.shape[0], k=0) @ A if islazylinop(A) else A,
             np.eye(B.shape[0], M=B.shape[0], k=0) @ B if islazylinop(B) else B
```

## lazylinop/wip/linalg/logm.py

```diff
@@ -50,23 +50,28 @@
         if islazylinop(L):
             L = np.eye(L.shape[0], M=L.shape[0]) @ L
         if use_numba:
             M = sp.linalg.logm(scale * L)
             nb.config.DISABLE_JIT = 0 if use_numba else 1
             @njit(nopython=True, parallel=True, cache=True)
             def _matmat(M, x):
-                # x is always 2d
+                if x.ndim == 1:
+                    is_1d = True
+                    batch_size = 1
+                    x = x.reshape(x.shape[0], 1)
+                else:
+                    is_1d = False
                 batch_size = x.shape[1]
                 if use_numba:
                     y = np.empty((M.shape[0], batch_size), dtype=x.dtype)
                     for b in prange(batch_size):
                         y[:, b] = M @ X[:, b]
                 else:
                     y = M @ X
-                return y
+                return y.ravel() if is_1d else y
             return LazyLinOp(
                 shape=L.shape,
                 matmat=lambda X: _matmat(M, X),
                 rmatmat=lambda X: _matmat(M.T.conj(), X)
             )
         else:
             return LazyLinOp(
@@ -74,32 +79,37 @@
                 matmat=lambda X: sp.linalg.logm(scale * L) @ X,
                 rmatmat=lambda X: sp.linalg.logm(scale * L.T.conj()) @ X
             )
     elif backend == 'serie':
         if nmax < 1:
             raise ValueError("nmax must be >= 1.")
         def _matmat(L, x):
-            # x is always 2d
             if L.shape[1] != x.shape[0]:
                 raise ValueError("L @ x does not work because # of columns of L is not equal to the # of rows of x.")
+            if x.ndim == 1:
+                is_1d = True
+                batch_size = 1
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             batch_size = x.shape[1]
             Lx = np.empty(L.shape[0], dtype=x.dtype)
             # Taylor expansion
             # It uses the equation log(scale * L) ~= sum((-1)^(n + 1) * (scale * L - Id)^n / n, n=1 to nmax)
             y = np.subtract(np.multiply(scale, L @ x), x)
             if nmax > 2:
                 # loop over the batch size
                 for b in range(batch_size):
                     # compute (scale * L - Id) @ x
                     np.subtract(np.multiply(scale, L @ x[:, b]), x[:, b], out=Lx)
                     for n in range(2, nmax):
                         factor = (2 * (n % 2) - 1) / n
                         np.add(y[:, b], np.multiply(factor, Lx), out=y[:, b])
                         np.subtract(np.multiply(scale, L @ Lx), Lx, out=Lx)
-            return y
+            return y.ravel() if is_1d else y
         return LazyLinOp(
             shape=L.shape,
             matmat=lambda X: _matmat(L, X),
             rmatmat=lambda X: _matmat(L.T.conj(), X)
         )
     else:
         raise ValueError("backend value is either 'scipy' or 'serie'.")
```

## lazylinop/wip/linalg/sinhm.py

```diff
@@ -53,39 +53,47 @@
         if islazylinop(L):
             L = np.eye(L.shape[0], M=L.shape[0]) @ L
         if use_numba:
             S = sp.linalg.sinhm(scale * L) @ X
             nb.config.DISABLE_JIT = 0 if use_numba else 1
             @njit(nopython=True, parallel=True, cache=True)
             def _matmat(S, x):
-                # x is always 2d
+                if x.ndim == 1:
+                    is_1d = True
+                    x = x.reshape(x.shape[0], 1)
+                else:
+                    is_1d = False
                 batch_size = x.shape[1]
                 if use_numba:
                     y = np.empty((S.shape[0], batch_size), dtype=x.dtype)
                     for b in prange(batch_size):
                         y[:, b] = S @ X[:, b]
                 else:
                     y = S @ X
-                return y
+                return y.ravel() if is_1d else y
             return LazyLinOp(
                 shape=L.shape,
                 matmat=lambda X: _matmat(S, X),
                 rmatmat=lambda X: _matmat(S.T.conj(), X)
             )
         else:
             return LazyLinOp(
                 shape=L.shape,
                 matmat=lambda X: sp.linalg.sinhm(scale * L) @ X,
                 rmatmat=lambda X: sp.linalg.sinhm(scale * L.T.conj()) @ X
             )
     elif backend == 'serie':
         def _matmat(L, x):
-            # x is always 2d
             if L.shape[1] != x.shape[0]:
                 raise ValueError("L @ x does not work because # of columns of L is not equal to the # of rows of x.")
+            if x.ndim == 1:
+                is_1d = True
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             batch_size = x.shape[1]
             y = np.zeros((L.shape[0], batch_size), dtype=x.dtype)
             Lx = np.empty(L.shape[0], dtype=x.dtype)
             # loop over the batch size
             for b in range(batch_size):
                 # Taylor expansion
                 # exp(scale * L) ~= Id + scale * L + (scale * L) ** 2 / 2 + ...
@@ -97,15 +105,15 @@
                     np.copyto(Lx, L @ x[:, b])
                     for i in range(1, nmax):
                         if (i % 2) == 1:
                             np.add(y[:, b], np.multiply(0.5 * (pfactor - mfactor), Lx), out=y[:, b])
                         pfactor *= scale / (i + 1)
                         mfactor *= -scale / (i + 1)
                         np.copyto(Lx, L @ Lx)
-            return y
+            return y.ravel() if is_1d else y
         return LazyLinOp(
             shape=L.shape,
             matmat=lambda X: _matmat(L, X),
             rmatmat=lambda X: _matmat(L.T.conj(), X)
         )
     else:
         raise ValueError("backend value is either 'scipy' or 'serie'.")
```

## lazylinop/wip/linalg/sqrtm.py

```diff
@@ -58,17 +58,22 @@
         return LazyLinOp(
             shape=L.shape,
             matmat=lambda X: R @ X,
             rmatmat=lambda X: R.T.conj() @ X
         )
     elif backend == 'serie':
         def _matmat(L, x):
-            # x is always 2d
             if L.shape[1] != x.shape[0]:
                 raise ValueError("L @ x does not work because # of columns of L is not equal to the # of rows of x.")
+            if x.ndim == 1:
+                is_1d = True
+                batch_size = 1
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             batch_size = x.shape[1]
             Lx = np.empty(L.shape[0], dtype=x.dtype)
             # Taylor expansion
             # It uses the equation (scale * L)^1/2 = sum((-1)^n * (1/2 n) * (Id - scale * L)^n, n=0 to inf)
             y = np.copy(x)
             if nmax > 1:
                 # loop over the batch size
@@ -76,15 +81,15 @@
                     # compute (Id - scale * L) @ x
                     np.subtract(x[:, b], np.multiply(scale, L @ x[:, b]), out=Lx)
                     for n in range(1, nmax):
                         # factor = (1 - 2 * (n % 2)) * sp.special.comb(0.5, n)
                         factor = (1 - 2 * (n % 2)) * sp.special.binom(0.5, n)
                         np.add(y[:, b], np.multiply(factor, Lx), out=y[:, b])
                         np.subtract(Lx, np.multiply(scale, L @ Lx), out=Lx)
-            return y
+            return y.ravel() if is_1d else y
         return LazyLinOp(
             shape=L.shape,
             matmat=lambda X: _matmat(L, X),
             rmatmat=lambda X: _matmat(L.T.conj(), X)
         )
     else:
         raise ValueError("backend value is either 'scipy' or 'serie'.")
```

## lazylinop/wip/signal/anti_eye.py

```diff
@@ -86,31 +86,35 @@
 
     if k >= nn or k <= - m:
         # diagonal is out of shape
         # return zeros LazyLinOp
         return zeros((m, nn))
 
     def _matmat(x, m, n, k):
-        # x is always 2d
         out_dtype = binary_dtype(dtype, x.dtype)
+        if len(x.shape) == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
         y = np.zeros((m, x.shape[1]), dtype=x.dtype)
         if k == 0:
             # Main anti-diagonal
             nr = min(m, n)
             y[:nr, :] = x[-1:- 1 - nr:-1]
         elif k > 0:
             # Above anti-diagonal
             # k <= n
             nr = max(0, min(m, n - k))
             y[:nr] = x[-1 - k:-1 - k - nr:-1]
         else:
             # Below anti-diagonal (k < 0)
             nr = max(0, min(m + k, n))
             y[-k:-k+nr] = x[-1:- 1 - nr:-1]
-        return y.astype(out_dtype)
+        return (y.ravel() if is_1d else y).astype(out_dtype)
 
     return LazyLinOp(
         shape=(m, nn),
         matmat=lambda x: _matmat(x, m, nn, k),
         rmatmat=lambda x: _matmat(x, nn, m, k),
         dtype=dtype
     )
```

## lazylinop/wip/signal/bc.py

```diff
@@ -1,107 +1,91 @@
+from lazylinop import LazyLinOp
 import sys
 from lazylinop.basicops import eye, vstack
 from lazylinop.wip.signal import anti_eye
 
 sys.setrecursionlimit(100000)
 
 
 def bc(L: int, n: int = 1, bn: int = 0,
        an: int = 0, boundary: str = 'periodic'):
-    r"""Builds a periodic or symmetric boundary condition :class:`.LazyLinOp`.
-
-    For an input $x_1, x_2, \ldots, x_N$:
-
-    - A symmetric boundary condition is such that:
-
-      $x_N, ..., x_2, x_1 | x_1, x_2, ..., x_N | x_N, ..., x_2, x_1$
-
-    - A periodic boundary condition is such that:
-
-      $x_1, x_2, ..., x_N | x_1, x_2, ..., x_N | x_1, x_2, ..., x_N$
-
-    If the input is a 2d array, it  works on each column and returns the
-    resulting horizontal concatenation (a 2d-array).
+    """Constructs a periodic or symmetric boundary
+    condition lazy linear operator.
+    If you apply the operator to a 2d array, it will work
+    on each column and returns a 2d array.
+    Symmetric boundary condition is something like:
+    xN, ..., x2, x1 | x1, x2, ..., xN | xN, ..., x2, x1
+    while a periodic boundary condition is something like:
+    x1, x2, ..., xN | x1, x2, ..., xN | x1, x2, ..., xN
 
     Args:
-        L: ``int``
-            Size of the input.
-        n: ``int``, optional
-            Duplicate the signal this number of times on both side.
-        bn: ``int``, optional
-            Add this number of elements before.
-            It first adds n times the input to the left of the
-            original input and then adds bn elements.
-        an: ``int``, optional
-            Add this number of elements after.
-            It first adds n times the input to the right of the
-            original input and then adds an elements.
-        boundary: ``str``, optional
-            ``'wrap'``/``'periodic'`` (default) or ``'symm'``/``'symmetric'``
-            boundary condition.
+        L: int
+        Size of the input
+        n: int, optional
+        Duplicate signal this number of times on both side
+        bn: int, optional
+        Add this number of elements before
+        an: int, optional
+        Add this number of elements after
+        boundary: str, optional
+        wrap/periodic (default) or symm/symmetric boundary condition
 
     Returns:
-        :class:`.LazyLinOp`
+        LazyLinOp
 
     Raises:
         ValueError
             L must be strictly positive.
         ValueError
             n must be >= 0.
         ValueError
-            bn and an must be >= 0.
+            an and bn must be >= 0.
         ValueError
-            bn and an must be <= L.
+            an must be <= L.
         ValueError
-            boundary is either 'periodic' ('wrap') or 'symmetric' ('symm').
+            bn must be <= L.
+        ValueError
+            boundary excepts 'wrap', 'periodic', 'symm' or 'symmetric'.
 
     Examples:
-        >>> import lazylinop as lz
+        >>> from lazylinop.wip.signal import bc
         >>> import numpy as np
-        >>> N = 3
-        >>> x = np.arange(N).astype(np.float_)
-        >>> L = lz.wip.signal.bc(N, n=1, boundary='periodic')
-        >>> L @ x
+        >>> x = np.array([0., 1., 2.])
+        >>> Op = bc(x.shape[0], n=1, bn=0, an=0, boundary='periodic')
+        >>> Op @ x
         array([0., 1., 2., 0., 1., 2., 0., 1., 2.])
-        >>> L = lz.wip.signal.bc(N, n=1, boundary='symmetric')
-        >>> L @ x
+        >>> Op = bc(x.shape[0], n=1, bn=0, an=0, boundary='symmetric')
+        >>> Op @ x
         array([2., 1., 0., 0., 1., 2., 2., 1., 0.])
         >>> X = np.array([[0., 0.], [1., 1.], [2., 2.]])
         >>> X
         array([[0., 0.],
                [1., 1.],
                [2., 2.]])
-        >>> L @ X
+        >>> Op @ X
         array([[2., 2.],
                [1., 1.],
                [0., 0.],
                [0., 0.],
                [1., 1.],
                [2., 2.],
                [2., 2.],
                [1., 1.],
                [0., 0.]])
-        >>> L = lz.wip.signal.bc(N, n=1, bn=1, boundary='periodic')
-        >>> L @ x
-        array([2., 0., 1., 2., 0., 1., 2., 0., 1., 2.])
-        >>> L = lz.wip.signal.bc(N, n=1, bn=2, an=1, boundary='symmetric')
-        >>> L @ x
-        array([1., 2., 2., 1., 0., 0., 1., 2., 2., 1., 0., 0.])
-
-    .. seealso::
-        :func:`.vstack`, :func:`.hstack`, :func:`.bc2d`
     """
     if L <= 0:
         raise ValueError("L must be strictly positive.")
     if n < 0:
         raise ValueError("n must be >= 0.")
     if bn < 0 or an < 0:
         raise ValueError("an and bn must be >= 0.")
-    if bn > L or an > L:
-        raise ValueError("bn and an must be <= L.")
+    if bn > L:
+        raise ValueError("bn must be <= L.")
+    if an > L:
+        raise ValueError("an must be <= L.")
 
     if boundary == 'symmetric' or boundary == 'symm':
         if (n % 2) == 0:
             Op = eye(L, n=L, k=0)
             flip = True
         else:
             Op = anti_eye(L) @ eye(L, n=L, k=0)
@@ -135,13 +119,13 @@
         if bn > 0:
             Op = vstack((eye(bn, n=L, k=L - bn), Op))
         if an > 0:
             Op = vstack((Op, eye(an, n=L, k=0)))
         return Op
     else:
         raise ValueError("boundary is either 'periodic' ('wrap')" +
-                         " or 'symmetric' ('symm').")
+                         " or 'symmetric' (symm).")
 
 
 # if __name__ == '__main__':
 #     import doctest
 #     doctest.testmod()
```

## lazylinop/wip/signal/convolve.py

```diff
@@ -300,15 +300,19 @@
             rmatmat=lambda x: (
                 _rmatvec(x, in2) if x.ndim == 1
                 else _rmatmat(x, in2)
             )
         )
     elif compute == 'scipy encapsulation':
         def _matmat(x):
-            # x is always 2d
+            if x.ndim == 1:
+                is_1d = True
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             batch_size = x.shape[1]
             if in2.shape[0] == 2 and batch_size >= 100:
                 # Because kernel length is 2 override compute
                 # and use the following dedicated function.
                 return _conv_filter2(in1, in2, mode=mode) @ x
             elif in2.shape[0] == 3 and batch_size >= 100:
                 # Because kernel length is 3 override compute
@@ -330,30 +334,34 @@
                 y = np.empty((dim[mode], batch_size),
                              dtype=(x[0, 0] * in2[0]).dtype)
                 # Use Dask ?
                 for b in range(batch_size):
                     y[:, b] = sp.signal.convolve(x[:, b],
                                                  in2, mode=mode,
                                                  method='auto')
-            return y
+            return y.ravel() if is_1d else y
 
         def _rmatmat(x):
-            # x is always 2d
+            if x.ndim == 1:
+                is_1d = True
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             batch_size = x.shape[1]
             y = np.empty((dim['same'], batch_size),
                          dtype=(x[0, 0] * in2[0]).dtype)
             # Use Dask ?
             for b in range(batch_size):
                 y[:, b] = np.flip(
                     sp.signal.convolve(np.flip(x[:, b]),
                                        in2,
                                        mode=rmode[mode],
                                        method='auto')
                 )
-            return y
+            return y.ravel() if is_1d else y
         C = LazyLinOp(
             shape=(dim[mode], dim['same']),
             matmat=lambda x: _matmat(x),
             rmatmat=lambda x: _rmatmat(x),
             dtype=in2.dtype
         )
     elif compute == 'pyfaust toeplitz':
@@ -446,48 +454,56 @@
         L = N
     else:
         pass
     start = (N + K - 1 - L) // 2
     end = start + L
 
     def _matmat(x):
-        # x is always 2d
+        if x.ndim == 1:
+            x = x.reshape(x.shape[0], 1)
+            is_1d = True
+        else:
+            is_1d = False
         batch_size = x.shape[1]
         if mode == 'full':
             y = np.zeros((L, batch_size),
                          dtype=binary_dtype(x.dtype, in2.dtype))
             y[:N, :] = in2[0] * x[:N, :]
             y[1:(N + 1), :] += in2[1] * x[:N, :]
         elif mode == 'valid':
             y = np.zeros((L, batch_size),
                          dtype=binary_dtype(x.dtype, in2.dtype))
             y[:L, :] = in2[1] * x[:L, :]
             y[:L, :] += in2[0] * x[1:(1 + L), :]
         elif mode == 'same':
             y = in2[0] * x[:N, :]
             y[1:N] += in2[1] * x[:(N - 1), :]
-        return y
+        return y.ravel() if is_1d else y
 
     def _rmatmat(x):
-        # x is always 2d
+        if x.ndim == 1:
+            x = x.reshape(x.shape[0], 1)
+            is_1d = True
+        else:
+            is_1d = False
         batch_size = x.shape[1]
         if mode == 'full':
             y = np.zeros((N, batch_size),
                          dtype=binary_dtype(x.dtype, in2.dtype))
             y[:N, :] = in2[0] * x[:N, :]
             y[:N, :] += in2[1] * x[1:min(L + 1, N + 1), :]
         elif mode == 'valid':
             y = np.zeros((N, batch_size),
                          dtype=binary_dtype(x.dtype, in2.dtype))
             y[:L, :] = in2[1] * x[:L, :]
             y[1:(1 + L), :] += in2[0] * x[:L, :]
         elif mode == 'same':
             y = in2[0] * x[:N, :]
             y[:(N - 1), :] += in2[1] * x[1:N, :]
-        return y
+        return y.ravel() if is_1d else y
 
     return LazyLinOp(
         shape=(L, N),
         matmat=lambda x: _matmat(x),
         rmatmat=lambda x: _rmatmat(x)
     )
 
@@ -517,15 +533,19 @@
         L = N
     else:
         pass
     start = (N + K - 1 - L) // 2
     end = start + L
 
     def _matmat(x):
-        # x is always 2d
+        if x.ndim == 1:
+            x = x.reshape(x.shape[0], 1)
+            is_1d = True
+        else:
+            is_1d = False
         batch_size = x.shape[1]
         if mode == 'full':
             y = np.zeros((L, batch_size),
                          dtype=binary_dtype(x.dtype, in2.dtype))
             y[:N, :] = in2[0] * x[:N, :]
             y[1:(N + 1), :] += in2[1] * x[:N, :]
             y[2:(N + 2), :] += in2[2] * x[:N, :]
@@ -535,18 +555,22 @@
             y[:L, :] = in2[2] * x[:L, :]
             y[:L, :] += in2[1] * x[1:(1 + L), :]
             y[:L, :] += in2[0] * x[2:(2 + L), :]
         elif mode == 'same':
             y = in2[1] * x[:N, :]
             y[1:N] += in2[2] * x[:(N - 1), :]
             y[:(N - 1)] += in2[0] * x[1:N, :]
-        return y
+        return y.ravel() if is_1d else y
 
     def _rmatmat(x):
-        # x is always 2d
+        if x.ndim == 1:
+            x = x.reshape(x.shape[0], 1)
+            is_1d = True
+        else:
+            is_1d = False
         batch_size = x.shape[1]
         if mode == 'full':
             y = np.zeros((N, batch_size),
                          dtype=binary_dtype(x.dtype, in2.dtype))
             y[:N, :] = in2[0] * x[:N, :]
             y[:N, :] += in2[1] * x[1:min(L + 1, N + 1), :]
             y[:N, :] += in2[2] * x[2:min(L + 2, N + 2), :]
@@ -556,15 +580,15 @@
             y[:L, :] = in2[2] * x[:L, :]
             y[1:(1 + L), :] += in2[1] * x[:L, :]
             y[2:(2 + L), :] += in2[0] * x[:L, :]
         elif mode == 'same':
             y = in2[1] * x[:N, :]
             y[:(N - 1), :] += in2[2] * x[1:N, :]
             y[1:N, :] += in2[0] * x[:(N - 1), :]
-        return y
+        return y.ravel() if is_1d else y
 
     return LazyLinOp(
         shape=(L, N),
         matmat=lambda x: _matmat(x),
         rmatmat=lambda x: _rmatmat(x)
     )
 
@@ -681,23 +705,27 @@
     indices = np.arange(start, start + extract, 1)
     # use eye operator to extract
     C = eye(extract, n=C.shape[0], k=start) @ C
 
     iscomplex = 'complex' in str(in2.dtype)
 
     def _batch(op, x):
-        # x is always 2d
+        if x.ndim == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
         batch_size = x.shape[1]
         y = np.empty((op.shape[0], batch_size),
                      dtype=binary_dtype(op.dtype, x.dtype))
         # print(op.dtype, x.dtype, y.dtype)
         # use Dask ?
         for b in range(batch_size):
             y[:, b] = op @ x[:, b]
-        return y
+        return y.ravel() if is_1d else y
 
     return LazyLinOp(
         shape=(C.shape[0], in1),
         matmat=lambda x: _batch(C, x) if (
             iscomplex or 'complex' in str(x.dtype)
         )
         else np.real(_batch(C, x)),
```

## lazylinop/wip/signal/convolve2d.py

```diff
@@ -177,36 +177,44 @@
         s2 = i2 + Y
     else:
         raise ValueError("mode is either 'full' (default), 'valid' or 'same'.")
 
     if compute == 'scipy encapsulation':
         # correlate2d is the adjoint operator of convolve2d
         def _matmat(x):
-            # x is always 2d
+            if x.ndim == 1:
+                is_1d = True
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             batch_size = x.shape[1]
             # use Dask ?
             y = np.empty((xdim[mode] * ydim[mode], batch_size),
                          dtype=(x[0, 0] * in2[0, 0]).dtype)
             for b in range(batch_size):
                 y[:, b] = sp.signal.convolve2d(
                     x[:, b].reshape(xdim['same'], ydim['same']),
                     in2, mode=mode, boundary=boundary).ravel()
-            return y
+            return y.ravel() if is_1d else y
 
         def _rmatmat(x):
-            # x is always 2d
+            if x.ndim == 1:
+                is_1d = True
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             batch_size = x.shape[1]
             # use Dask ?
             y = np.empty((xdim['same'] * ydim['same'], batch_size),
                          dtype=(x[0, 0] * in2[0, 0]).dtype)
             for b in range(batch_size):
                 y[:, b] = sp.signal.correlate2d(
                     x[:, b].reshape(xdim[mode], ydim[mode]),
                     in2, mode=rmode[mode], boundary=boundary).ravel()
-            return y
+            return y.ravel() if is_1d else y
         C = LazyLinOp(
             shape=(xdim[mode] * ydim[mode], xdim['same'] * ydim['same']),
             matmat=lambda x: _matmat(x),
             rmatmat=lambda x: _rmatmat(x)
         )
     elif compute == 'scipy fft' or compute == 'auto':
         from lazylinop.wip.signal import fft2
```

## lazylinop/wip/signal/dct.py

```diff
@@ -271,23 +271,41 @@
 
     if (not np.isscalar(type) or type - np.floor(type) != 0
        or type < 1 or type > 4):
         raise ValueError("type must be either 1, 2, 3 or 4.")
 
     def _matmat(x):
         _dtype_sanitized_x(x, 'dct')
-        # x is always 2d
-        return sp.fft.dctn(x, type, (None), 0, norm,
-                           False, n_workers, orthogonalize=orthogonalize)
+        if len(x.shape) == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
+        if is_1d:
+            y = sp.fft.dct(x, type, None, 0, norm, False,
+                           n_workers, orthogonalize)
+        else:
+            y = sp.fft.dctn(x, type, (None), 0, norm,
+                            False, n_workers, orthogonalize=orthogonalize)
+        return y.ravel() if is_1d else y
 
     def _rmatmat(x):
         _dtype_sanitized_x(x, 'dct')
-        # x is always 2d
-        return sp.fft.idctn(x, type, (None), 0, norm, False,
-                            n_workers, orthogonalize=orthogonalize) * scale
+        if len(x.shape) == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
+        if is_1d:
+            y = sp.fft.idct(x, type, None, 0, norm, False,
+                            n_workers, orthogonalize) * scale
+        else:
+            y = sp.fft.idctn(x, type, (None), 0, norm, False,
+                             n_workers, orthogonalize=orthogonalize) * scale
+        return y.ravel() if is_1d else y
 
     L = LazyLinOp(
         shape=(M, M),
         matmat=lambda x: _matmat(x),
         rmatmat=lambda x: _rmatmat(x),
         dtype='float'
     )
```

## lazylinop/wip/signal/decimate.py

```diff
@@ -71,24 +71,31 @@
     if end is not None:
         if end > N:
             raise ValueError("end is > number of elements along axis.")
         if end <= start:
             raise Exception("end is <= start.")
 
     def _matmat(x, start, end, every):
-        # x is always 2d
         L = int(np.ceil((end - start) / every))
-        return x[start + np.arange(L) * every, :]
+        if x.ndim == 1:
+            return x[start + np.arange(L) * every]
+        else:
+            return x[start + np.arange(L) * every, :]
 
     def _rmatmat(x, start, end, every):
-        # x is always 2d
-        y = np.zeros((end, x.shape[1]), dtype=x.dtype)
-        indices = np.arange(x.shape[0])
-        y[start + indices * every, :] = x[indices, :]
-        return y
+        if x.ndim == 1:
+            y = np.zeros(end, dtype=x.dtype)
+            indices = np.arange(x.shape[0])
+            y[start + indices * every] = x[indices]
+            return y
+        else:
+            y = np.zeros((end, x.shape[1]), dtype=x.dtype)
+            indices = np.arange(x.shape[0])
+            y[start + indices * every, :] = x[indices, :]
+            return y
 
     last = N if end is None else end
     L = int(np.ceil((last - start) / every))
     return LazyLinOp(
         shape=(L, N),
         matmat=lambda x: _matmat(x, start, last, every),
         rmatmat=lambda x: _rmatmat(x, start, last, every)
```

## lazylinop/wip/signal/ds_mconv.py

```diff
@@ -159,16 +159,15 @@
     if L <= 0:
         raise Exception(
             "mode and offset values every are incompatibles"
             + " with kernel and signal sizes."
         )
 
     def _matmat(x, in2, in3, _T):
-        # x is always 2d
-        batch_size = x.shape[1]
+        batch_size = 1 if x.ndim == 1 else x.shape[1]
         perT = int(np.ceil((dims[0] - start) / _T))
         perT += perT % every
         use_parallel = bool((perT * K * batch_size) > 1000)
 
         # Because of Numba split 1d and 2d
         @njit(parallel=use_parallel, cache=True)
         def _1d(x, in2, in3):
@@ -204,19 +203,18 @@
                                 in2[j] * x[i - j, b]
                             )
                             y[L + (i - start) // every, b] += (
                                 in3[j] * x[i - j, b]
                             )
             return y
 
-        return _1d(x.ravel(), in2, in3).reshape(-1, 1) if x.shape[1] == 1 else _2d(x, in2, in3)
+        return _1d(x, in2, in3) if x.ndim == 1 else _2d(x, in2, in3)
 
     def _rmatmat(x, in2, in3, T):
-        # x is always 2d
-        batch_size = x.shape[1]
+        batch_size = 1 if x.ndim == 1 else x.shape[1]
         rperT = int(np.ceil(dims[2] / T))
         use_rparallel = bool((rperT * K * batch_size) > 1000)
 
         # Because of Numba split 1d and 2d
         @njit(parallel=use_rparallel, cache=True)
         def _1d(x, in2, in3):
             a = 0 if imode == 0 and offset == 0 else 1
@@ -264,15 +262,15 @@
                         if k < K:
                             # NumPy uses row-major format
                             for b in range(batch_size):
                                 y[i, b] += in2[k] * x[j, b]
                                 y[i, b] += in3[k] * x[L + j, b]
             return y
 
-        return _1d(x.ravel(), in2, in3).reshape(-1, 1) if x.shape[1] == 1 else _2d(x, in2, in3)
+        return _1d(x, in2, in3) if x.ndim == 1 else _2d(x, in2, in3)
 
     return LazyLinOp(
         shape=(2 * L, dims[2]),
         matmat=lambda x: _matmat(x, in2, in3, T),
         rmatmat=lambda x: _rmatmat(x, in2, in3, T),
         dtype=in2.dtype,
     )
```

## lazylinop/wip/signal/dsconvolve.py

```diff
@@ -130,16 +130,16 @@
     end = min(dims[0], start + dims[imode] - offset)
     L = int(np.ceil((dims[imode] - offset) / every))
     if L <= 0:
         raise Exception("mode, offset and every are incompatibles" +
                         " with kernel and signal sizes.")
 
     def _matmat(x, kernel, T):
-        # x is always 2d
-        batch_size = x.shape[1]
+
+        batch_size = 1 if x.ndim == 1 else x.shape[1]
         perT = int(np.ceil((dims[0] - start) / T))
         use_parallel_1d = bool((perT * K) > 100000)
         use_parallel_2d = bool((perT * K * batch_size) > 100000)
 
         # Because of Numba split 1d and 2d
         @njit(parallel=use_parallel_1d, cache=True)
         def _1d(x, kernel):
@@ -174,19 +174,19 @@
                         for j in range(max(0, i - in1 + 1), min(K, i + 1)):
                             # NumPy uses row-major format
                             for b in range(batch_size):
                                 y[(i - start) // every,
                                   b] += kernel[j] * x[i - j, b]
             return y
 
-        return _1d(x.ravel(), kernel).reshape(-1, 1) if x.shape[1] == 1 else _2d(x, kernel)
+        return _1d(x, kernel) if x.ndim == 1 else _2d(x, kernel)
 
     def _rmatmat(x, kernel, T):
-        # x is always 2d
-        batch_size = x.shape[1]
+
+        batch_size = 1 if x.ndim == 1 else x.shape[1]
         rperT = int(np.ceil(dims[2] / T))
         use_rparallel_1d = bool((rperT * K) > 100000)
         use_rparallel_2d = bool((rperT * K * batch_size) > 100000)
 
         # Because of Numba split 1d and 2d
         @njit(parallel=use_rparallel_1d, cache=True)
         def _1d(x, kernel):
@@ -244,15 +244,15 @@
                             pass
                         if k < K:
                             # NumPy uses row-major format
                             for b in range(batch_size):
                                 y[i, b] += kernel[k] * x[j, b]
             return y
 
-        return _1d(x.ravel(), kernel).reshape(-1, 1) if x.shape[1] == 1 else _2d(x, kernel)
+        return _1d(x, kernel) if x.ndim == 1 else _2d(x, kernel)
 
     return LazyLinOp(
         shape=(L, dims[2]),
         matmat=lambda x: _matmat(x, in2, T),
         rmatmat=lambda x: _rmatmat(x, in2, T),
         dtype=in2.dtype
     )
```

## lazylinop/wip/signal/dst.py

```diff
@@ -274,24 +274,41 @@
 
     if (not np.isscalar(type) or type - np.floor(type) != 0
        or type < 1 or type > 4):
         raise ValueError("type must be either 1, 2, 3 or 4.")
 
     def _matmat(x):
         _dtype_sanitized_x(x, 'dst')
-        # x is always 2d
-        return sp.fft.dstn(x, type, (None), 0, norm,
-                           False, n_workers, orthogonalize=orthogonalize)
-
+        if len(x.shape) == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
+        if is_1d:
+            y = sp.fft.dst(x, type, None, 0, norm, False,
+                           n_workers, orthogonalize)
+        else:
+            y = sp.fft.dstn(x, type, (None), 0, norm,
+                            False, n_workers, orthogonalize=orthogonalize)
+        return y.ravel() if is_1d else y
 
     def _rmatmat(x):
         _dtype_sanitized_x(x, 'dst')
-        # x is always 2d
-        return sp.fft.idstn(x, type, (None), 0, norm, False,
+        if len(x.shape) == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
+        if is_1d:
+            y = sp.fft.idst(x, type, None, 0, norm, False,
+                            n_workers, orthogonalize) * scale
+        else:
+            y = sp.fft.idstn(x, type, (None), 0, norm, False,
                              n_workers, orthogonalize=orthogonalize) * scale
+        return y.ravel() if is_1d else y
 
     L = LazyLinOp(
         shape=(M, M),
         matmat=lambda x: _matmat(x),
         rmatmat=lambda x: _rmatmat(x),
         dtype='float'
     )
@@ -314,19 +331,23 @@
             List of factors.
 
     Returns:
         LazyLinOp
     """
 
     def _matmat(x):
-        # x is always 2d
+        if len(x.shape) == 1:
+            x = x.reshape(N, 1)
+            is_1d = True
+        else:
+            is_1d = False
         y = np.copy(x)
         for i, v in enumerate(idx):
             y[v, :] *= a[i]
-        return y
+        return y.ravel() if is_1d else y
 
     return LazyLinOp(
         shape=(N, N),
         matmat=lambda x: _matmat(x),
         rmatmat=lambda x: _matmat(x)
     )
```

## lazylinop/wip/signal/dwt1d.py

```diff
@@ -110,35 +110,43 @@
             tmp = pywt.dwt_coeff_len(tmp, W, mode=mode)
             ncoeffs += tmp
         # Number of approximation coefficients
         ncoeffs += tmp
 
         def _matmat(x):
             # Decomposition (return array from coefficients)
-            # x is always 2d
+            if x.ndim == 1:
+                is_1d = True
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             y = pywt.coeffs_to_array(
                 pywt.wavedecn(
                     x, wavelet=wavelet.name,
                     level=level, mode=mode, axes=(0, )
                 ), axes=(0, ))[0]
-            return y
+            return y.ravel() if is_1d else y
 
         def _rmatmat(x):
             # Reconstruction
-            # x is always 2d
+            if x.ndim == 1:
+                is_1d = True
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             # Get slices for further reconstruction
             tmp = np.full((N, x.shape[1]), 1.0)
             slices = pywt.coeffs_to_array(
                 pywt.wavedecn(
                     tmp, wavelet=wavelet.name,
                     level=level, mode=mode, axes=(0, )
                 ), axes=(0, ))[1]
             x = pywt.array_to_coeffs(x, slices, output_format='wavedecn')
             y = pywt.waverecn(x, wavelet=rwavelet, mode=mode, axes=(0, ))
-            return y[:N, :]
+            return y[:N, 0].ravel() if is_1d else y[:N, :]
 
         return LazyLinOp(
             shape=(ncoeffs, N),
             matmat=lambda x: _matmat(x),
             rmatmat=lambda x: _rmatmat(x)
         )
     elif backend == 'lazylinop':
```

## lazylinop/wip/signal/dwt2d.py

```diff
@@ -138,15 +138,19 @@
             ncoeffs += 3 * tmpM * tmpN
         # Number of approximation coefficients
         ncoeffs += tmpM * tmpN
 
         def _matmat(x):
             # Decomposition (return 1d array from coefficients)
             # cAn matrix will be flattened as-well-as the cHs, cVs and cDs.
-            # x is always 2d
+            if x.ndim == 1:
+                is_1d = True
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             batch_size = x.shape[1]
 
             y = np.empty((ncoeffs, batch_size), dtype=x.dtype)
 
             # Loop over a batch of flattened 2d array
             for b in range(batch_size):
                 tmp = pywt.wavedec2(x[:, b].reshape(M, N),
@@ -158,19 +162,23 @@
                     if type(tmp[i]) is tuple:
                         for j in range(len(tmp[i])):
                             tmp_ravel = np.append(tmp_ravel,
                                                   tmp[i][j].flatten())
                     else:
                         tmp_ravel = np.append(tmp_ravel, tmp[i].flatten())
                 y[:, b] = np.array(tmp_ravel)
-            return y
+            return y.ravel() if is_1d else y
 
         def _rmatmat(x):
             # Reconstruction
-            # x is always 2d
+            if x.ndim == 1:
+                is_1d = True
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             batch_size = x.shape[1]
 
             y = np.empty((MN, batch_size), dtype=x.dtype)
 
             # Loop over a batch of flattened 2d array
             for b in range(batch_size):
                 # Compute length of the output of _matmat(x)
@@ -197,15 +205,15 @@
                 A = x[:offset, b].reshape(tmpM, tmpN)
                 coeffs.insert(0, A)
 
                 y[:, b] = pywt.waverec2(coeffs,
                                         wavelet=rwavelet,
                                         mode=mode,
                                         axes=(-2, -1))[:M, :N].ravel()
-            return y
+            return y.ravel() if is_1d else y
 
         return LazyLinOp(
             shape=(ncoeffs, MN),
             matmat=lambda x: _matmat(x),
             rmatmat=lambda x: _rmatmat(x)
         )
     elif backend == 'lazylinop':
```

## lazylinop/wip/signal/fft.py

```diff
@@ -88,40 +88,48 @@
             else:
                 raise ValueError("Invalid norm value for 'scipy' backend.")
         else:
             # default is backward
             norm = L
 
         def _matmat(x):
-            # x is always 2d
+            if x.ndim == 1:
+                is_1d = True
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             batch_size = x.shape[1]
-            if batch_size == 1:
+            if is_1d:
                 y = sp.fft.fft(x, axis=0, **kwargs)
             else:
                 tmp = kwargs['n']
                 kwargs['s'] = (L)
                 kwargs.pop('n', None)
                 y = sp.fft.fftn(x, axes=0, **kwargs)
                 kwargs['n'] = tmp
                 kwargs.pop('s', None)
-            return y
+            return y.ravel() if is_1d else y
 
         def _rmatmat(x):
-            # x is always 2d
+            if x.ndim == 1:
+                is_1d = True
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             batch_size = x.shape[1]
-            if batch_size == 1:
+            if is_1d:
                 y = sp.fft.ifft(x, axis=0, **kwargs) * norm
             else:
                 kwargs['s'] = (L)
                 tmp = kwargs['n']
                 kwargs.pop('n', None)
                 y = sp.fft.ifftn(x, axes=0, **kwargs) * norm
                 kwargs['n'] = tmp
                 kwargs.pop('s', None)
-            return y
+            return y.ravel() if is_1d else y
 
         F = LazyLinOp(
             shape=(L, N),
             matmat=lambda x: _matmat(x),
             rmatmat=lambda x: _rmatmat(x),
             dtype='complex'
         )
@@ -258,15 +266,15 @@
                                 # y[n + k, b] = u[0] + v[0]
                                 y[n + k, b] += v[0]
                                 y[n + k + hstep[0], b] = u[0] - v[0]
                                 iterations[0] += 1
                             wmk[0] *= wm
                 return y
 
-            return _1d(x.ravel(), adjoint).reshape(-1, 1) if x.shape[1] == 1 else _2d(x, adjoint)
+            return _1d(x, adjoint) if x.ndim == 1 else _2d(x, adjoint)
 
         F = LazyLinOp(
             shape=(N, N),
             matmat=lambda x: _matmat(x, False),
             rmatmat=lambda x: _matmat(x, True)
         )
     else:
```

## lazylinop/wip/signal/flip.py

```diff
@@ -72,18 +72,24 @@
         raise ValueError("end is < 1.")
     if end is not None and end > A:
         raise ValueError("end is > number of elements along axis.")
     if end is not None and end <= start:
         raise Exception("end is <= start.")
 
     def _matmat(x, start, end):
-        # x is always 2d
-        y = np.copy(x)
-        y[start:end, :] = x[end - 1 - np.arange(end - start), :]
-        return y
+        if x.ndim == 1:
+            y = np.copy(x.reshape(x.shape[0], 1))
+            x_is_1d = True
+            y[start:end, 0] = x[end - 1 - np.arange(end - start)]
+            return y.ravel()
+        else:
+            y = np.copy(x)
+            x_is_1d = False
+            y[start:end, :] = x[end - 1 - np.arange(end - start), :]
+            return y
 
     return LazyLinOp(
         shape=(N, N),
         matmat=lambda x: _matmat(x, start, N if end is None else end),
         rmatmat=lambda x: _matmat(x, start, N if end is None else end)
     )
```

## lazylinop/wip/signal/fwht.py

```diff
@@ -129,15 +129,14 @@
             for d in range(1, D - 1):
                 Hd = kron(H1, Hd)
             return Hd
     elif new_backend == 'direct' or (new_backend == 'auto'
                                      and N > 32768):
 
         def _matmat(x):
-            # x is always 2d
 
             @njit(cache=True)
             def _1d(x):
                 H = 1
                 D = int(np.floor(np.log2(N)))
                 tmp1, tmp2 = 0.0, 0.0
                 y = np.empty(N, dtype=x.dtype)
@@ -179,15 +178,15 @@
                                 tmp1[0] = y[j, b]
                                 tmp2[0] = y[j + H, b]
                                 y[j, b] = tmp1[0] + tmp2[0]
                                 y[j + H, b] = tmp1[0] - tmp2[0]
                     H *= 2
                 return y
 
-            return _1d(x.ravel()).reshape(-1, 1) if x.shape[1] == 1 else _2d(x)
+            return _1d(x) if x.ndim == 1 else _2d(x)
 
         return LazyLinOp(
             shape=(N, N),
             matmat=lambda x: _matmat(x),
             rmatmat=lambda x: _matmat(x),
             dtype='float'
         )
```

## lazylinop/wip/signal/mslices.py

```diff
@@ -68,34 +68,42 @@
         if start[s] >= shape[0]:
             raise Exception("start must be < shape[0].")
         if end[s] >= shape[0]:
             raise Exception("end must be < shape[0].")
         L += end[s] - start[s] + 1
 
     def _matmat(x, start, end):
-        # x is always 2d
+        if x.ndim == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
         y = np.empty((L, x.shape[1]), dtype=x.dtype)
         offset = 0
         for s in range(S):
             y[offset:(offset + end[s] - start[s] + 1), :] = (
                 x[start[s]:(end[s] + 1), :]
             )
             offset += end[s] - start[s] + 1
-        return y
+        return y.ravel() if is_1d else y
 
     def _rmatmat(x, start, end):
-        # x is always 2d
+        if x.ndim == 1:
+            is_1d = True
+            x = x.reshape(x.shape[0], 1)
+        else:
+            is_1d = False
         y = np.zeros((shape[0], x.shape[1]), dtype=x.dtype)
         offset = 0
         for s in range(S):
             y[start[s]:(end[s] + 1), :] = (
                 x[offset:(offset + end[s] - start[s] + 1), :]
             )
             offset += end[s] - start[s] + 1
-        return y
+        return y.ravel() if is_1d else y
 
     return LazyLinOp(
         shape=(L, shape[0]),
         matmat=lambda x: _matmat(x, start, end),
         rmatmat=lambda x: _rmatmat(x, start, end)
     )
```

## lazylinop/wip/signal/oa.py

```diff
@@ -44,31 +44,38 @@
     if X <= 0:
         raise ValueError("X is strictly positive.")
     if overlap < 0 or overlap > L:
         raise ValueError("overlap must be > 0 and <= L.")
     M = L * X - (X - 1) * overlap
 
     def _matmat(x):
-        # x is always 2d
+        if x.ndim == 1:
+            x_is_1d = True
+            x = np.reshape(x, newshape=(x.size, 1))
+        else:
+            x_is_1d = False
         y = np.full((M, x.shape[1]), 0.0 * x[0, 0], dtype=x.dtype)
         y[:L, :] = x[:L, :]
         offset = L - overlap
         for i in range(X - 1):
             y[offset:(offset + L), :] += x[((i + 1) * L):((i + 2) * L), :]
             offset += L - overlap
-        return y
+        return y.ravel() if x_is_1d else y
 
     def _rmatmat(x):
-        # x is always 2d
+        if x.ndim == 1:
+            x_is_1d = True
+            x = np.reshape(x, newshape=(x.size, 1))
+        else:
+            x_is_1d = False
         y = np.full((X * L, x.shape[1]), 0.0 * x[0, 0], dtype=x.dtype)
         for i in range(X):
             y[(i * L):((i + 1) * L), :] = x[(i * (L - overlap)):
                                             (i * (L - overlap) + L), :]
-        return y
-
+        return y.ravel() if x_is_1d else y
     return LazyLinOp(
         (M, X * L),
         matmat=lambda x: _matmat(x),
         rmatmat=lambda x: _rmatmat(x)
     )
```

## lazylinop/wip/signal/scatter_and_gather_windows.py

```diff
@@ -63,26 +63,34 @@
     if nhop <= 0 or nhop > window:
         raise ValueError("nhop argument expects a value > 0 and <= window.")
 
     # number of windows in the original signal
     nwindows = 1 + (N - window) // nhop
 
     def _matmat(window, nhop, x):
-        # x is always 2d
+        if x.ndim == 1:
+            x = x.reshape(x.shape[0], 1)
+            is_1d = True
+        else:
+            is_1d = False
         Op = eye(window, n=N, k=0)
         for i in range(1, nwindows, 1):
             Op = vstack((Op, eye(window, n=N, k=i * nhop)))
-        return Op @ x
+        return (Op @ x).ravel() if is_1d else Op @ x
 
     def _rmatmat(window, nhop, x):
-        # x is always 2d
+        if x.ndim == 1:
+            x = x.reshape(x.shape[0], 1)
+            is_1d = True
+        else:
+            is_1d = False
         Op = eye(N, n=window, k=0)
         for i in range(1, nwindows, 1):
             Op = hstack((Op, eye(N, n=window, k=-i * nhop)))
-        return Op @ x
+        return (Op @ x).ravel() if is_1d else Op @ x
 
     return LazyLinOp(
         shape=(nwindows * window, N),
         matmat=lambda x: _matmat(window, nhop, x),
         rmatmat=lambda x: _rmatmat(window, nhop, x)
     )
```

## lazylinop/wip/signal/slices.py

```diff
@@ -142,48 +142,62 @@
             if offset[1] is not None:
                 if offset[1] > N:
                     raise ValueError("offset[1] > number of elements.")
                 if offset[1] <= offset[0]:
                     raise Exception("offset[1] is <= offset[0].")
 
             def _matmat(x, a, b, every):
-                # x is always 2d
                 L = int(np.ceil((b - a) / every))
-                return x[a + np.arange(L) * every, :]
+                if x.ndim == 1:
+                    return x[a + np.arange(L) * every]
+                else:
+                    return x[a + np.arange(L) * every, :]
 
             def _rmatmat(x, a, b, every):
-                # x is always 2d
-                y = np.zeros((N, x.shape[1]), dtype=x.dtype)
-                indices = np.arange(x.shape[0])
-                y[a + indices * every, :] = x[indices, :]
+                if x.ndim == 1:
+                    y = np.zeros(N, dtype=x.dtype)
+                    indices = np.arange(x.shape[0])
+                    y[a + indices * every] = x[indices]
+                else:
+                    y = np.zeros((N, x.shape[1]), dtype=x.dtype)
+                    indices = np.arange(x.shape[0])
+                    y[a + indices * every, :] = x[indices, :]
                 return y
 
             M = N if offset[1] is None else offset[1]
             return LazyLinOp(
                 shape=(int(np.ceil((M - offset[0]) / nhop)), N),
                 matmat=lambda x: _matmat(x, offset[0], M, nhop),
                 rmatmat=lambda x: _rmatmat(x, offset[0], M, nhop)
             )
         else:
             # number of windows in the original signal
             nwindows = 1 + (N - window) // nhop
 
             def _matmat(window, nhop, x):
-                # x is always 2d
+                if x.ndim == 1:
+                    x = x.reshape(x.shape[0], 1)
+                    is_1d = True
+                else:
+                    is_1d = False
                 Op = eye(window, n=N, k=0)
                 for i in range(1, nwindows, 1):
                     Op = vstack((Op, eye(window, n=N, k=i * nhop)))
-                return Op @ x
+                return (Op @ x).ravel() if is_1d else Op @ x
 
             def _rmatmat(window, nhop, x):
-                # x is always 2d
+                if x.ndim == 1:
+                    x = x.reshape(x.shape[0], 1)
+                    is_1d = True
+                else:
+                    is_1d = False
                 Op = eye(N, n=window, k=0)
                 for i in range(1, nwindows, 1):
                     Op = hstack((Op, eye(N, n=window, k=-i * nhop)))
-                return Op @ x
+                return (Op @ x).ravel() if is_1d else Op @ x
 
             return LazyLinOp(
                 shape=(nwindows * window, N),
                 matmat=lambda x: _matmat(window, nhop, x),
                 rmatmat=lambda x: _rmatmat(window, nhop, x)
             )
     else:
@@ -209,34 +223,42 @@
             if start[s] >= N:
                 raise Exception("start must be < N.")
             if end[s] >= N:
                 raise Exception("end must be < N.")
             L += end[s] - start[s] + 1
 
         def _matmat(x, start, end):
-            # x is always 2d
+            if x.ndim == 1:
+                is_1d = True
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             y = np.empty((L, x.shape[1]), dtype=x.dtype)
             offset = 0
             for s in range(S):
                 y[offset:(offset + end[s] - start[s] + 1), :] = (
                     x[start[s]:(end[s] + 1), :]
                 )
                 offset += end[s] - start[s] + 1
-            return y
+            return y.ravel() if is_1d else y
 
         def _rmatmat(x, start, end):
-            # x is always 2d
+            if x.ndim == 1:
+                is_1d = True
+                x = x.reshape(x.shape[0], 1)
+            else:
+                is_1d = False
             y = np.zeros((N, x.shape[1]), dtype=x.dtype)
             offset = 0
             for s in range(S):
                 y[start[s]:(end[s] + 1), :] = (
                     x[offset:(offset + end[s] - start[s] + 1), :]
                 )
                 offset += end[s] - start[s] + 1
-            return y
+            return y.ravel() if is_1d else y
 
         return LazyLinOp(
             shape=(L, N),
             matmat=lambda x: _matmat(x, start, end),
             rmatmat=lambda x: _rmatmat(x, start, end)
         )
```

## Comparing `lazylinop-1.9.0.dist-info/LICENSE.txt` & `lazylinop-1.9.0a0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `lazylinop-1.9.0.dist-info/METADATA` & `lazylinop-1.9.0a0.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: lazylinop
-Version: 1.9.0
+Version: 1.9.0a0
 Summary: A package dedicated to lazy linear operators based on diverse backends/libraries.
 Author-email: Inria <remi.gribonval@inria.fr>, Pascal Carrivain <pascal.carrivain@inria.fr>, Simon Delamare <simon.delamare@ens-lyon.fr>, Hakim Hadj-Djilani <hakim.hadj-djilani@inria.fr>, Rémi Gribonval <remi.gribonval@inria.fr>
 License: Copyright 2023, Inria
         
         BSD License 2.0
         
         Redistribution and use in source and binary forms, with or without
```

## Comparing `lazylinop-1.9.0.dist-info/RECORD` & `lazylinop-1.9.0a0.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,69 +1,69 @@
-lazylinop/__init__.py,sha256=vtMszzSaq3C1-E30UEpGW97Guf9BhIWfsApEAW6zCHg,176
+lazylinop/__init__.py,sha256=GhBzCu1T3HKXvjVOrNCAKz0vATBLZm9WF19BYay7MlQ,178
 lazylinop/check_op.py,sha256=F9Fruv6a-_izX9E-RqgY369FsSUPJBzNexfa1eCc310,8275
-lazylinop/lazylinop.py,sha256=wD0DAIKoI6I83WHKPhheBu7ShGjV9BkTKmhRikO3M2Y,53134
+lazylinop/lazylinop.py,sha256=m0Cqn98_RXoo7Qpxa5RirtZ1kwU64GCcBsc4W0HnQec,52918
 lazylinop/basicops/__init__.py,sha256=_LNJ-J7w_qkGvBs0OgSWJl5mYwcgBIVc6E5qf4fp2YU,370
 lazylinop/basicops/add.py,sha256=PVftHIYd4l5Ddhr_kSoZCzcpPza14ylL0u4-62gYW3M,1508
-lazylinop/basicops/blockdiag.py,sha256=L6lldGAiZjr8hJ9c27xPg2r6yk7HnaqkhZtzBQqoCbQ,2098
+lazylinop/basicops/blockdiag.py,sha256=Ka2zsHzFNSoyDQC3VVMVsL_xHriY4QWg53Bf6zZeSKw,2255
 lazylinop/basicops/cat.py,sha256=8anT601uU8owwDD1LrxNN4mEVyBzSRqLdY_5IgBns_I,1852
-lazylinop/basicops/diag.py,sha256=IH4QC0zl6_cH2g2V2swNyxh2LRsLjk2kt6XFqzs-sYk,11518
-lazylinop/basicops/eye.py,sha256=yRzRaP2hBxgqCkN71XUl8mdmVixAVVOCCViXm0npVic,3287
-lazylinop/basicops/kron.py,sha256=AZYn4zfdgpRHxD5lqHqlyDXb7m_qFhFtxDn_bEES_7o,3600
-lazylinop/basicops/ones.py,sha256=EluOEGBjAAWw553C7UPY3OzzgIwC9upoRuyK4qM5pmc,2261
-lazylinop/basicops/pad.py,sha256=VmHsyagRx9RXK0TX5gxswzZjSJ_0TD5TvHbu0FYXGQ8,42194
-lazylinop/basicops/zeros.py,sha256=FnSBgNA8bOLZAj0F4P9o_JaqBRtIFjcMC0njEHtKVIU,1343
+lazylinop/basicops/diag.py,sha256=I3Ai0tT5SFfM9mg33oirV4P5LeNgBN3eAawn6kH6rNE,11699
+lazylinop/basicops/eye.py,sha256=0IT1OWR8laofAnHpbGijbYKzC4ZCeGdg4AdwqPZ0KdU,3449
+lazylinop/basicops/kron.py,sha256=Tz9tFkXqZ95F4IWdMIlVrN-vVuIi3mkf4A2TjfBuunQ,3792
+lazylinop/basicops/ones.py,sha256=W3cH6TY4uyc0ZORGut9xfWJY924WSFkZJX-TIKb5mTg,2309
+lazylinop/basicops/pad.py,sha256=0n6skeJqIDZeV7rWRTDGPlF5PZS-mWe9QF1rzmIZ-Ac,42464
+lazylinop/basicops/zeros.py,sha256=tLcN6ohWz51r5iwVtWYFDWCambR0txymC_18og9-B9s,1573
 lazylinop/polynomial/__init__.py,sha256=dGdRlm9i4HmqWcFIHAYV4MueUfQ1KNIsuuKadGa4-q0,264
-lazylinop/polynomial/polynomial.py,sha256=T_JPdM1XieysPmLnqhCGfNP6_nJ-10UFNONfF46Mrjc,52587
+lazylinop/polynomial/polynomial.py,sha256=iCf5AmHZJ4PNwFYKjQdaqQlwcfpZDkYhJhbFpXdlpdY,53006
 lazylinop/wip/__init__.py,sha256=xZ_vPqFoP84vFy9mKWqTGWrapzv_NBUaBM5KN8yWe3M,100
 lazylinop/wip/code_optimization.py,sha256=ieJcsn0RFt8E3FBwv6zH2bddRkEDiupp0nsnYDLr__U,1443
 lazylinop/wip/linear_algebra.py,sha256=KuUz7eyOxNn3PlymsQ_h5l1qCmLUfbSoCI5ZbwIFYfw,1447
-lazylinop/wip/special_matrices.py,sha256=0KCDKUCPqfcucxWYP41-kEh-0EyDhRfS_2Lxj98OEEo,56577
+lazylinop/wip/special_matrices.py,sha256=bU0UPHN4VbvpKEes7FQSJi5FnBPpzMlyQxKmt6nNWi0,57938
 lazylinop/wip/basicops/__init__.py,sha256=VwLjxYoj0msANRJ9SHHLTcsFR7v9TmcJT_QErMf9pBg,85
-lazylinop/wip/basicops/anti_diag.py,sha256=CO8fdw6ZNFB_AcAEH94SKr2fyGB9SAE2i8dYXOtc4XQ,12477
+lazylinop/wip/basicops/anti_diag.py,sha256=Ew1P2b9YFI5m_gl-Ljahe7apCzYrtJHcZO8Dc3IvNG4,12658
 lazylinop/wip/basicops/average.py,sha256=hVq1VL9k-2micYUpkGdpnDdrbOk4wbn32Sp8GgefN8k,4066
 lazylinop/wip/basicops/mean.py,sha256=AASwMFwm1sHpFIad11XK5FlzzYhINvX5ZDTZPUCUGpc,2206
 lazylinop/wip/linalg/__init__.py,sha256=tT37IW-J-jlBQzHhIWeCZLyqypFOra8B7t3Tlz3fCTA,298
-lazylinop/wip/linalg/coshm.py,sha256=wgZiKaJCPQ4NXMF1XQE0T_WWwOZnyIID2ZP4_j4QNBY,4486
+lazylinop/wip/linalg/coshm.py,sha256=81UlRLwlaeYT6vjIQjsG41MRHp56P8GuDaHWrdgYtAY,4792
 lazylinop/wip/linalg/cosm.py,sha256=LGLKc1VGIX2J7BuL4AFJVoKpcglcAl3Aqvdmgy4wSJ0,2816
-lazylinop/wip/linalg/expm.py,sha256=nbodvin-oadIu8X3rLR3w1uvbGYXZRE_eyUzsoFnK6w,4037
-lazylinop/wip/linalg/khatri_rao.py,sha256=k-YvUZ4-bzeBHY6K3PiSt2oM7bxrVcez-t9O3wXfPyQ,5461
-lazylinop/wip/linalg/logm.py,sha256=a8lNIaORlGNOZ_rVrix5_U4xfQB0A9PUN3vy8ymWL1w,4295
-lazylinop/wip/linalg/sinhm.py,sha256=5MWbQyKcAuwoa_ZLEj_Tz1BfElh_NOxhtPgA7VUpUlI,4388
+lazylinop/wip/linalg/expm.py,sha256=ivEsbaAkqBHH6qulGwrbEZBB7ONJLDCLWIRHH7O9gWc,4233
+lazylinop/wip/linalg/khatri_rao.py,sha256=ob4rPDzX2WU4DTZz-02mdcJaaQTBwpS3Koldaxofozw,5587
+lazylinop/wip/linalg/logm.py,sha256=JJDAiqOeWN7fBY_6Ju04Qn1tIN0hLqKVpG4luWKP3bM,4667
+lazylinop/wip/linalg/sinhm.py,sha256=qdr8snq2n5AzbOpHhThHFbjivdZXCp34oBrR1p9dxFc,4694
 lazylinop/wip/linalg/sinm.py,sha256=QYt4VggWD-k4D7eZtg6F3cxMCBJrMn2aymyzTMXnUrI,2823
 lazylinop/wip/linalg/spectral_norm.py,sha256=oJi89eZA3yzzeIpE5f1AyDmvkQ2QXLC-G3Lw7oOoHfQ,1582
-lazylinop/wip/linalg/sqrtm.py,sha256=SoJ4mr11FciWlOvK30fvo0FWFtZrDF_o-oRICxwhIIo,3578
+lazylinop/wip/linalg/sqrtm.py,sha256=NCC-rQdNwOScIawgTYGHiRPRTA54O2t1sBZ6i2rRXiI,3754
 lazylinop/wip/parallel/__init__.py,sha256=iuAy8qIWtIv67Ro5GkkFUUaJUyJOuYXhkCaF0k8fFAo,253
 lazylinop/wip/parallel/mpilop.py,sha256=zJQXwVoQIOiL1bHbg_wNgqMKq4DH45LVayRxq-wp-rA,8328
 lazylinop/wip/parallel/pmatmat.py,sha256=2stXEbqFsj984Eg47EPbMtAutQFb9shSTtJHVQPefKw,4897
 lazylinop/wip/parallel/pmatmat_mpi.py,sha256=9s6o8dfWQitLlZSa5MVdkzke6JM7606d_i88a7aVMGo,4250
 lazylinop/wip/parallel/pmatmat_process.py,sha256=pTUXpSB0vq4KGNepKH3uQkmN8fkF6KFEuTWcLUwt-p0,5902
 lazylinop/wip/parallel/pmatmat_thread.py,sha256=gzjIoNV-ePz30K0l_TipJi--VyVfO3vl0E8oOn9OH5o,3306
 lazylinop/wip/random/__init__.py,sha256=Sqc6D6d_4UI8LQTlBzRz5IDpSLVK5ejcL5FJmC_A2FI,23
 lazylinop/wip/random/rand.py,sha256=8ZEtNxJtInOsJxcYDj8JdTobNzLXlaEmCDAlqbMq_mo,6721
 lazylinop/wip/signal/__init__.py,sha256=K2vTKn5YiH6_OTfxXpbSLWkBvglieumLhQ4Cad6D7U4,501
-lazylinop/wip/signal/anti_eye.py,sha256=qBW5Y3hzSyUr-9VSVsYRTMa8-zRLsc36fnYilvA6EkU,3659
-lazylinop/wip/signal/bc.py,sha256=pD2xqy9STjaLN0TsJIAw5dfYtaMku9Xo4Ofm5HBN9a4,4752
+lazylinop/wip/signal/anti_eye.py,sha256=TgvScGKTtKh52LZVIWKLuviQXLz98ow2O5MqG_vX0m8,3796
+lazylinop/wip/signal/bc.py,sha256=GDx-V7o-BNiEgxTLz0aNrBBAZuEPVQLzmXjWYgrp7Tc,4084
 lazylinop/wip/signal/bc2d.py,sha256=qePWqhrJ2l2NvCz3K42x-p2nI_CIar82oSTbNR9lTd0,4117
-lazylinop/wip/signal/convolve.py,sha256=4E_rgR2qmB1wnYq-3QSdoeW55dV_4IzRcMgcvAKHunE,33110
-lazylinop/wip/signal/convolve2d.py,sha256=9EPiXgVGupaJ5EARabB66UmBL8nMseT3z_GKER-NmWE,12402
-lazylinop/wip/signal/dct.py,sha256=mPDkDPNwoJSc7ynWqZKhkpuVZAtvGZSITG201zJUIN8,10537
-lazylinop/wip/signal/decimate.py,sha256=rXj50VYw6vOsbw3U3iRXEVNcy3SnBu0zV6CB0pNjqd0,3013
-lazylinop/wip/signal/ds_mconv.py,sha256=S01OrJtJRTd3fyn7sXZWfynY5EqydQ_tv_9lXrmEK4Y,10239
-lazylinop/wip/signal/dsconvolve.py,sha256=ItnXqtED_O573D5AcG1ID3U0ZhiIaMEfQzO5MRxcXA8,9694
-lazylinop/wip/signal/dst.py,sha256=FbqTz8pLvr1HUagT9dXHQrDGGItnU-X5KNw0JEmxTOw,11453
-lazylinop/wip/signal/dwt1d.py,sha256=iHK5zS_NwWizT7st6rYYGk7k8M7oP1itjyUhYXkdR-k,11717
-lazylinop/wip/signal/dwt2d.py,sha256=TVlDogm6zjp_AP-edFdstJAQfEnQwqF-i141ZMN8_2k,18373
-lazylinop/wip/signal/fft.py,sha256=i_oQhmYJ9V0dCeItdUNTvMol9zeGFmlteF5jwFV_ldc,10924
+lazylinop/wip/signal/convolve.py,sha256=ybGZQ7IWOT2QPpnmNSZqmnll7fX3B3NJ_AaIMU07gbE,34045
+lazylinop/wip/signal/convolve2d.py,sha256=qP-tSpOxzAfUO2-6RPjljF34xlNrO4nSroNqCEYPMi0,12692
+lazylinop/wip/signal/dct.py,sha256=Ic4JmY6iIZvG4h2VUqGxu61V1b5jEu1xr4x5RfMe-eo,11141
+lazylinop/wip/signal/decimate.py,sha256=bBJ9Kqw9PqTcybT-L6THf8YraW8BDWo1pvlXqKW73M0,3272
+lazylinop/wip/signal/ds_mconv.py,sha256=NHL_OgNq0IzFwBBIE5GmeQoPmOGlIYRvZZd6oz7qm-g,10179
+lazylinop/wip/signal/dsconvolve.py,sha256=iqxbql8U-02mDbnA-zzW_ALPZIs7Eljj2liHLRolbME,9636
+lazylinop/wip/signal/dst.py,sha256=Z4BCrVoQOQdqRap7KTv5FwcqlBUk3Cunlfa8aJrcfX4,12181
+lazylinop/wip/signal/dwt1d.py,sha256=kWTFj3xqMxSLSNIZ0tb47euThPBuwmaTBumvqxiy4Do,12014
+lazylinop/wip/signal/dwt2d.py,sha256=nBUuyu1PL8Ul8Z4mVDkm54p6iRjzG3uCAfo9WCHhkhM,18663
+lazylinop/wip/signal/fft.py,sha256=kWXxSKhVgsSR6GHOIxWMR5OWz6wgpu_F-nTJ5cW5iDo,11167
 lazylinop/wip/signal/fft2.py,sha256=H8cTZpj8MDJHCq70Gi82PxhLtfjK3zDM1ZOTsZywRDI,2714
-lazylinop/wip/signal/flip.py,sha256=Cr_zT_xAhlRDabuI5we4aIjCgVNmHd50hdO_b0QScic,2629
-lazylinop/wip/signal/fwht.py,sha256=8EIx9C2rccv49-TfHjqBKuum8KXTVeCOC0tsygkYC08,7242
+lazylinop/wip/signal/flip.py,sha256=d_hmvb-YqMNNE1fVkFLrZzJE2lOAOfDFh7ZtQaNUR0M,2854
+lazylinop/wip/signal/fwht.py,sha256=RMSH_EUpzHy7gaUXiYrSkmOUwx_m_HQLoNpgc8PouCc,7186
 lazylinop/wip/signal/is_power_of_two.py,sha256=ZrPcCTQXAXqLbgA1hcUOFDtIf3cRVDubYBC8PzoB1kg,269
-lazylinop/wip/signal/mslices.py,sha256=5khzV90C-eJ9Ow3xmTVsCIba7BJNxWg3ClRn2sOHDAE,3057
-lazylinop/wip/signal/oa.py,sha256=saUo5_UAEMqljr1hmh-V6zeRTyqHlhqBPUjPjcZ7YPQ,2147
-lazylinop/wip/signal/scatter_and_gather_windows.py,sha256=xJkXI_tn2F_1MKk1V7GC6uokl2CUFqZiLSE8q3r5RmA,2958
-lazylinop/wip/signal/slices.py,sha256=EuOYQmJ8DUwLxw5IVmqGn1LSZOxrxeaq34SsTBb7ut4,8565
+lazylinop/wip/signal/mslices.py,sha256=7URvTFWv7VA_YHfF5GhNDyLv3GZgd9UCvzTA_nAXCjg,3315
+lazylinop/wip/signal/oa.py,sha256=b8iDO6DsOBFw23FhytK6VQPBpWLA42NcETIpyI1eOLE,2438
+lazylinop/wip/signal/scatter_and_gather_windows.py,sha256=C8AcKiKatby7qyNChUDEPpmxwJdXslcbpKmR9_SJ4lw,3230
+lazylinop/wip/signal/slices.py,sha256=lZ6eCalHexhflgbNuWnj4dP1akmfkgZhIwR6iE9WEKg,9463
 lazylinop/wip/signal/stft.py,sha256=FQmNAXVBkPWud0rQwPfbubnhgn9J15iAa8mqZ9P2FG4,4291
-lazylinop-1.9.0.dist-info/LICENSE.txt,sha256=jHt8qQXwxwsxIgSCZbFwPqQw3R1QWQK0JzZZ3PAwlU0,1434
-lazylinop-1.9.0.dist-info/METADATA,sha256=vsnOUZIa6Hnu-r4q3ixJb8z8WA4S60Otlv4iSTzRECg,8483
-lazylinop-1.9.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-lazylinop-1.9.0.dist-info/top_level.txt,sha256=zq6N2WH1Vl_0zSzqC12W0Oil87_uMjD3sYhWGjRioIc,10
-lazylinop-1.9.0.dist-info/RECORD,,
+lazylinop-1.9.0a0.dist-info/LICENSE.txt,sha256=jHt8qQXwxwsxIgSCZbFwPqQw3R1QWQK0JzZZ3PAwlU0,1434
+lazylinop-1.9.0a0.dist-info/METADATA,sha256=lcftsO661OZtj6HnO_2Q-ZHtmGlMluU7vF_FwERTm_s,8485
+lazylinop-1.9.0a0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+lazylinop-1.9.0a0.dist-info/top_level.txt,sha256=zq6N2WH1Vl_0zSzqC12W0Oil87_uMjD3sYhWGjRioIc,10
+lazylinop-1.9.0a0.dist-info/RECORD,,
```

